[
  {
    "slug": "llama3.1",
    "name": "llama3.1",
    "description": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
    "pull_count": "110.5M",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "llama3.1:latest",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "llama3.1:8b",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "llama3.1:70b",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "llama3.1:405b",
        "size": "243GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Meta Llama 3.1\nLlama 3.1\nfamily of models available:\n8B\n70B\n405B\nLlama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation.\nThe upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables Meta‚Äôs latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants.\nMeta also has made changes to their license, allowing developers to use the outputs from Llama models, including the 405B model, to improve other models.\nModel evaluations\nFor this release, Meta has evaluation the performance on over 150 benchmark datasets that span a wide range of languages. In addition, Meta performed extensive human evaluations that compare Llama 3.1 with competing models in real-world scenarios. Meta‚Äôs experimental evaluation suggests that our flagship model is competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. Additionally, Meta‚Äôs smaller models are competitive with closed and open models that have a similar number of parameters.\nReferences\nMeta AI Llama 3.1 launch blog post"
  },
  {
    "slug": "llama3.2",
    "name": "llama3.2",
    "description": "Meta's Llama 3.2 goes small with 1B and 3B models.",
    "pull_count": "58M",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "llama3.2:latest",
        "size": "2.0GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "llama3.2:1b",
        "size": "1.3GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "llama3.2:3b",
        "size": "2.0GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\nSizes\n3B parameters (default)\nThe 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as:\nFollowing instructions\nSummarization\nPrompt rewriting\nTool use\nollama run llama3.2\n1B parameters\nThe 1B model is competitive with other 1-3B parameter models. It‚Äôs use cases include:\nPersonal information management\nMultilingual knowledge retrieval\nRewriting tasks running locally on edge\nollama run llama3.2:1b\nBenchmarks\nSupported Languages:\nEnglish, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages."
  },
  {
    "slug": "mistral",
    "name": "mistral",
    "description": "The 7B model released by Mistral AI, updated to version 0.3.",
    "pull_count": "25.6M",
    "updated": "7 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "mistral:latest",
        "size": "4.4GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "mistral:7b",
        "size": "4.4GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Mistral is a 7B parameter model, distributed with the Apache license. It is available in both instruct (instruction following) and text completion.\nThe Mistral AI team has noted that Mistral 7B:\nOutperforms Llama 2 13B on all benchmarks\nOutperforms Llama 1 34B on many benchmarks\nApproaches CodeLlama 7B performance on code, while remaining good at English tasks\nVersions\nTag\nDate\nNotes\nv0.3\nlatest\n05/22/2024\nA new version of Mistral 7B that supports function calling.\nv0.2\n03/23/2024\nA minor release of Mistral 7B\nv0.1\n09/27/2023\nInitial release\nFunction calling\nMistral 0.3 supports function calling with Ollama‚Äôs\nraw mode\n.\nExample raw prompt\n[AVAILABLE_TOOLS] [{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}][/AVAILABLE_TOOLS][INST] What is the weather like today in San Francisco [/INST]\nExample response\n[TOOL_CALLS] [{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"San Francisco, CA\", \"format\": \"celsius\"}}]\nFor more information on raw mode, see the\nAPI documentation\n.\nVariations\ninstruct\nInstruct models follow instructions\ntext\nText models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion.\nUsage\nCLI\nInstruct:\nollama run mistral\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\nReferences\nHuggingFace\nMistral AI News Release"
  },
  {
    "slug": "deepseek-r1",
    "name": "deepseek-r1",
    "description": "DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.",
    "pull_count": "78.6M",
    "updated": "7 months ago",
    "capabilities": [
      "tools",
      "thinking"
    ],
    "tags": [
      {
        "tag": "deepseek-r1:latest",
        "size": "5.2GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-r1:1.5b",
        "size": "1.1GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-r1:7b",
        "size": "4.7GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-r1:8b",
        "size": "5.2GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-r1:14b",
        "size": "9.0GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-r1:32b",
        "size": "20GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-r1:70b",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-r1:671b",
        "size": "404GB",
        "context_window": "160K",
        "input_type": "Text"
      }
    ],
    "readme": "DeepSeek-R1 has received a minor version upgrade to DeepSeek-R1-0528 for the 8 billion parameter distilled model and the full 671 billion parameter model. In this update, DeepSeek R1 has significantly improved its reasoning and inference capabilities. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\nModels\nDeepSeek-R1-0528-Qwen3-8B\nollama run deepseek-r1\nDeepSeek-R1\nollama run deepseek-r1:671b\nNote: to update the model from an older version, run\nollama pull deepseek-r1\nDistilled models\nDeepSeek team has demonstrated that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.\nBelow are the models created via fine-tuning against several dense models widely used in the research community using reasoning data generated by DeepSeek-R1. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.\nDeepSeek-R1-0528-Qwen3-8B\nollama run deepseek-r1:8b\nDeepSeek-R1-Distill-Qwen-1.5B\nollama run deepseek-r1:1.5b\nDeepSeek-R1-Distill-Qwen-7B\nollama run deepseek-r1:7b\nDeepSeek-R1-Distill-Qwen-14B\nollama run deepseek-r1:14b\nDeepSeek-R1-Distill-Qwen-32B\nollama run deepseek-r1:32b\nDeepSeek-R1-Distill-Llama-70B\nollama run deepseek-r1:70b\nLicense\nThe model weights are licensed under the MIT License. DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\nThe Qwen distilled models are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.\nThe Llama 8B distilled model is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license.\nThe Llama 70B distilled model is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license."
  },
  {
    "slug": "qwen2.5",
    "name": "qwen2.5",
    "description": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
    "pull_count": "22M",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "qwen2.5:latest",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5:0.5b",
        "size": "398MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5:1.5b",
        "size": "986MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5:3b",
        "size": "1.9GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5:7b",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5:14b",
        "size": "9.0GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5:32b",
        "size": "20GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5:72b",
        "size": "47GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, a range of base language models and instruction-tuned models are released, with sizes ranging from 0.5 to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2:\nIt possesses\nsignificantly more knowledge\nand has greatly enhanced capabilities in\ncoding\nand\nmathematics\n, due to specialized expert models in these domains.\nIt demonstrates significant advancements in\ninstruction following\n,\nlong-text generation\n(over 8K tokens),\nunderstanding structured data\n(e.g., tables), and\ngenerating structured outputs\n, especially in JSON format. It is also\nmore resilient to diverse system prompts\n, improving role-play and condition-setting for chatbots.\nIt supports\nlong contexts\nof up to 128K tokens and can generate up to 8K tokens.\nIt offers\nmultilingual support\nfor over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nPlease note: all models except the 3B and 72B are released under the Apache 2.0 license, while the 3B and 72B models are under the Qwen license.\nReferences\nGitHub\nBlog post\nHuggingFace"
  },
  {
    "slug": "gemma2",
    "name": "gemma2",
    "description": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
    "pull_count": "16.4M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "gemma2:latest",
        "size": "5.4GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "gemma2:2b",
        "size": "1.6GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "gemma2:9b",
        "size": "5.4GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "gemma2:27b",
        "size": "16GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "Google‚Äôs Gemma 2 model is available in three sizes, 2B, 9B and 27B, featuring a brand new architecture designed for class leading performance and efficiency.\nClass leading performance\nAt 27 billion parameters, Gemma 2 delivers performance surpassing models more than twice its size in benchmarks. This breakthrough efficiency sets a new standard in the open model landscape.\nThree sizes: 2B, 9B and 27B parameters\n2B Parameters\nollama run gemma2:2b\n9B Parameters\nollama run gemma2\n27B Parameters\nollama run gemma2:27b\nBenchmark\nIntended Usage\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\nResearch and Education\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\nUsing Gemma 2 with popular tooling\nLangChain\nfrom langchain_community.llms import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.invoke(\"Why is the sky blue?\")\nLlamaIndex\nfrom llama_index.llms.ollama import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.complete(\"Why is the sky blue?\")"
  },
  {
    "slug": "mxbai-embed-large",
    "name": "mxbai-embed-large",
    "description": "State-of-the-art large embedding model from mixedbread.ai",
    "pull_count": "7.6M",
    "updated": "1 year ago",
    "capabilities": [
      "embedding",
      "335m"
    ],
    "tags": [
      {
        "tag": "mxbai-embed-large:latest",
        "size": "670MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "mxbai-embed-large:335m",
        "size": "670MB",
        "context_window": "512",
        "input_type": "Text"
      }
    ],
    "readme": "mxbai-embed-large\nAs of March 2024, this model archives SOTA performance for Bert-large sized models on the MTEB. It outperforms commercial models like OpenAIs\ntext-embedding-3-large\nmodel and matches the performance of model 20x its size.\nmxbai-embed-large\nwas trained with no overlap of the MTEB data, which indicates that the model generalizes well across several domains, tasks and text length.\nUsage\nREST API\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"mxbai-embed-large\",\n  \"prompt\": \"Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering\"\n}'\nPython library\nollama.embeddings(model='mxbai-embed-large', prompt='Represent this sentence for searching relevant passages: The sky is blue because of Rayleigh scattering')\nJavascript library\nollama.embeddings({ model: 'mxbai-embed-large', prompt: 'Represent this sentence for searching relevant passages:  The sky is blue because of Rayleigh scattering' })\nReferences\nBlog post\nHugging Face"
  },
  {
    "slug": "phi4",
    "name": "phi4",
    "description": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
    "pull_count": "7.2M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "phi4:latest",
        "size": "9.1GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "phi4:14b",
        "size": "9.1GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Phi-4\nis a 14B parameter, state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets.\nThe model underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nContext length:\n16k tokens\nPrimary use cases\nThe model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\nMemory/compute constrained environments.\nLatency bound scenarios.\nReasoning and logic.\nOut-of-scope use cases\nThe models are not specifically designed or evaluated for all downstream purposes, thus:\nDevelopers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model‚Äôs focus on English.\nNothing contained in this readme should be interpreted as or deemed a restriction or modification to the license the model is released under."
  },
  {
    "slug": "gemma",
    "name": "gemma",
    "description": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
    "pull_count": "5.9M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "gemma:latest",
        "size": "5.0GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "gemma:2b",
        "size": "1.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "gemma:7b",
        "size": "5.0GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires Ollama 0.1.26 or later.\nDownload it here\n.\nGemma is a new open model developed by Google and its DeepMind team. It‚Äôs inspired by Gemini models at Google.\nGemma is available in both\n2b\nand\n7b\nparameter sizes:\nollama run gemma:2b\nollama run gemma:7b\n(default)\nThe models undergo training on a diverse dataset of web documents to expose them to a wide range of linguistic styles, topics, and vocabularies. This includes code to learn syntax and patterns of programming languages, as well as mathematical text to grasp logical reasoning.\nTo ensure the safety of the model, the team employed various data cleaning and filtering techniques, including rigorous filtering for CSAM (child sexual abuse material), sensitive data filtering, and filtering based on content quality in compliance with Google‚Äôs policies.\nReference\nGoogle Gemma"
  },
  {
    "slug": "qwen2.5-coder",
    "name": "qwen2.5-coder",
    "description": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
    "pull_count": "11.3M",
    "updated": "9 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "qwen2.5-coder:latest",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5-coder:0.5b",
        "size": "398MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5-coder:1.5b",
        "size": "986MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5-coder:3b",
        "size": "1.9GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5-coder:7b",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5-coder:14b",
        "size": "9.0GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2.5-coder:32b",
        "size": "20GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Qwen 2.5 Coder series of models are now updated in 6 sizes:\n0.5B, 1.5B, 3B, 7B, 14B and 32B\n.\nThere are significant improvements in\ncode generation\n,\ncode reasoning\nand\ncode fixing\n. The 32B model has competitive performance with OpenAI‚Äôs GPT-4o.\n32B:\nollama run qwen2.5-coder:32b\n14B:\nollama run qwen2.5-coder:14b\n7B:\nollama run qwen2.5-coder:7b\n3B:\nollama run qwen2.5-coder:3b\n1.5B:\nollama run qwen2.5-coder:1.5b\n0.5B:\nollama run qwen2.5-coder:0.5b\nCode capabilities reaching state of the art for open-source models\nCode Generation:\nQwen2.5 Coder 32B Instruct, as the flagship model of this open-source release, has achieved the best performance among open-source models on multiple popular code generation benchmarks (EvalPlus, LiveCodeBench, BigCodeBench), and has competitive performance with GPT-4o.\nCode Repair:\nCode repair is an important programming skill. Qwen2.5 Coder 32B Instruct can help users fix errors in their code, making programming more efficient. Aider is a popular benchmark for code repair, and Qwen2.5 Coder 32B Instruct scored 73.7, performing comparably to GPT-4o on Aider.\nCode Reasoning:\nCode reasoning refers to the model‚Äôs ability to learn the process of code execution and accurately predict the model‚Äôs inputs and outputs. The recently released Qwen2.5 Coder 7B Instruct has already shown impressive performance in code reasoning, and this 32B model takes it a step further.\nMultiple programming languages\nAn intelligent programming assistant should be familiar with all programming languages. Qwen 2.5 Coder 32B performs excellent across more than 40 programming languages, scoring 65.9 on McEval, with impressive performances in languages like Haskell and Racket. The Qwen team used their own unique data cleaning and balancing during the pre-training phase.\nAdditionally, the multi-language code repair capabilities of Qwen 2.5 Coder 32B Instruct remain impressive, aiding users in understanding and modifying programming languages they are familiar with, significantly reducing the learning cost of unfamiliar languages. Similar to McEval, MdEval is a multi-language code repair benchmark, where Qwen 2.5 Coder 32B Instruct scored 75.2, ranking first among all open-source models.\nHuman Preference\nTo evaluate the alignment performance of Qwen 2.5 Coder 32B Instruct with human preferences, we constructed an internal annotated code preference evaluation benchmark called Code Arena (similar to Arena Hard). We used GPT-4o as the evaluation model for preference alignment, employing an ‚ÄòA vs. B win‚Äô evaluation method, which measures the percentage of instances in the test set where model A‚Äôs score exceeds model B‚Äôs. The results below demonstrate the advantages of Qwen 2.5 Coder 32B Instruct in preference alignment.\nComprehensive model sizes to fit your device\nReferences\nBlog Post\nHuggingFace"
  },
  {
    "slug": "llava",
    "name": "llava",
    "description": "üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
    "pull_count": "12.9M",
    "updated": "2 years ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "llava:latest",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text, Image"
      },
      {
        "tag": "llava:7b",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text, Image"
      },
      {
        "tag": "llava:13b",
        "size": "8.0GB",
        "context_window": "4K",
        "input_type": "Text, Image"
      },
      {
        "tag": "llava:34b",
        "size": "20GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "üåã LLaVA: Large Language and Vision Assistant\nLLaVA is a multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4.\nNew in LLaVA 1.6:\nIncreasing the input image resolution to up to 4x more pixels, supporting 672x672, 336x1344, 1344x336 resolutions.\nBetter visual reasoning and OCR capability with an improved visual instruction tuning data mixture.\nBetter visual conversation for more scenarios, covering different applications.\nBetter world knowledge and logical reasoning.\nCLI Usage\nRun the model:\nollama run llava\nThen at the prompt, include the path to your image in the prompt:\n>>> What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\nAPI Usage\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llava\",\n  \"prompt\":\"What is in this picture?\",\n  \"images\": [\"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\"]\n}'\nReferences\nWebsite\nGitHub\nHuggingFace"
  },
  {
    "slug": "phi3",
    "name": "phi3",
    "description": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
    "pull_count": "16M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "phi3:latest",
        "size": "2.2GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "phi3:3.8b",
        "size": "2.2GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "phi3:14b",
        "size": "7.9GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Phi-3 is a family of open AI models developed by Microsoft.\nParameter sizes\nPhi-3 Mini\n‚Äì 3B parameters ‚Äì\nollama run phi3:mini\nPhi-3 Medium\n‚Äì 14B parameters ‚Äì\nollama run phi3:medium\nContext window sizes\nNote: the 128k version of this model requires\nOllama 0.1.39\nor later.\n4k\nollama run phi3:mini\nollama run phi3:medium\n128k\nollama run phi3:medium-128k\nPhi-3 Mini\nPhi-3 Mini is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\nPhi-3 Medium\nPhi-3 Medium is a 14B parameter language model, and outperforms Gemini 1.0 Pro.\nIntended Uses\nPrimary use cases\nThe model is intended for commercial and research use in English. The model provides uses for applications which require\n1) memory/compute constrained environments\n2) latency bound scenarios\n3) strong reasoning (especially math and logic)\n4) long context\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nUse case considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios.\nDevelopers  should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nResponsible AI Considerations\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as ‚Äútyping, math, random, collections, datetime, itertools‚Äù. If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nTraining\nModel\nArchitecture: Phi-3 Mini has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines.\nInputs: Text. It is best suited for prompts using chat format.\nContext length: 128K tokens\nGPUS: 512 H100-80G\nTraining time: 7 days\nTraining data: 3.3T tokens\nOutputs: Generated text in response to the input\nDates: Our models were trained between February and April 2024\nStatus: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\nDatasets\nOur training data includes a wide variety of sources, totaling 3.3 trillion tokens, and is a combination of\n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nSoftware\nPyTorch\nDeepSpeed\nTransformers\nFlash-Attention\nLicense\nThe model is licensed under the\nMIT license\n.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\nMicrosoft‚Äôs Trademark & Brand Guidelines\n. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.\nResources\nHuggingFace\nPhi-3 Microsoft Blog\nPhi-3 Technical Report\nPhi-3 on Azure AI Studio\nPhi-3 on Hugging Face\nPhi-3 ONNX:\n4K\nand\n128K"
  },
  {
    "slug": "gpt-oss",
    "name": "gpt-oss",
    "description": "OpenAI‚Äôs open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
    "pull_count": "7.1M",
    "updated": "4 months ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "gpt-oss:latest",
        "size": "14GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "gpt-oss:20b",
        "size": "14GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "gpt-oss:120b",
        "size": "65GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "gpt-oss:20b-cloud",
        "size": "-",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "gpt-oss:120b-cloud",
        "size": "-",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Welcome OpenAI‚Äôs gpt-oss!\nOllama partners with OpenAI to bring its latest state-of-the-art open weight models to Ollama. The two models, 20B and 120B, bring a whole new local chat experience, and are designed for powerful reasoning, agentic tasks, and versatile developer use cases.\nGet started\nYou can get started by\ndownloading the latest Ollama version\n.\nThe model can be downloaded directly in Ollama‚Äôs new app or via the terminal:\nollama run gpt-oss:20b\nollama run gpt-oss:120b\nFeature highlights\nAgentic capabilities:\nUse the models‚Äô native capabilities for function calling, web browsing (Ollama is introducing built-in web search that can be optionally enabled), python tool calls, and structured outputs.\nFull chain-of-thought:\nGain complete access to the model‚Äôs reasoning process, facilitating easier debugging and increased trust in outputs.\nConfigurable reasoning effort:\nEasily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\nFine-tunable:\nFully customize models to your specific use case through parameter fine-tuning.\nPermissive Apache 2.0 license:\nBuild freely without copyleft restrictions or patent risk‚Äîideal for experimentation, customization, and commercial deployment.\nQuantization - MXFP4 format\nOpenAI utilizes quantization to reduce the memory footprint of the gpt-oss models. The models are post-trained with quantization of the mixture-of-experts (MoE) weights to MXFP4 format, where the weights are quantized to 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter count, and quantizing these to MXFP4 enables the smaller model to run on systems with as little as 16GB memory, and the larger model to fit on a single 80GB GPU.\nOllama is supporting the MXFP4 format natively without additional quantizations or conversions. New kernels are developed for Ollama‚Äôs new engine to support the MXFP4 format.\nOllama collaborated with OpenAI to benchmark against their reference implementations to ensure Ollama‚Äôs implementations have the same quality.\n20B parameter model\ngpt-oss-20b model is designed for lower latency, local, or specialized use-cases.\n120B parameter model\nReference\nOpenAI launch blog\nOpenAI model card\nNVIDIA RTX blog\nNVIDIA cloud to edge"
  },
  {
    "slug": "qwen",
    "name": "qwen",
    "description": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
    "pull_count": "5.6M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "qwen:latest",
        "size": "2.3GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen:0.5b",
        "size": "395MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen:1.8b",
        "size": "1.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen:4b",
        "size": "2.3GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen:7b",
        "size": "4.5GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen:14b",
        "size": "8.2GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen:32b",
        "size": "18GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen:72b",
        "size": "41GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen:110b",
        "size": "63GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Qwen 2 is now available\nhere\n.\nQwen is a series of transformer-based large language models by Alibaba Cloud, pre-trained on a large volume of data, including web texts, books, code, etc.\nNew in Qwen 1.5\n6 model sizes, including 0.5B, 1.8B, 4B (default), 7B, 14B, 32B (new) and 72B\nollama run qwen:0.5b\nollama run qwen:1.8b\nollama run qwen:4b\nollama run qwen:7b\nollama run qwen:14b\nollama run qwen:32b\nollama run qwen:72b\nollama run qwen:110b\nSignificant performance improvement in human preference for chat models\nMultilingual support of both base and chat models\nStable support of 32K context length for models of all sizes\nThe original Qwen model is offered in four different parameter sizes: 1.8B, 7B, 14B, and 72B.\nFeatures\nLow-cost deployment\n: the minimum memory requirement for inference is less than 2GB.\nLarge-scale high-quality training corpora\n: Models are pre-trained on over 2.2 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields. The distribution of the pre-training corpus has been optimized through a large number of ablation experiments.\nGood performance\n: Qwen supports long context lengths (8K on the\n1.8b\n,\n7b\nand\n14b\nparameter models, and 32K on the\n72b\nparameter model), and significantly surpasses existing open-source models of similar scale on multiple Chinese and English downstream evaluation tasks (including common-sense, reasoning, code, mathematics, etc.), and even surpasses some larger-scale models in several benchmarks.\nMore comprehensive vocabulary coverage\n: Compared with other open-source models based on Chinese and English vocabularies, Qwen uses a vocabulary of over 150K tokens. This vocabulary is more friendly to multiple languages, enabling users to directly further enhance the capability for certain languages without expanding the vocabulary.\nSystem prompt\n: Qwen can realize role playing, language style transfer, task setting, and behavior-setting by using a system prompt.\nReference\nGitHub\nHugging Face"
  },
  {
    "slug": "qwen3",
    "name": "qwen3",
    "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.",
    "pull_count": "19.8M",
    "updated": "4 months ago",
    "capabilities": [
      "tools",
      "thinking"
    ],
    "tags": [
      {
        "tag": "qwen3:latest",
        "size": "5.2GB",
        "context_window": "40K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3:0.6b",
        "size": "523MB",
        "context_window": "40K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3:1.7b",
        "size": "1.4GB",
        "context_window": "40K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3:4b",
        "size": "2.5GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3:8b",
        "size": "5.2GB",
        "context_window": "40K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3:14b",
        "size": "9.3GB",
        "context_window": "40K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3:30b",
        "size": "19GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3:32b",
        "size": "20GB",
        "context_window": "40K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3:235b",
        "size": "142GB",
        "context_window": "256K",
        "input_type": "Text"
      }
    ],
    "readme": "Qwen 3\nis the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models:\nNew 30B model\nollama run qwen3:30b\nNew 235B model\nollama run qwen3:235b\nOverview\nThe Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model,\nQwen3-235B-A22B\n, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model,\nQwen3-30B-A3B\n, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\nSignificantly enhancement in its reasoning capabilities\n, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\nSuperior human preference alignment\n, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\nExpertise in agent capabilities\n, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\nSupport of 100+ languages and dialects\nwith strong capabilities for\nmultilingual instruction following and translation\n.\nReference\nBlog"
  },
  {
    "slug": "gemma3",
    "name": "gemma3",
    "description": "The current, most capable model that runs on a single GPU.",
    "pull_count": "32.2M",
    "updated": "2 months ago",
    "capabilities": [
      "vision",
      "cloud",
      "270m"
    ],
    "tags": [
      {
        "tag": "gemma3:latest",
        "size": "3.3GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "gemma3:270m",
        "size": "292MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "gemma3:1b",
        "size": "815MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "gemma3:4b",
        "size": "3.3GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "gemma3:12b",
        "size": "8.1GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "gemma3:27b",
        "size": "17GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "gemma3:4b-cloud",
        "size": "-",
        "context_window": "32K",
        "input_type": "Text, Image"
      },
      {
        "tag": "gemma3:12b-cloud",
        "size": "-",
        "context_window": "32K",
        "input_type": "Text, Image"
      },
      {
        "tag": "gemma3:27b-cloud",
        "size": "-",
        "context_window": "128K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "This model requires Ollama 0.6 or later.\nDownload Ollama\nGemma is a lightweight, family of models from Google built on Gemini technology. The Gemma 3 models are multimodal‚Äîprocessing text and images‚Äîand feature a 128K context window with support for over 140 languages. Available in 270M, 1B, 4B, 12B, and 27B parameter sizes, they excel in tasks like question answering, summarization, and reasoning, while their compact design allows deployment on resource-limited devices.\nModels\nText\n270M parameter model\n(32k context window)\nollama run gemma3:270m\n1B parameter model\n(32k context window)\nollama run gemma3:1b\nMultimodal (Vision)\n4B parameter model\n(128k context window)\nollama run gemma3:4b\n12B parameter model\n(128k context window)\nollama run gemma3:12b\n27B parameter model\n(128k context window)\nollama run gemma3:27b\nQuantization aware trained models (QAT)\nThe quantization aware trained Gemma 3 models preserves similar quality as half precision models (BF16) while maintaining a lower memory footprint (3x less compared to non-quantized models).\n1B parameter model\nollama run gemma3:1b-it-qat\n4B parameter model\nollama run gemma3:4b-it-qat\n12B parameter model\nollama run gemma3:12b-it-qat\n27B parameter model\nollama run gemma3:27b-it-qat\nEvaluation\nBenchmark Results\nGemma 3 270M\nBenchmark\nn-shot\nGemma 3 270m instruction tuned\nHellaSwag\n0-shot\n37.7\nPIQA\n0-shot\n66.2\nARC-c\n0-shot\n28.2\nWinoGrande\n0-shot\n52.3\nBIG-Bench Hard\nfew-shot\n26.7\nIF Eval\n0-shot\n51.2\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nReasoning, logic and code capabilities\nBenchmark\nMetric\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\n28.4\n50.9\n72.6\n77.7\nDROP\n3-shot, F1\n42.4\n60.1\n72.2\n77.2\nAGIEval\n3-5-shot\n22.2\n42.1\n57.4\n66.2\nMMLU\n5-shot, top-1\n26.5\n59.6\n74.5\n78.6\nMATH\n4-shot\n‚Äì\n24.2\n43.3\n50.0\nGSM8K\n5-shot, maj@1\n1.36\n38.4\n71.0\n82.6\nGPQA\n9.38\n15.0\n25.4\n24.3\nMMLU\n(Pro)\n5-shot\n11.2\n23.7\n40.8\n43.9\nMBPP\n3-shot\n9.80\n46.0\n60.4\n65.6\nHumanEval\npass@1\n6.10\n36.0\n45.7\n48.8\nMMLU\n(Pro COT)\n5-shot\n9.7\nNaN\nNaN\nNaN\nMultilingual capabilities\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nBelebele\n26.6\n59.4\n78.0\n‚Äì\nWMT24++\n(ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXL-Sum\n4.82\n8.55\n12.2\n14.9\nXQuAD\n(all)\n43.9\n68.0\n74.5\n76.8\nMultimodal capabilities\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA\n(val)\n72.8\n82.3\n85.6\nInfoVQA\n(val)\n44.1\n54.8\n59.4\nMMMU\n(pt)\n39.2\n50.3\n56.1\nTextVQA\n(val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n45.4\n60.9\n63.8\nChartQA\n(augmented)\n81.8\n88.5\n88.7\nVQAv2\n‚Äì\n‚Äì\n‚Äì\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nReference\nGemma Terms of Use"
  },
  {
    "slug": "llama2",
    "name": "llama2",
    "description": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
    "pull_count": "5.5M",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "llama2:latest",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama2:7b",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama2:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama2:70b",
        "size": "39GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Llama 2 is released by Meta Platforms, Inc. This model is trained on 2 trillion tokens, and by default supports a context length of 4096. Llama 2 Chat models are fine-tuned on over 1 million human annotations, and are made for chat.\nCLI\nOpen the terminal and run\nollama run llama2\nAPI\nExample using curl:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\nAPI documentation\nMemory requirements\n7b models generally require at least 8GB of RAM\n13b models generally require at least 16GB of RAM\n70b models generally require at least 64GB of RAM\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\nModel variants\nChat\nis fine-tuned for chat/dialogue use cases. These are the default in Ollama, and for models tagged with -chat in the tags tab.\nExample:\nollama run llama2\nPre-trained\nis without the chat fine-tuning. This is tagged as -text in the tags tab.\nExample:\nollama run llama2:text\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\nReferences\nLlama 2: Open Foundation and Fine-Tuned Chat Models\nMeta‚Äôs Hugging Face repo"
  },
  {
    "slug": "minicpm-v",
    "name": "minicpm-v",
    "description": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
    "pull_count": "4.6M",
    "updated": "1 year ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "minicpm-v:latest",
        "size": "5.5GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "minicpm-v:8b",
        "size": "5.5GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires\nOllama 0.3.10\nor later.\nMiniCPM-V 2.6 is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include:\nüî• Leading Performance\n: MiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet for single image understanding.\nüñºÔ∏è Multi Image Understanding and In-context Learning\n. MiniCPM-V 2.6 can also perform conversation and reasoning over multiple images. It achieves state-of-the-art performance on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability.\nüí™ Strong OCR Capability\n: MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro. Based on the the latest RLAIF-V and VisCPM techniques, it features trustworthy behaviors, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports multilingual capabilities on English, Chinese, German, French, Italian, Korean, etc.\nüöÄ Superior Efficiency\n: In addition to its friendly size, MiniCPM-V 2.6 also shows state-of-the-art token density (i.e., number of pixels encoded into each visual token). It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models. This directly improves the inference speed, first-token latency, memory usage, and power consumption.\nRefrences\nGitHub\nHugging Face"
  },
  {
    "slug": "tinyllama",
    "name": "tinyllama",
    "description": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
    "pull_count": "3.7M",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "tinyllama:latest",
        "size": "638MB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "tinyllama:1.1b",
        "size": "638MB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "TinyLlama is a compact model with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\nReferences\nHugging Face\nGitHub"
  },
  {
    "slug": "codellama",
    "name": "codellama",
    "description": "A large language model that can use text prompts to generate and discuss code.",
    "pull_count": "4.3M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "codellama:latest",
        "size": "3.8GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "codellama:7b",
        "size": "3.8GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "codellama:13b",
        "size": "7.4GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "codellama:34b",
        "size": "19GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "codellama:70b",
        "size": "39GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Code Llama is a model for generating and discussing code, built on top of\nLlama 2\n. It‚Äôs designed to make workflows faster and efficient for developers and make it easier for people to learn how to code. It can generate both code and natural language about code. Code Llama supports many of the most popular programming languages used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, Bash and more.\nParameter counts\nParameter Count\n7 billion\nView\nollama run codellama:7b\n13 billion\nView\nollama run codellama:13b\n34 billion\nView\nollama run codellama:34b\n70 billion\nView\nollama run codellama:70b\nUsage\nCLI\nollama run codellama \"Write me a function that outputs the fibonacci sequence\"\nAPI\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"codellama\",\n  \"prompt\": \"Write me a function that outputs the fibonacci sequence\"\n}'\nVariations\ninstruct\nFine-tuned to generate helpful and safe answers in natural language\npython\nA specialized variation of Code Llama further fine-tuned on 100B tokens of Python code\ncode\nBase model for code completion\nExample prompts\nAsk questions\nollama run codellama:7b-instruct 'You are an expert programmer that writes simple, concise code and explanations. Write a python function to generate the nth fibonacci number.'\nFill-in-the-middle (FIM) or infill\nollama run codellama:7b-code '<PRE> def compute_gcd(x, y): <SUF>return result <MID>'\nFill-in-the-middle (FIM) is a special prompt format supported by the code completion model can complete code between two already written code blocks. Code Llama expects a specific format for infilling code:\n<PRE> {prefix} <SUF>{suffix} <MID>\n<PRE>\n,\n<SUF>\nand\n<MID>\nare special tokens that guide the model.\nCode review\nollama run codellama '\nWhere is the bug in this code?\n\ndef fib(n):\n    if n <= 0:\n        return n\n    else:\n        return fib(n-1) + fib(n-2)\n'\nWriting tests\nollama run codellama \"write a unit test for this function: $(cat example.py)\"\nCode completion\nollama run codellama:7b-code '# A simple python function to remove whitespace from a string:'\nMore information\nHow to prompt Code Llama\nWhitepaper\nCodeLlama GitHub\nHugging Face"
  },
  {
    "slug": "llama3.2-vision",
    "name": "llama3.2-vision",
    "description": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.",
    "pull_count": "3.8M",
    "updated": "9 months ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "llama3.2-vision:latest",
        "size": "7.8GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "llama3.2-vision:11b",
        "size": "7.8GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "llama3.2-vision:90b",
        "size": "55GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\nSupported Languages: For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported.\nUsage\nFirst, pull the model:\nollama pull llama3.2-vision\nPython Library\nTo use Llama 3.2 Vision with the Ollama\nPython library\n:\nimport ollama\n\nresponse = ollama.chat(\n    model='llama3.2-vision',\n    messages=[{\n        'role': 'user',\n        'content': 'What is in this image?',\n        'images': ['image.jpg']\n    }]\n)\n\nprint(response)\nJavaScript Library\nTo use Llama 3.2 Vision with the Ollama\nJavaScript library\n:\nimport ollama from 'ollama'\n\nconst response = await ollama.chat({\n  model: 'llama3.2-vision',\n  messages: [{\n    role: 'user',\n    content: 'What is in this image?',\n    images: ['image.jpg']\n  }]\n})\n\nconsole.log(response)\ncURL\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2-vision\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is in this image?\",\n      \"images\": [\"<base64-encoded image data>\"]\n    }\n  ]\n}'\nReferences\nGitHub\nHuggingFace"
  },
  {
    "slug": "qwen2",
    "name": "qwen2",
    "description": "Qwen2 is a new series of large language models from Alibaba group",
    "pull_count": "4.9M",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "qwen2:latest",
        "size": "4.4GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2:0.5b",
        "size": "352MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2:1.5b",
        "size": "935MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2:7b",
        "size": "4.4GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2:72b",
        "size": "41GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Qwen2\nis trained on data in\n29 languages\n, including\nEnglish and Chinese\n.\nIt is available in 4 parameter sizes:\n0.5B\n,\n1.5B\n,\n7B\n,\n72B\n.\nIn the 7B and 72B models, context length has been extended to\n128k tokens\n.\nModels\nQwen2-0.5B\nQwen2-1.5B\nQwen2-7B\nQwen2-72B\nParams\n0.49B\n1.54B\n7.07B\n72.71B\nNon-Emb Params\n0.35B\n1.31B\n5.98B\n70.21B\nGQA\nTrue\nTrue\nTrue\nTrue\nTie Embedding\nTrue\nTrue\nFalse\nFalse\nContext Length\n32K\n32K\n128K\n128K\nSupported languages\nThis is in addition to English and Chinese\nRegions\nLanguages\nWestern Europe\nGerman, French, Spanish, Portuguese, Italian, Dutch\nEastern & Central Europe\nRussian, Czech, Polish\nMiddle East\nArabic, Persian, Hebrew, Turkish\nEastern Asia\nJapanese, Korean\nSouth-Eastern Asia\nVietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog\nSouthern Asia\nHindi, Bengali, Urdu\nPerformance\nLicense\nAll models with the exception of Qwen2 72B (both instruct and base models) are Apache 2.0 licensed.\nQwen2 72B model still uses the original Qianwen License."
  },
  {
    "slug": "deepseek-v3",
    "name": "deepseek-v3",
    "description": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
    "pull_count": "3.5M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "deepseek-v3:latest",
        "size": "404GB",
        "context_window": "160K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-v3:671b",
        "size": "404GB",
        "context_window": "160K",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires\nOllama 0.5.5\nor later.\nDeepSeek-V3 achieves a significant breakthrough in inference speed over previous models. It tops the leaderboard among open-source models and rivals the most advanced closed-source models globally.\nReferences\nGitHub\nPaper"
  },
  {
    "slug": "dolphin3",
    "name": "dolphin3",
    "description": "Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.",
    "pull_count": "3.6M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "dolphin3:latest",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "dolphin3:8b",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Dolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models. Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\nDolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini. But these models present problems for businesses seeking to include AI in their products.\nThey maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break.\nThey maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on.\nThey maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application.\nThey can see all your queries and they can potentially use that data in ways you wouldn‚Äôt want. Dolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt. You decide the alignment. You have control of your data. Dolphin does not impose its ethics or guidelines on you. You are the one who decides the guidelines.\nReferences\nHugging Face"
  },
  {
    "slug": "olmo2",
    "name": "olmo2",
    "description": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.",
    "pull_count": "3.5M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "olmo2:latest",
        "size": "4.5GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "olmo2:7b",
        "size": "4.5GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "olmo2:13b",
        "size": "8.4GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires\nOllama 0.5.5\nOLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.\nReferences\nBlog post"
  },
  {
    "slug": "bge-m3",
    "name": "bge-m3",
    "description": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.",
    "pull_count": "3.3M",
    "updated": "1 year ago",
    "capabilities": [
      "embedding",
      "567m"
    ],
    "tags": [
      {
        "tag": "bge-m3:latest",
        "size": "1.2GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "bge-m3:567m",
        "size": "1.2GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "BGE-M3 is based on the XLM-RoBERTa architecture and is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity:\nMulti-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\nMulti-Linguality: It can support more than 100 working languages.\nMulti-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.\nBenchmarks from the open-source community\n@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "slug": "qwen3-coder",
    "name": "qwen3-coder",
    "description": "Alibaba's performant long context models for agentic and coding tasks.",
    "pull_count": "3.2M",
    "updated": "5 months ago",
    "capabilities": [
      "tools",
      "cloud"
    ],
    "tags": [
      {
        "tag": "qwen3-coder:latest",
        "size": "19GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-coder:30b",
        "size": "19GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-coder:480b",
        "size": "290GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-coder:480b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text"
      }
    ],
    "readme": "Qwen3-Coder\nis the most agentic code model to date in the Qwen series.\nGet started\n480B\nCloud\nollama run qwen3-coder:480b-cloud\nLocal\nollama run qwen3-coder:480b\nRunning locally requires a minimum of 250GB of memory or unified memory.\n30B\nollama run qwen3-coder:30b\nOverview\nqwen3-coder:30b\noffers 30B total parameters with only 3.3B activated, delivering strong performance while maintaining efficiency.\nExceptional agentic capabilities for real-world software engineering tasks through advanced long-horizon reinforcement learning on SWE-Bench and similar benchmarks.\nLong context support with 256K tokens natively and up to 1M tokens using extrapolation methods, optimized for repository-scale understanding.\nScaled pretraining on 7.5T tokens (70% code ratio) while preserving strong general and mathematical abilities.\nExecution-driven reinforcement learning that significantly boosts code execution success rates across diverse real-world coding tasks.\nReference\nBlog"
  },
  {
    "slug": "all-minilm",
    "name": "all-minilm",
    "description": "Embedding models on very large sentence level datasets.",
    "pull_count": "2.5M",
    "updated": "1 year ago",
    "capabilities": [
      "embedding",
      "22m",
      "33m"
    ],
    "tags": [
      {
        "tag": "all-minilm:latest",
        "size": "46MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "all-minilm:22m",
        "size": "46MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "all-minilm:33m",
        "size": "67MB",
        "context_window": "512",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires Ollama 0.1.26 or later.\nDownload it here\n. It can only be used to generate embeddings.\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised contrastive learning objective.\nUsage\nREST API\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"all-minilm\",\n  \"prompt\": \"The sky is blue because of Rayleigh scattering\"\n}'\nPython library\nollama.embeddings(model='all-minilm', prompt='The sky is blue because of Rayleigh scattering')\nJavascript library\nollama.embeddings({ model: 'all-minilm', prompt: 'The sky is blue because of Rayleigh scattering' })\nReferences\nHuggingFace\nWebsite"
  },
  {
    "slug": "codegemma",
    "name": "codegemma",
    "description": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.",
    "pull_count": "2.2M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "codegemma:latest",
        "size": "5.0GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "codegemma:2b",
        "size": "1.6GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "codegemma:7b",
        "size": "5.0GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.\nVariants:\ninstruct\na 7b instruction-tuned variant for natural language-to-code chat and instruction following\ncode\na 7b pretrained variant that specializes in code completion and generation from code prefixes and/or suffixes\n2b\na state of the art 2B pretrained variant that provides up to 2x faster code completion\nAdvantages:\nIntelligent code completion and generation\n: Complete lines, functions, and even generate entire blocks of code, whether you‚Äôre working locally or using Google Cloud resources.\nEnhanced accuracy\n: Trained on 500 billion tokens of primarily English language data from web documents, mathematics, and code, CodeGemma models generate code that‚Äôs not only more syntactically correct but also semantically meaningful, reducing errors and debugging time.\nMulti-language proficiency\n: Supports Python, JavaScript, Java, Kotlin, C++, C#, Rust, Go, and other languages.\nStreamlined workflows\n: Integrate a CodeGemma model into your development environment to write less boilerplate and focus on interesting and differentiated code that matters, faster.\nFill-in-the-middle\nCodeGemma models support fill-in-the-middle (FIM), for use in autocomplete or coding assistant tooling. Below is an example using the Ollama\nPython\nlibrary:\nresponse = generate(\n  model='codegemma:2b-code',\n  prompt=f'<|fim_prefix|>{prefix}<|fim_suffix|>{suffix}<|fim_middle|>',\n  options={\n    'num_predict': 128,\n    'temperature': 0,\n    'top_p': 0.9,\n    'stop': ['<|file_separator|>'],\n  },\n)\nReferences\nHugging Face\nReport"
  },
  {
    "slug": "llama3.3",
    "name": "llama3.3",
    "description": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.",
    "pull_count": "3.3M",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "llama3.3:latest",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "llama3.3:70b",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "New state-of-the-art 70B\nmodel from Meta that offers similar performance compared to Llama 3.1 405B model.\nThe Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\nSupported languages\n: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\nNew capabilities\nThis release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\nTool-use:\nJust like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\nMultilinguality:\nLlama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\nIntended Use\nIntended Use Cases\nLlama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.\nOut-of-scope\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\nNote:\nLlama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner."
  },
  {
    "slug": "mistral-small",
    "name": "mistral-small",
    "description": "Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.",
    "pull_count": "2.4M",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "mistral-small:latest",
        "size": "14GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "mistral-small:22b",
        "size": "13GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "mistral-small:24b",
        "size": "14GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models.\nMistral Small can be deployed locally and is exceptionally ‚Äúknowledge-dense‚Äù, fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized.\nPerfect for:\nFast response conversational agents.\nLow latency function calling.\nSubject matter experts via fine-tuning.\nLocal inference for hobbyists and organizations handling sensitive data.\nKey Features\nMultilingual:\nSupports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\nAgent-Centric:\nOffers best-in-class agentic capabilities with native function calling and JSON outputting.\nAdvanced Reasoning:\nState-of-the-art conversational and reasoning capabilities.\nApache 2.0 License:\nOpen license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window:\nA 32k context window.\nSystem Prompt:\nMaintains strong adherence and support for system prompts.\nTokenizer:\nUtilizes a Tekken tokenizer with a 131k vocabulary size.\nHuman Evaluations\nWe conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts. Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model. We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.\nInstruct performance\nOur instruction tuned model performs competitively with open weight models three times its size and with proprietary GPT4o-mini model across Code, Math, General knowledge and Instruction following benchmarks.\nPerformance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance (Qwen2.5-32B-Instruct, Llama-3.3-70B-Instruct, Gemma-2-27B-IT). Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.\nCustomers are evaluating Mistral Small 3 across multiple industries, including:\nFinancial services customers for fraud detection\nHealthcare providers for customer triaging\nRobotics, automotive, and manufacturing companies for on-device command and control\nHorizontal use cases across customers include virtual customer service, and sentiment and feedback analysis."
  },
  {
    "slug": "falcon3",
    "name": "falcon3",
    "description": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.",
    "pull_count": "2.2M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "falcon3:latest",
        "size": "4.6GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "falcon3:1b",
        "size": "1.8GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "falcon3:3b",
        "size": "2.0GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "falcon3:7b",
        "size": "4.6GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "falcon3:10b",
        "size": "6.3GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Falcon3 represents TII‚Äôs latest advancement in efficient language models under 10B parameters, focused on enhancing science, math, and code capabilities while maintaining training efficiency.\nKey Features\nFour sizes: 1B, 3B, 7B, 10B\nDepth up-scaling technique used to create 10B model from 7B\nKnowledge distillation for smaller models (1B, 3B)\nPerformance Highlights\nfalcon3:1b\noutperforms\nsmollm2:1.7b\n, matches\ngemma2:2b\nfalcon3:10b\nachieves SOTA in under-13B category\nExtended context length up to 32K tokens (8K for 1B model)\nReferences\nHugging Face"
  },
  {
    "slug": "llava-llama3",
    "name": "llava-llama3",
    "description": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
    "pull_count": "2.1M",
    "updated": "1 year ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "llava-llama3:latest",
        "size": "5.5GB",
        "context_window": "8K",
        "input_type": "Text, Image"
      },
      {
        "tag": "llava-llama3:8b",
        "size": "5.5GB",
        "context_window": "8K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "llava-llama3\nis a LLaVA model fine-tuned from Llama 3 Instruct and CLIP-ViT-Large-patch14-336 with ShareGPT4V-PT and InternVL-SFT by XTuner.\nReferences\nHugging Face\nGitHub"
  },
  {
    "slug": "snowflake-arctic-embed",
    "name": "snowflake-arctic-embed",
    "description": "A suite of text embedding models by Snowflake, optimized for performance.",
    "pull_count": "2.1M",
    "updated": "1 year ago",
    "capabilities": [
      "embedding",
      "22m",
      "33m",
      "110m",
      "137m",
      "335m"
    ],
    "tags": [
      {
        "tag": "snowflake-arctic-embed:latest",
        "size": "669MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "snowflake-arctic-embed:22m",
        "size": "46MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "snowflake-arctic-embed:33m",
        "size": "67MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "snowflake-arctic-embed:110m",
        "size": "219MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "snowflake-arctic-embed:137m",
        "size": "274MB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "snowflake-arctic-embed:335m",
        "size": "669MB",
        "context_window": "512",
        "input_type": "Text"
      }
    ],
    "readme": "snowflake-arctic-embed\nis a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance.\nThe models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance.\nThis model is available in 5 parameter sizes:\nsnowflake-arctic-embed:335m\n(default)\nsnowflake-arctic-embed:137m\nsnowflake-arctic-embed:110m\nsnowflake-arctic-embed:33m\nsnowflake-arctic-embed:22m\nReference\nBlog Post\nHuggingFace"
  },
  {
    "slug": "qwq",
    "name": "qwq",
    "description": "QwQ is the reasoning model of the Qwen series.",
    "pull_count": "2.1M",
    "updated": "11 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "qwq:latest",
        "size": "20GB",
        "context_window": "40K",
        "input_type": "Text"
      },
      {
        "tag": "qwq:32b",
        "size": "20GB",
        "context_window": "40K",
        "input_type": "Text"
      }
    ],
    "readme": "QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.\nFuture Work\nThis marks Qwen‚Äôs initial step in scaling Reinforcement Learning (RL) to enhance reasoning capabilities. Through this journey, we have not only witnessed the immense potential of scaled RL but also recognized the untapped possibilities within pretrained language models. As we work towards developing the next generation of Qwen, we are confident that combining stronger foundation models with RL powered by scaled computational resources will propel us closer to achieving Artificial General Intelligence (AGI). Additionally, we are actively exploring the integration of agents with RL to enable long-horizon reasoning, aiming to unlock greater intelligence with inference time scaling.\nReference\nBlog"
  },
  {
    "slug": "orca-mini",
    "name": "orca-mini",
    "description": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
    "pull_count": "2M",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "orca-mini:latest",
        "size": "2.0GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "orca-mini:3b",
        "size": "2.0GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "orca-mini:7b",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "orca-mini:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "orca-mini:70b",
        "size": "39GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Orca Mini is a Llama and Llama 2 model trained on Orca Style datasets created using the approaches defined in the paper, Orca: Progressive Learning from Complex Explanation Traces of GPT-4. There are two variations available. The original Orca Mini based on Llama in 3, 7, and 13 billion parameter sizes, and v3 based on Llama 2 in 7, 13, and 70 billion parameter sizes.\nUsage\nCLI\nOpen the terminal and run\nollama run orca-mini\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n    \"model\": \"orca-mini\",\n    \"prompt\":\"Why is the sky blue?\"\n   }'\nMemory requirements\n7b models generally require at least 8GB of RAM\n13b models generally require at least 16GB of RAM\n70b models generally require at least 64GB of RAM\nReference\n3b parameters original source:\nPankaj Mathur\n7b parameters original source:\nPankaj Mathur\n13b parameters original source:\nPankaj Mathur\nOrca Mini v3 source on Ollama\n13b parameters original source:\nPankaj Mathur\n70b parameters source:\nPankaj Mathur\nOrca: Progressive Learning from Complex Explanation Traces of GPT-4"
  },
  {
    "slug": "mistral-nemo",
    "name": "mistral-nemo",
    "description": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.",
    "pull_count": "3.4M",
    "updated": "7 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "mistral-nemo:latest",
        "size": "7.1GB",
        "context_window": "1000K",
        "input_type": "Text"
      },
      {
        "tag": "mistral-nemo:12b",
        "size": "7.1GB",
        "context_window": "1000K",
        "input_type": "Text"
      }
    ],
    "readme": "Mistral NeMo is a 12B model built in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B.\nReference\nBlog\nHugging Face"
  },
  {
    "slug": "deepseek-coder",
    "name": "deepseek-coder",
    "description": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
    "pull_count": "3.1M",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "deepseek-coder:latest",
        "size": "776MB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-coder:1.3b",
        "size": "776MB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-coder:6.7b",
        "size": "3.8GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-coder:33b",
        "size": "19GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "DeepSeek Coder is trained from scratch on both 87% code and 13% natural language in English and Chinese. Each of the models are pre-trained on 2 trillion tokens.\nModels available\n1.3 billion parameter model\nollama run deepseek-coder\n6.7 billion parameter model\nollama run deepseek-coder:6.7b\n33 billion parameter model\nollama run deepseek-coder:33b\nCLI\nOpen the terminal and run\nollama run deepseek-coder\nAPI\nExample using curl:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"deepseek-coder\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\nAPI documentation\nReferences\nHuggingFace"
  },
  {
    "slug": "smollm2",
    "name": "smollm2",
    "description": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.",
    "pull_count": "2.5M",
    "updated": "1 year ago",
    "capabilities": [
      "tools",
      "135m",
      "360m"
    ],
    "tags": [
      {
        "tag": "smollm2:latest",
        "size": "1.8GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "smollm2:135m",
        "size": "271MB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "smollm2:360m",
        "size": "726MB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "smollm2:1.7b",
        "size": "1.8GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device.\nReferences\nHuggingFace"
  },
  {
    "slug": "llama2-uncensored",
    "name": "llama2-uncensored",
    "description": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
    "pull_count": "1.8M",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "llama2-uncensored:latest",
        "size": "3.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "llama2-uncensored:7b",
        "size": "3.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "llama2-uncensored:70b",
        "size": "39GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Llama 2 Uncensored is based on Meta‚Äôs Llama 2 model, and was created by George Sung and Jarrad Hope using the process defined by Eric Hartford in his\nblog post\n.\nCLI\nOpen the terminal and run\nollama run llama2-uncensored\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2-uncensored\",\n  \"prompt\":\"Write a recipe for dangerously spicy mayo.\"\n }'\nAPI documentation\nMemory requirements\n7b models generally require at least 8GB of RAM\n70b models generally require at least 64GB of RAM\nReferences\nhttps://erichartford.com/uncensored-models\n7b parameters original source:\nGeorge Sung\n70b parameters original source:\nGeorge Sung"
  },
  {
    "slug": "deepseek-coder-v2",
    "name": "deepseek-coder-v2",
    "description": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.",
    "pull_count": "1.6M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "deepseek-coder-v2:latest",
        "size": "8.9GB",
        "context_window": "160K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-coder-v2:16b",
        "size": "8.9GB",
        "context_window": "160K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-coder-v2:236b",
        "size": "133GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "DeepSeek-Coder-V2 is an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. DeepSeek-Coder-V2 is further pre-trained from DeepSeek-Coder-V2-Base with 6 trillion tokens sourced from a high-quality and multi-source corpus.\nReferences\nHugging Face"
  },
  {
    "slug": "granite3.1-moe",
    "name": "granite3.1-moe",
    "description": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
    "pull_count": "2.2M",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "granite3.1-moe:latest",
        "size": "2.0GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite3.1-moe:1b",
        "size": "1.4GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite3.1-moe:3b",
        "size": "2.0GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Granite mixture of experts models\nThe IBM Granite\n1B and 3B models\nare long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.\nThe models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference.\nParameter Sizes\n1B:\nollama run granite3.1-moe:1b\n3B:\nollama run granite3.1-moe:3b\nSupported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\nCapabilities\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related tasks\nFunction-calling tasks\nMultilingual dialog use cases\nLong-context tasks including long document/meeting summarization, long document QA, etc.\nGranite dense models\nThe Granite dense models are available in\n2B and 8B\nparameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\nSee model page\nLearn more\nDevelopers:\nIBM Research\nGitHub Repository:\nibm-granite/granite-language-models\nWebsite\n:\nGranite Docs\nRelease Date\n: December 18th, 2024\nLicense:\nApache 2.0\n."
  },
  {
    "slug": "qwen3-vl",
    "name": "qwen3-vl",
    "description": "The most powerful vision-language model in the Qwen model family to date.",
    "pull_count": "1.6M",
    "updated": "4 months ago",
    "capabilities": [
      "vision",
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "qwen3-vl:latest",
        "size": "6.1GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3-vl:2b",
        "size": "1.9GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3-vl:4b",
        "size": "3.3GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3-vl:8b",
        "size": "6.1GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3-vl:30b",
        "size": "20GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3-vl:32b",
        "size": "21GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3-vl:235b",
        "size": "143GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3-vl:235b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3-vl:235b-instruct-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Qwen3-VL models require\nOllama 0.12.7\nQwen3-VL is the most powerful vision-language model in the Qwen family to date.\nIn this generation, there are improvements to the model in many areas: its understanding and generating text, perceiving and reasoning about visual content, supporting longer context lengths, understanding spatial relationships and dynamic videos, or interacting with AI agents ‚Äî Qwen3-VL shows clear and significant progress in every area.\nModels\n2B\nollama run qwen3-vl:2b\n4B\nollama run qwen3-vl:4b\n8B\nollama run qwen3-vl:8b\n30B\nollama run qwen3-vl:30b\n32B\nollama run qwen3-vl:32b\n235B\nollama run qwen3-vl:235b\nollama run qwen3-vl:235b-cloud\nKey features\nVisual Agent Capabilities\n: Qwen3-VL can operate computer and mobile interfaces ‚Äî recognize GUI elements, understand button functions, call tools, and complete tasks. It achieves top global performance on benchmarks like OS World, and using tools significantly improves its performance on fine-grained perception tasks.\nSuperior Text-Centric Performance\n: Qwen3-VL employs early-stage joint pretraining of text and visual modalities, continuously strengthening its language capabilities. Its performance on text-based tasks matches that of Qwen3-235B-A22B-2507 ‚Äî the flagship language model ‚Äî making it a truly ‚Äútext-grounded, multimodal powerhouse‚Äù for the next generation of vision-language models.\nGreatly Improved Visual Coding\n: It can now generate code from images or videos ‚Äî for example, turning a design mockup into Draw.io, HTML, CSS, or JavaScript code ‚Äî making ‚Äúwhat you see is what you get‚Äù visual programming a reality.\nMuch Better Spatial Understanding\n: 2D grounding from absolute coordinates to relative coordinates. It can judge object positions, viewpoint changes, and occlusion relationships. It supports 3D grounding, laying the foundation for complex spatial reasoning and embodied AI applications.\nLong Context & Long Video Understanding\n: All models natively support 256K tokens of context, expandable up to 1 million tokens. This means you can input hundreds of pages of technical documents, entire textbooks, or even two-hour videos ‚Äî and the model will remember everything and retrieve details accurately, down to the exact second in videos.\nStronger Multimodal Reasoning (Thinking Version)\n: The Thinking model is specially optimized for STEM and math reasoning. When facing complex subject questions, it can notice fine details, break down problems step by step, analyze cause and effect, and give logical, evidence-based answers. It achieves strong performance on reasoning benchmarks like MathVision, MMMU, and MathVista.\nUpgraded Visual Perception & Recognition\n: By improving the quality and diversity of pre-training data, the model can now recognize a much wider range of objects ‚Äî from celebrities, anime characters, products, and landmarks, to animals and plants ‚Äî covering both everyday life and professional ‚Äúrecognize anything‚Äù needs.\nBetter OCR Across More Languages & Complex Scenes\n: OCR now supports 32 languages (up from 10), covering more countries and regions. It performs more reliably under challenging real-world conditions like poor lighting, blur, or tilted text. Recognition accuracy for rare characters, ancient scripts, and technical terms has also improved significantly. Its ability to understand long documents and reconstruct fine structures is further enhanced."
  },
  {
    "slug": "cogito",
    "name": "cogito",
    "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.",
    "pull_count": "1.4M",
    "updated": "10 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "cogito:latest",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "cogito:3b",
        "size": "2.2GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "cogito:8b",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "cogito:14b",
        "size": "9.0GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "cogito:32b",
        "size": "20GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "cogito:70b",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "The Cogito v1 Preview LLMs are instruction tuned generative models (text in/text out). All models are released under an open license for commercial use.\nCogito models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models).\nThe LLMs are trained using\nIterated Distillation and Amplification (IDA)\n- an scalable and efficient alignment strategy for superintelligence using iterative self-improvement.\nThe models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts.\nIn both standard and reasoning modes, Cogito v1-preview models outperform their size equivalent counterparts on common industry benchmarks.\nEach model is trained in over 30 languages and supports a context length of 128k.\nExtended thinking\nTo enable extended thinking, include\nEnable deep thinking subroutine.\nin the system prompt:\n/set system \"\"\"Enable deep thinking subroutine.\"\"\"\nOr via the API:\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"cogito\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Enable deep thinking subroutine.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"How many letter Rs are in the word Strawberry?\"\n    }\n  ]\n}'\nSizes\n3B\nollama run cogito:3b\n8B\nollama run cogito:8b\n14B\nollama run cogito:14b\n32B\nollama run cogito:32b\n70B\nollama run cogito:70b\nBenchmarks\nSmaller models - 3B and 8B\n3B performance\n8B performance\n3B tool calling\nMedium models - 14B and 32B\n14B\n32B\nLarger models - 70B\nReferences\nBlog post\nHugging Face"
  },
  {
    "slug": "mistral-small3.2",
    "name": "mistral-small3.2",
    "description": "An update to Mistral Small that improves on function calling, instruction following, and less repetition errors.",
    "pull_count": "1.3M",
    "updated": "8 months ago",
    "capabilities": [
      "vision",
      "tools"
    ],
    "tags": [
      {
        "tag": "mistral-small3.2:latest",
        "size": "15GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "mistral-small3.2:24b",
        "size": "15GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.\nSmall-3.2 improves in the following categories:\nInstruction following\n: Small-3.2 is better at following precise instructions\nRepetition errors\n: Small-3.2 produces less infinite generations or repetitive answers\nFunction calling\n: Small-3.2‚Äôs function calling template is more robust (see here and examples)\nIn all other categories Small-3.2 should match or slightly improve compared to Mistral-Small-3.1-24B-Instruct-2503.\nReferences\nHugging Face"
  },
  {
    "slug": "starcoder2",
    "name": "starcoder2",
    "description": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters.",
    "pull_count": "2.1M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "starcoder2:latest",
        "size": "1.7GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "starcoder2:3b",
        "size": "1.7GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "starcoder2:7b",
        "size": "4.0GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "starcoder2:15b",
        "size": "9.1GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Supporting a context window of up to 16,384 tokens, StarCoder2 is the next generation of transparently trained open code LLMs.\nstarcoder2:instruct\n: a 15B model that follows natural and human-written instructions\nstarcoder2:15b\nwas trained on 600+ programming languages and 4+ trillion tokens.\nstarcoder2:7b\nwas trained on 17 programming languages and 3.5+ trillion tokens.\nstarcoder2:3b\nwas trained on 17 programming languages and 3+ trillion tokens.\nStarCoder2-15B is the best in its size class and matches 33B+ models on many evaluations. StarCoder2-3B matches the performance of StarCoder1-15B.\nReferences\nGitHub\nHuggingFace\nPaper"
  },
  {
    "slug": "qwen2.5vl",
    "name": "qwen2.5vl",
    "description": "Flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.",
    "pull_count": "1.3M",
    "updated": "9 months ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "qwen2.5vl:latest",
        "size": "6.0GB",
        "context_window": "125K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen2.5vl:3b",
        "size": "3.2GB",
        "context_window": "125K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen2.5vl:7b",
        "size": "6.0GB",
        "context_window": "125K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen2.5vl:32b",
        "size": "21GB",
        "context_window": "125K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen2.5vl:72b",
        "size": "49GB",
        "context_window": "125K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Note: this model requires Ollama 0.7.0.\nDownload Ollama\nQwen2.5-VL, the new flagship vision-language model of Qwen and also a significant leap from the previous Qwen2-VL.\nThe key features include:\nUnderstand things visually:\nQwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\nBeing agentic:\nQwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\nCapable of visual localization in different formats:\nQwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\nGenerating structured outputs:\nfor data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\nPerformance\nWe evaluate our models with the SOTA models as well as the best models of similar model sizes. In terms of the flagship model Qwen2.5-VL-72B-Instruct, it achieves competitive performance in a series of benchmarks covering domains and tasks, including college-level problems, math, document understanding, general question answering, math, and visual agent. Notably, Qwen2.5-VL achieves significant advantages in understanding documents and diagrams, and it is capable of playing as a visual agent without task-specific fine tuning.\nIn terms of smaller models, Qwen2.5-VL-7B-Instruct outperforms GPT-4o-mini in a number of tasks, and Qwen2.5-VL-3B, which is a solution for edge AI, even outperforms the 7B model of our previous version Qwen2-VL.\nReferences\nBlog\nQwen2.5-VL Technical Report"
  },
  {
    "slug": "llama4",
    "name": "llama4",
    "description": "Meta's latest collection of multimodal models.",
    "pull_count": "1.2M",
    "updated": "8 months ago",
    "capabilities": [
      "vision",
      "tools"
    ],
    "tags": [
      {
        "tag": "llama4:latest",
        "size": "67GB",
        "context_window": "10M",
        "input_type": "Text, Image"
      },
      {
        "tag": "llama4:16x17b",
        "size": "67GB",
        "context_window": "10M",
        "input_type": "Text, Image"
      },
      {
        "tag": "llama4:128x17b",
        "size": "245GB",
        "context_window": "1M",
        "input_type": "Text, Image"
      }
    ],
    "readme": "The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These two models leverage a mixture-of-experts (MoE) architecture and support native multimodality (image input).\nSupported languages:\nArabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\nInput:\nmultilingual text, image\nOutput:\nmultilingual text, code\nModels\nLlama 4 Scout\nollama run llama4:scout\n109B parameter MoE model with 17B active parameters\nLlama 4 Maverick\nollama run llama4:maverick\n400B parameter MoE model with 17B active parameters\nIntended Use\nIntended Use Cases:\nLlama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.\nOut-of-scope\n: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card.\nNote:\nLlama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes\n200 total languages\n). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\nLlama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\nBenchmarks\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.3 70B\nLlama 3.1 405B\nLlama 4 Scout\nLlama 4 Maverick\nImage Reasoning\nMMMU\n0\naccuracy\nNo multimodal support\n69.4\n73.4\nMMMU Pro^\n0\naccuracy\n52.2\n59.6\nMathVista\n0\naccuracy\n70.7\n73.7\nImage Understanding\nChartQA\n0\nrelaxed_accuracy\n88.8\n90.0\nDocVQA (test)\n0\nanls\n94.4\n94.4\nCode\nLiveCodeBench\n(10/01/2024-02/01/2025)\n0\npass@1\n33.3\n27.7\n32.8\n43.4\nReasoning & Knowledge\nMMLU Pro\n0\nmacro_avg/acc\n68.9\n73.4\n74.3\n80.5\nGPQA Diamond\n0\naccuracy\n50.5\n49.0\n57.2\n69.8\nMultilingual\nMGSM\n0\naverage/em\n91.1\n91.6\n90.6\n92.3\nLong Context\nMTOB (half book) eng->kgv/kgv->eng\n-\nchrF\nContext window is 128K\n42.2 / 36.6\n54.0 / 46.4\nMTOB (full book) eng->kgv/kgv->eng\n-\nchrF\n39.7 / 36.3\n50.8 / 46.7\n*reported numbers for MMMU Pro is the average of Standard and Vision tasks\nReference\nMeta Llama 4\npost"
  },
  {
    "slug": "mixtral",
    "name": "mixtral",
    "description": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
    "pull_count": "1.9M",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "mixtral:latest",
        "size": "26GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "mixtral:8x7b",
        "size": "26GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "mixtral:8x22b",
        "size": "80GB",
        "context_window": "64K",
        "input_type": "Text"
      }
    ],
    "readme": "The Mixtral large Language Models (LLM) are a set of pretrained generative Sparse Mixture of Experts.\nSizes\nmixtral:8x22b\nmixtral:8x7b\nMixtral 8x22b\nollama run mixtral:8x22b\nMixtral 8x22B sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size.\nMixtral 8x22B comes with the following strengths:\nIt is fluent in English, French, Italian, German, and Spanish\nIt has strong maths and coding capabilities\nIt is natively capable of function calling\n64K tokens context window allows precise information recall from large documents\nReferences\nAnnouncement\nHuggingFace"
  },
  {
    "slug": "phi4-reasoning",
    "name": "phi4-reasoning",
    "description": "Phi 4 reasoning and reasoning plus are 14-billion parameter open-weight reasoning models that rival much larger models on complex reasoning tasks.",
    "pull_count": "1.2M",
    "updated": "10 months ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "phi4-reasoning:latest",
        "size": "11GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "phi4-reasoning:14b",
        "size": "11GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Phi 4 reasoning and reasoning plus models are 14 billion parameter models that rival much larger models on complex reasoning tasks.\nPhi 4 reasoning model is trained via supervised fine-tuning of Phi 4 on carefully curated reasoning demonstrations from OpenAI‚Äôs o3-mini. This model demonstrates meticulous data curation and high quality synthetic datasets allow smaller models to compete with larger counterparts.\nPhi 4 reasoning plus model builds on top of Phi 4 reasoning, and is further trained with reinforcement learning to deliver higher accuracy.\nModels\nPhi 4 reasoning\nollama run phi4-reasoning\nPhi 4 reasoning plus\nollama run phi4-reasoning:plus\nBenchmarks\nPhi-4-reasoning performance across representative reasoning benchmarks spanning mathematical and scientific reasoning. We illustrate the performance gains from reasoning-focused post-training of Phi-4 via Phi-4-reasoning (SFT) and Phi-4-reasoning-plus (SFT+RL), alongside a representative set of baselines from two model families: open-weight models from DeepSeek including DeepSeek R1 (671B Mixture-of-Experts) and its distilled dense variant DeepSeek-R1 Distill Llama 70B, and OpenAI‚Äôs proprietary frontier models o1-mini and o3-mini. Phi-4-reasoning and Phi-4-reasoning-plus consistently outperform the base model Phi-4 by significant margins, exceed DeepSeek-R1 Distill Llama 70B (5x larger)‚ÄØand demonstrate competitive performance against significantly larger models such as Deepseek-R1.\nAccuracy of models across general-purpose benchmarks for: long input context QA (FlenQA), instruction following (IFEval), Coding (HumanEvalPlus), knowledge & language understanding (MMLUPro), safety detection (ToxiGen), and other general skills (ArenaHard and PhiBench).\nReferences\nBlog post"
  },
  {
    "slug": "dolphin-phi",
    "name": "dolphin-phi",
    "description": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.",
    "pull_count": "1.2M",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "dolphin-phi:latest",
        "size": "1.6GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "dolphin-phi:2.7b",
        "size": "1.6GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Dolphin Phi 2.6 is an uncensored model based on the 2.7B\nPhi\nmodel by Microsoft Research, using similar datasets as other versions of this model such as\nDolphin Mixtral\n.\nIt was created by\nEric Hartford\nand Cognitive Computations.\nReferences\nHuggingFace"
  },
  {
    "slug": "deepscaler",
    "name": "deepscaler",
    "description": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o1-preview with just 1.5B parameters on popular math evaluations.",
    "pull_count": "1.1M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "deepscaler:latest",
        "size": "3.6GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepscaler:1.5b",
        "size": "3.6GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "DeepScaleR\nüöÄ Democratizing Reinforcement Learning for LLMs üåü\nDeepScaleR-1.5B-Preview is a language model fine-tuned from DeepSeek-R1-Distilled-Qwen-1.5B using distributed reinforcement learning (RL) to scale up to long context lengths. The model achieves 43.1% Pass@1 accuracy on AIME 2024, representing a 15% improvement over the base model (28.8%) and surpassing OpenAI‚Äôs O1-Preview performance with just 1.5B parameters.\nModel\nAIME 2024\nMATH 500\nAMC 2023\nMinerva Math\nOlympiad Bench\nAvg.\nDeepScaleR-1.5B-Preview\n43.1\n87.8\n73.6\n30.2\n50.0\n57.0\nDeepSeek-R1-Distill-Qwen-1.5B\n28.8\n82.8\n62.9\n26.5\n43.3\n48.9\nO1-Preview\n40.0\n81.4\n-\n-\n-\n-\nData\nOur training dataset consists of approximately 40,000 unique problem-answer pairs compiled from:\nAIME problems (1984-2023)\nAMC problems (prior to 2023)\nOmni-MATH dataset\nStill dataset\nEvaluation\nWe evaluate our model on competition-level mathematics benchmarks, including AIME 2024, AMC 2023, MATH-500, Minerva Math, and OlympiadBench. Below, Pass@1 accuracy is reported, averaged over 16 samples for each problem.\nModel\nAIME 2024\nMATH 500\nAMC 2023\nMinerva Math\nOlympiadBench\nAvg.\nQwen-2.5-Math-7B-Instruct\n13.3\n79.8\n50.6\n34.6\n40.7\n43.8\nrStar-Math-7B\n26.7\n78.4\n47.5\n-\n47.1\n-\nEurus-2-7B-PRIME\n26.7\n79.2\n57.8\n38.6\n42.1\n48.9\nQwen2.5-7B-SimpleRL\n26.7\n82.4\n62.5\n39.7\n43.3\n50.9\nDeepSeek-R1-Distill-Qwen-1.5B\n28.8\n82.8\n62.9\n26.5\n43.3\n48.9\nStill-1.5B\n32.5\n84.4\n66.7\n29.0\n45.4\n51.6\nDeepScaleR-1.5B-Preview\n43.1\n87.8\n73.6\n30.2\n50.0\n57.0\nO1-Preview\n40.0\n81.4\n-\n-\n-\n-\nWe compare DeepScaleR with the base DeepSeek model we use, as well as recent academic works exploring RL for reasoning tasks. DeepScaleR significantly outperforms the base model across all benchmarks, achieving a 14.4% absolute gain on AIME2024 and an 8.1% overall improvement. Additionally, DeepScaleR surpasses recent academic works such as rSTAR, Prime, and SimpleRL, which are finetuned from 7B models. DeepScaleR achieves O1-preview-level performance with only 1.5B parameters‚Äîa remarkable efficiency gain.\nReferences\nArticle\nGitHub\nHugging Face"
  },
  {
    "slug": "lfm2",
    "name": "lfm2",
    "description": "LFM2 is a family of hybrid models designed for on-device deployment. LFM2-24B-A2B is the largest model in the family, scaling the architecture to 24 billion parameters while keeping inference efficient.",
    "pull_count": "1.6M",
    "updated": "3 days ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "lfm2:latest",
        "size": "14GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "lfm2:24b",
        "size": "14GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "LFM2 is a family of hybrid models designed for on-device deployment. LFM2-24B-A2B is the largest model in the family, scaling the architecture to 24 billion parameters while keeping inference efficient.\nBest-in-class efficiency\n: A 24B MoE model with only 2B active parameters per token, fitting in 32 GB of RAM for deployment on consumer laptops and desktops.\nFast edge inference\n: 112 tok/s decode on AMD CPU, 293 tok/s on H100. Fits in 32B GB of RAM.\nPredictable scaling\n: Quality improves log-linearly from 350M to 24B total parameters, confirming the LFM2 hybrid architecture scales reliably across nearly two orders of magnitude."
  },
  {
    "slug": "dolphin-llama3",
    "name": "dolphin-llama3",
    "description": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, conversational, and coding skills.",
    "pull_count": "1.1M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "dolphin-llama3:latest",
        "size": "4.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "dolphin-llama3:8b",
        "size": "4.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "dolphin-llama3:70b",
        "size": "40GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "üê¨ Dolphin 2.9 Llama 3\nDolphin-2.9 has a variety of instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling.\nDolphin is\nuncensored\n. The dataset has been filtered to remove alignment and bias. This makes the model more compliant.\nCurated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations.\nSizes\ndolphin-llama3:8b\ndolphin-llama3:70b\n256K Context Window\nNote: using a 256k context window requires at least 64GB of memory.\nDolphin Llama 3 also has a 256k context window version. To extend the context window use:\nAPI\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"dolphin-llama3:8b-256k\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 256000\n  }\n}'\nCLI\nollama run dolphin-llama3:8b-256k\n>>> /set parameter num_ctx 256000\nReferences\nHuggingFace"
  },
  {
    "slug": "dolphin-mixtral",
    "name": "dolphin-mixtral",
    "description": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford.",
    "pull_count": "1.1M",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "dolphin-mixtral:latest",
        "size": "26GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "dolphin-mixtral:8x7b",
        "size": "26GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "dolphin-mixtral:8x22b",
        "size": "80GB",
        "context_window": "64K",
        "input_type": "Text"
      }
    ],
    "readme": "The Dolphin model by Eric Hartford based on\nMixtral\nthat is trained with additional datasets:\nSynthia, OpenHermes and PureDove\nNew Dolphin-Coder\nMagiCoder\nSizes\ndolphin-mixtral:8x22b\ndolphin-mixtral:8x7b\nReferences\nHuggingFace"
  },
  {
    "slug": "magistral",
    "name": "magistral",
    "description": "Magistral is a small, efficient reasoning model with 24B parameters.",
    "pull_count": "1.1M",
    "updated": "8 months ago",
    "capabilities": [
      "tools",
      "thinking"
    ],
    "tags": [
      {
        "tag": "magistral:latest",
        "size": "14GB",
        "context_window": "39K",
        "input_type": "Text"
      },
      {
        "tag": "magistral:24b",
        "size": "14GB",
        "context_window": "39K",
        "input_type": "Text"
      }
    ],
    "readme": "Magistral is the first reasoning model by Mistral AI, excelling in domain-specific, transparent, and multilingual reasoning.\nKey features\nReasoning:\nCapable of long chains of reasoning traces before providing an answer.\nMultilingual:\nSupports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.\nApache 2.0 License:\nOpen license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window:\nA 128k context window,\nbut\nperformance might degrade past\n40k\n. Hence we recommend setting the maximum model length to 40k.\nMagistral is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling ‚Äî this model solves multi-step challenges where transparency and precision are critical.\nBusiness strategy and operations.\nBuilding on our flagship\nmodels\n, Magistral is designed for research, strategic planning, operational optimization, and data-driven decision making ‚Äî whether executing risk assessment and modelling with multiple factors, or calculating optimal delivery windows under constraints.\nRegulated industries and sectors.\nLegal, finance, healthcare, and government professionals get traceable reasoning that meets compliance requirements. Every conclusion can be traced back through its logical steps, providing auditability for high-stakes environments with domain-specialized AI.\nSystems, software, and data engineering.\nMagistral enhances coding and development use cases: compared to non-reasoning models, it significantly improves project planning, backend architecture, frontend design, and data engineering through sequenced, multi-step actions involving external tools or API.\nContent and communication.\nOur early tests indicated that Magistral is an excellent creative companion. We highly recommend it for creative writing and storytelling, with the model capable of producing coherent or ‚Äî if needed ‚Äî delightfully eccentric copy.\nReference\nMagistral paper\nBlog post"
  },
  {
    "slug": "phi",
    "name": "phi",
    "description": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding capabilities.",
    "pull_count": "1M",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "phi:latest",
        "size": "1.6GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "phi:2.7b",
        "size": "1.6GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Phi-2 is a small language model capable of common-sense reasoning and language understanding. It showcases ‚Äústate-of-the-art performance‚Äù among language models with less than 13 billion parameters.\nExample prompt\nBy default,\nphi\nincludes a chat prompt template designed for multi-turn conversations:\n% ollama run phi\n>>> Hello, can you help me find my way to Toronto?\n Certainly! What is the exact location in Toronto that you are looking for?\n\n>>> Yonge & Bloor\n Sure, Yonge and Bloor is a busy intersection in downtown Toronto. Would you like to take public transportation or drive there?\n\n>>> Public transportation\n Great! The easiest way to get there is by taking the TTC subway. You can take Line 1, which runs along Yonge Street and passes through downtown Toronto.\nUsing Ollama‚Äôs API:\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"phi\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ]\n}'\nExample prompts (raw mode)\nPhi also responds well to a wide variety of prompt formats when using\nraw mode\nin Ollama‚Äôs API, which bypasses all default prompt templating:\nInstruct\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"phi\",\n  \"prompt\": \"Instruct: Write a detailed analogy between mathematics and a lighthouse.\\nOutput:\",\n  \"options\": {\n    \"stop\": [\"Instruct:\", \"Output:\"]\n  },\n  \"raw\": true,\n  \"stream\": false\n}'\nCode Completion\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"phi\",\n  \"prompt\": \"def print_prime(n):\\n  \",\n  \"raw\": true,\n  \"stream\": false\n}'\nText completion\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"phi\",\n  \"prompt\": \"There once was a mouse named\",\n  \"raw\": true,\n  \"stream\": false\n}'\nReferences\nHuggingFace\nBlog Post"
  },
  {
    "slug": "smollm",
    "name": "smollm",
    "description": "ü™ê A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
    "pull_count": "973.3K",
    "updated": "1 year ago",
    "capabilities": [
      "135m",
      "360m"
    ],
    "tags": [
      {
        "tag": "smollm:latest",
        "size": "991MB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "smollm:135m",
        "size": "92MB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "smollm:360m",
        "size": "229MB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "smollm:1.7b",
        "size": "991MB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "SmolLM is a series of small language models available in three sizes: 135M, 360M, and 1.7B parameters.\nReferences\nBlog post\nHugging Face"
  },
  {
    "slug": "phi4-mini",
    "name": "phi4-mini",
    "description": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.",
    "pull_count": "906.9K",
    "updated": "12 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "phi4-mini:latest",
        "size": "2.5GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "phi4-mini:3.8b",
        "size": "2.5GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires\nOllama 0.5.13\nor later.\nPhi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures.\nPrimary use cases\nThe model is intended for broad multilingual commercial and research use. The model provides uses for general purpose AI systems and applications which require:\nMemory/compute constrained environments\nLatency bound scenarios\nStrong reasoning (especially math and logic).\nThe model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nReferences\nHugging Face\nBlog post"
  },
  {
    "slug": "gemma3n",
    "name": "gemma3n",
    "description": "Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones.",
    "pull_count": "1.2M",
    "updated": "8 months ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "gemma3n:latest",
        "size": "7.5GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "gemma3n:e2b",
        "size": "5.6GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "gemma3n:e4b",
        "size": "7.5GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones. These models were trained with data in over 140 spoken languages.\nGemma 3n models use selective parameter activation technology to reduce resource requirements. This technique allows the models to operate at an effective size of 2B and 4B parameters, which is lower than the total number of parameters they contain.\nModels\nEffective 2B\nollama run gemma3n:e2b\nEffective 4B\nollama run gemma3n:e4b\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated at full precision (float32) against a large\ncollection of different datasets and metrics to cover different aspects of\ncontent generation. Evaluation results marked with\nIT\nare for\ninstruction-tuned models. Evaluation results marked with\nPT\nare for\npre-trained models. The models available on Ollama are instruction-tuned models.\nReasoning and factuality\nBenchmark\nMetric\nn-shot\nE2B PT\nE4B PT\nHellaSwag\nAccuracy\n10-shot\n72.2\n78.6\nBoolQ\nAccuracy\n0-shot\n76.4\n81.6\nPIQA\nAccuracy\n0-shot\n78.9\n81.0\nSocialIQA\nAccuracy\n0-shot\n48.8\n50.0\nTriviaQA\nAccuracy\n5-shot\n60.8\n70.2\nNatural Questions\nAccuracy\n5-shot\n15.5\n20.9\nARC-c\nAccuracy\n25-shot\n51.7\n61.6\nARC-e\nAccuracy\n0-shot\n75.8\n81.6\nWinoGrande\nAccuracy\n5-shot\n66.8\n71.7\nBIG-Bench Hard\nAccuracy\nfew-shot\n44.3\n52.9\nDROP\nToken F1 score\n1-shot\n53.9\n60.8\nMultilingual\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMGSM\nAccuracy\n0-shot\n53.1\n60.7\nWMT24++\n(ChrF)\nCharacter-level F-score\n0-shot\n42.7\n50.1\nInclude\nAccuracy\n0-shot\n38.6\n57.2\nMMLU\n(ProX)\nAccuracy\n0-shot\n8.1\n19.9\nOpenAI MMLU\nAccuracy\n0-shot\n22.3\n35.6\nGlobal-MMLU\nAccuracy\n0-shot\n55.1\n60.3\nECLeKTic\nECLeKTic score\n0-shot\n2.5\n1.9\nSTEM and code\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nGPQA\nDiamond\nRelaxedAccuracy/accuracy\n0-shot\n24.8\n23.7\nLiveCodeBench\nv5\npass@1\n0-shot\n18.6\n25.7\nCodegolf v2.2\npass@1\n0-shot\n11.0\n16.8\nAIME 2025\nAccuracy\n0-shot\n6.7\n11.6\nAdditional benchmarks\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMMLU\nAccuracy\n0-shot\n60.1\n64.9\nMBPP\npass@1\n3-shot\n56.6\n63.6\nHumanEval\npass@1\n0-shot\n66.5\n75.0\nLiveCodeBench\npass@1\n0-shot\n13.2\n13.2\nHiddenMath\nAccuracy\n0-shot\n27.7\n37.7\nGlobal-MMLU-Lite\nAccuracy\n0-shot\n59.0\n64.5\nMMLU\n(Pro)\nAccuracy\n0-shot\n40.5\n50.6\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen generative models have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation\n: Generate creative text formats such as\npoems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI\n: Power conversational\ninterfaces for customer service, virtual assistants, or interactive\napplications.\nText Summarization\n: Generate concise summaries of a text\ncorpus, research papers, or reports.\nImage Data Extraction\n: Extract, interpret, and summarize\nvisual data for text communications.\nAudio Data Extraction\n: Transcribe spoken language, translate speech\nto text in other languages, and analyze sound-based data.\nResearch and Education\nNatural Language Processing (NLP) and generative model\nResearch\n: These models can serve as a foundation for researchers to\nexperiment with generative models and NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools\n: Support interactive language\nlearning experiences, aiding in grammar correction or providing writing\npractice.\nKnowledge Exploration\n: Assist researchers in exploring large\nbodies of data by generating summaries or answering questions about\nspecific topics.\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety\n: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety:\nEvaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms\n: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct ‚Äúassurance\nevaluations‚Äù which are our ‚Äòarms-length‚Äô internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results‚Äô ability to inform decision making. Notable\nassurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms relative\nto previous Gemma models. All testing was conducted without safety filters to\nevaluate the model capabilities and behaviors. For text-to-text,  image-to-text,\nand audio-to-text, and across all model sizes, the model produced minimal policy\nviolations, and showed significant improvements over previous Gemma models‚Äô\nperformance with respect to high severity violations. A limitation of our\nevaluations was they included primarily English language prompts."
  },
  {
    "slug": "granite3.3",
    "name": "granite3.3",
    "description": "IBM Granite 2B and 8B models are 128K context length language models that have been fine-tuned for improved reasoning and instruction-following capabilities.",
    "pull_count": "904.2K",
    "updated": "10 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "granite3.3:latest",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite3.3:2b",
        "size": "1.5GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite3.3:8b",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Granite 3.3\nThe IBM Granite\n2B and 8B models\nare 128K context length language models that have been fine-tuned for improved reasoning and instruction-following capabilities. These models deliver significant gains on benchmarks for measuring generic performance including AlpacaEval-2.0 and Arena-Hard, and improvements in mathematics, coding, and instruction following. They also supports Fill-in-the-Middle (FIM) for code completion tasks and structured reasoning.\nParameter Sizes\n2B:\nollama run granite3.3:2b\n8B:\nollama run granite3.3:8b\nSupported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages.\nIntended Use\nThese models are designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications.\nCapabilities\nThinking\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related tasks\nFunction-calling tasks\nMultilingual dialog use cases\nFill-in-the-middle\nLong-context tasks including long document/meeting summarization, long document QA, etc.\nThinking\nTo enable thinking, add a message with ‚Äúrole‚Äù: ‚Äúcontrol‚Äù and set ‚Äúcontent‚Äù to ‚Äúthinking‚Äù. For example:\n{\n    \"messages\": [\n        {\"role\": \"control\", \"content\": \"thinking\"},\n        {\"role\": \"user\", \"content\": \"How do I get to the airport if my car won't start?\"}\n    ]\n}\nLearn more\nDevelopers:\nIBM Research\nWebsite\n:\nGranite Docs\nRelease Date\n: April 16th, 2025\nLicense:\nApache 2.0"
  },
  {
    "slug": "granite3.2-vision",
    "name": "granite3.2-vision",
    "description": "A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.",
    "pull_count": "767.9K",
    "updated": "12 months ago",
    "capabilities": [
      "vision",
      "tools"
    ],
    "tags": [
      {
        "tag": "granite3.2-vision:latest",
        "size": "2.4GB",
        "context_window": "16K",
        "input_type": "Text, Image"
      },
      {
        "tag": "granite3.2-vision:2b",
        "size": "2.4GB",
        "context_window": "16K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Note: this model requires\nOllama 0.5.13\n.\nA compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more. The model was trained on a meticulously curated instruction-following dataset, comprising diverse public datasets and synthetic datasets tailored to support a wide range of document understanding and general image tasks. It was trained by fine-tuning a Granite large language model with both image and text modalities.\nReferences\nHugging Face"
  },
  {
    "slug": "devstral",
    "name": "devstral",
    "description": "Devstral: the best open source model for coding agents",
    "pull_count": "762.2K",
    "updated": "7 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "devstral:latest",
        "size": "14GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "devstral:24b",
        "size": "14GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Devstral\nis an agentic LLM for software engineering tasks built under a collaboration between\nMistral AI\nand\nAll Hands AI\nüôå. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench which positionates it as the #1 open source model.\nIt is finetuned from Mistral Small 3.1, therefore it has a long context window of up to 128k tokens. As a coding agent, Devstral is text-only and before fine-tuning from\nMistral-Small-3.1\nthe vision encoder was removed.\nKey Features:\nAgentic coding\n: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\nLightweight\n: with its compact size of just 24 billion parameters, Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an appropriate model for local deployment and on-device use.\nApache 2.0 License\n: Open license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window\n: A 128k context window.\nSWE-Bench\nDevstral achieves a score of\n46.8% on SWE-Bench Verified\n, outperforming prior open-source state-of-the-art by 6%.\nModel\nScaffold\nSWE-Bench Verified (%)\nDevstral\nOpenHands Scaffold\n46.8\nGPT-4.1-mini\nOpenAI Scaffold\n23.6\nClaude 3.5 Haiku\nAnthropic Scaffold\n40.6\nSWE-smith-LM 32B\nSWE-agent Scaffold\n40.2\nWhen evaluated under the same test scaffold (OpenHands, provided by All Hands AI üôå), Devstral exceeds far larger models such as Deepseek-V3-0324 and Qwen3 232B-A22B.\nIn the chart below, we also compare Devstral to closed and open models evaluated under any scaffold (including ones custom for the model). Here, we find that Devstral achieves substantially better performance than a number of closed-source alternatives. For example, Devstral surpasses the recent GPT-4.1-mini by over 20%.\nReference\nBlog"
  },
  {
    "slug": "codestral",
    "name": "codestral",
    "description": "Codestral is Mistral AI‚Äôs first-ever code model designed for code generation tasks.",
    "pull_count": "836.1K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "codestral:latest",
        "size": "13GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "codestral:22b",
        "size": "13GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Codestral\nCodestral is Mistral AI‚Äôs first-ever code model designed for code generation tasks. It is a 22B model.\nFluent in 80+ programming languages\nCodestral is trained on a dataset of over 80 programming languages, including Python, Java, C, C++, JavaScript, Swift, Fortran and Bash.\nThe model can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism.\nBenchmarks\nReference\nMistral AI - Codestral: Hello, World"
  },
  {
    "slug": "openthinker",
    "name": "openthinker",
    "description": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.",
    "pull_count": "798.7K",
    "updated": "10 months ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "openthinker:latest",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openthinker:7b",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openthinker:32b",
        "size": "20GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "OpenThinker is a family of fine-tuned models from Qwen2.5 on the\nOpenThoughts-114k dataset\n, surpassing DeepSeek-R1 distillation models on some benchmarks.\nThe dataset is derived by distilling DeepSeek-R1 using the data pipeline available on github. More info about the dataset can be found on the dataset card at OpenThoughts-114k dataset.\nModels\n32B\nollama run openthinker:32b\nModel Name\nDataset Size\nAIME24 I/II\nAIME25 I\nMATH500\nGPQA Diamond\nLCBv2\nLIMO-32B\n0.8k\n56.7\n49.3\n86.6\n58.1\n60.0\ns1-32B\n1k\n36.0\n25.3\n84.8\n50.5\n40.9\ns1.1-32B\n1k\n64.7\n49.3\n89.0\n60.1\n65.5\nDeepSeek-R1-Distill-Qwen-32B\n800k (closed)\n76.7\n55.9\n89.4\n57.6\n71.2\nOpenThinker-32B\n114k\n66.0\n53.3\n90.6\n61.6\n68.9\n7B (default)\nollama run openthinker:7b\nAIME24\nMATH500\nGPQA-Diamond\nLCBv2 Easy\nLCBv2 Medium\nLCBv2 Hard\nLCBv2 All\nOpenThinker-7B\n31.3\n83.0\n42.4\n75.3\n28.6\n6.5\n39.9\nBespoke-Stratos-7B\n22.7\n79.6\n38.9\n71.4\n25.2\n0.8\n35.8\nDeepSeek-R1-Distill-Qwen-7B\n60\n88.2\n46.9\n79.7\n45.1\n14.6\n50.1\ngpt-4o-0513\n8.7\n75.8\n46.5\n87.4\n42.7\n8.9\n50.5\no1-mini\n64\n85.6\n60\n92.8\n74.7\n39.8\n72.8\nReferences\nGitHub\nBlog post"
  },
  {
    "slug": "dolphin-mistral",
    "name": "dolphin-mistral",
    "description": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
    "pull_count": "750.4K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "dolphin-mistral:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "dolphin-mistral:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "The Dolphin model by Eric Hartford, based on\nMistral\nversion 0.2 released in March 2024. This model is uncensored, available for both commercial and non-commercial use, and excels at coding.\nVersions\nTag\nDate\nNotes\nv2.8\nlatest\n03/31/2024\nBased on Mistral 0.2 with support for a context window of 32K tokens.\nv2.6\n12/27/2023\nFixed a training configuration issue that improved quality, and improvements to the training dataset for empathy.\nv2.2.1\n10/30/2023\nThis is a checkpoint release, to fix overfit training\nv2.2\n10/29/2023\nAdded conversation and empathy data.\nv2.1\n10/11/2023\nEnhanced with the airoboros dataset.\nv2.0\n10/2/2023\nInitial release of the model.\nReferences\nHuggingFace"
  },
  {
    "slug": "command-r",
    "name": "command-r",
    "description": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
    "pull_count": "722.2K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "command-r:latest",
        "size": "19GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "command-r:35b",
        "size": "19GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Command R is a generative model optimized for long context tasks such as retrieval-augmented generation (RAG) and using external APIs and tools. As a model built for companies to implement at scale, Command R boasts:\nStrong accuracy on RAG and Tool Use\nLow latency, and high throughput\nLonger 128k context\nStrong capabilities across 10 key languages\nThere are currently two versions of Command R:\nOriginal release tagged\nv0.1\nAugust 2024 update tagged\n08-2024\nReferences\nBlog Post\nHugging Face"
  },
  {
    "slug": "granite4",
    "name": "granite4",
    "description": "Granite 4 features improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.",
    "pull_count": "735.5K",
    "updated": "4 months ago",
    "capabilities": [
      "tools",
      "350m"
    ],
    "tags": [
      {
        "tag": "granite4:latest",
        "size": "2.1GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite4:350m",
        "size": "708MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "granite4:1b",
        "size": "3.3GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite4:3b",
        "size": "2.1GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Granite 4.0 models\nGranite 4.0 models\nare finetuned from their base models using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets. They feature improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.\nPlease Note:\nthe 3b, 1b, and 350m model sizes are alternative options for users when mamba-2 support is not yet optimized. Models denoted -h use the hybrid mamba-2 architecture.\nParameter Sizes\n350m\nollama run granite4:350m\n350m-h\nollama run granite4:350m-h\n1b\nollama run granite4:1b\n1b-h\nollama run granite4:1b-h\n3b (micro)\nollama run granite4:3b\nollama run granite4:micro\n3b-h (micro-h)\nollama run granite4:3b-h\nollama run granite4:micro-h\n7b-a1b-h (tiny-h)\nollama run granite4:7b-a1b-h\nollama run granite4:tiny-h\n32b-a9b-h (small-h)\nollama run granite4:32b-a9b-h\nollama run granite4:small-h\nSupported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite 4.0 models for languages beyond these languages.\nIntended Use\nThis model is designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications.\nCapabilities\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related tasks\nFunction-calling tasks\nMultilingual dialog use cases\nFill-In-the-Middle (FIM) code completions\nLearn more\nDevelopers: Granite Team, IBM\nWebsite:\nGranite Docs\nGitHub Repository:\nibm-granite/granite-4.0-language-models\nRelease Date: October 2nd, 2025\nLicense:\nApache 2.0"
  },
  {
    "slug": "granite-code",
    "name": "granite-code",
    "description": "A family of open foundation models by IBM for Code Intelligence",
    "pull_count": "691.7K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "granite-code:latest",
        "size": "2.0GB",
        "context_window": "125K",
        "input_type": "Text"
      },
      {
        "tag": "granite-code:3b",
        "size": "2.0GB",
        "context_window": "125K",
        "input_type": "Text"
      },
      {
        "tag": "granite-code:8b",
        "size": "4.6GB",
        "context_window": "125K",
        "input_type": "Text"
      },
      {
        "tag": "granite-code:20b",
        "size": "12GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "granite-code:34b",
        "size": "19GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "Granite Code is a family of decoder-only code model designed for code generative tasks (e.g. code generation, code explanation, code fixing, etc.).\nParameter Sizes\n34B Parameters\nollama run granite-code:34b\n20B Parameters\nollama run granite-code:20b\n8B Parameters (with 128K context window)\nollama run granite-code:8b\n3B Parameters (with 128K context window)\nollama run granite-code:3b\nResources\nHugging Face\nPaper"
  },
  {
    "slug": "qwen3-coder-next",
    "name": "qwen3-coder-next",
    "description": "Qwen3-Coder-Next is a coding-focused language model from Alibaba's Qwen team, optimized for agentic coding workflows and local development.",
    "pull_count": "693.1K",
    "updated": "3 weeks ago",
    "capabilities": [
      "tools",
      "cloud"
    ],
    "tags": [
      {
        "tag": "qwen3-coder-next:latest",
        "size": "52GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-coder-next:cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-coder-next:q4_K_M",
        "size": "52GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-coder-next:q8_0",
        "size": "85GB",
        "context_window": "256K",
        "input_type": "Text"
      }
    ],
    "readme": "Built on top of Qwen3-Next-80B-A3B-Base, which adopts a novel architecture with hybrid attention and MoE, Qwen3-Coder-Next has been agentically trained at scale on large-scale executable task synthesis, environment interaction, and reinforcement learning, obtaining strong coding and agentic capabilities with significantly lower inference costs.\nFeatures\nUltra-efficient inference:\n80B total parameters, 3B active per token. Runs on consumer hardware with quantization.\n256K native context:\nFull repository-scale understanding without chunking or retrieval hacks.\nAgentic training:\nTrained on 800K executable tasks with environment interaction and reinforcement learning‚Äînot just static code-text pairs.\nTool calling:\nWorks with coding agents like Claude Code, Qwen Code, Cline, and OpenCode out of the box.\nNon-thinking mode only:\nFast responses without\n<think></think>\nblocks.\nBenchmarks"
  },
  {
    "slug": "wizardlm2",
    "name": "wizardlm2",
    "description": "State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoning and agent use cases.",
    "pull_count": "669.2K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "wizardlm2:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm2:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm2:8x22b",
        "size": "80GB",
        "context_window": "64K",
        "input_type": "Text"
      }
    ],
    "readme": "WizardLM-2 is a next generation state-of-the-art large language model with improved performance on complex chat, multilingual, reasoning and agent use cases. This family includes three cutting-edge models:\nwizardlm2:7b\n: fastest model, comparable performance with 10x larger open-source models.\nwizardlm2:8x22b\n: the most advanced model, and the best opensource LLM in Microsoft‚Äôs internal evaluation on highly complex tasks.\nwizardlm2:70b\n: model with top-tier reasoning capabilities for its size (coming soon)\nReferences\nBlog Post\nHuggingFace"
  },
  {
    "slug": "hermes3",
    "name": "hermes3",
    "description": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
    "pull_count": "633.5K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "hermes3:latest",
        "size": "4.7GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "hermes3:3b",
        "size": "2.0GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "hermes3:8b",
        "size": "4.7GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "hermes3:70b",
        "size": "40GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "hermes3:405b",
        "size": "229GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\nThe ethos of the Hermes series of models is focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\nHermes 3\nseries contains four models:\n3B\n,\n8B\n,\n70B\n, and\n405B\nReferences:\nHermes 3 Technical Report\nHugging Face"
  },
  {
    "slug": "qwen3-embedding",
    "name": "qwen3-embedding",
    "description": "Building upon the foundational models of the Qwen3 series, Qwen3 Embedding provides a comprehensive range of text embeddings models in various sizes",
    "pull_count": "961.7K",
    "updated": "5 months ago",
    "capabilities": [
      "embedding"
    ],
    "tags": [
      {
        "tag": "qwen3-embedding:latest",
        "size": "4.7GB",
        "context_window": "40K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-embedding:0.6b",
        "size": "639MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-embedding:4b",
        "size": "2.5GB",
        "context_window": "40K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-embedding:8b",
        "size": "4.7GB",
        "context_window": "40K",
        "input_type": "Text"
      }
    ],
    "readme": "Highlights\nThe Qwen3 Embedding model series is specifically designed for text embedding tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\nExceptional Versatility\n: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks\nNo.1\nin the MTEB multilingual leaderboard (as of June 5, 2025, score\n70.58\n).\nComprehensive Flexibility\n: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for embedding models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, these models allow for flexible vector definitions across all dimensions, support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\nMultilingual Capability\n: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\nQwen3-Embedding-8B\nhas the following features:\nModel Type: Text Embedding\nSupported Languages: 100+ Languages\nNumber of Paramaters: 8B\nContext Length: 32k\nEmbedding Dimension: Up to 4096, supports user-defined output dimensions ranging from 32 to 4096\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to the model‚Äôs\nblog\n,\nGitHub\n."
  },
  {
    "slug": "mistral-small3.1",
    "name": "mistral-small3.1",
    "description": "Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.",
    "pull_count": "609.9K",
    "updated": "10 months ago",
    "capabilities": [
      "vision",
      "tools"
    ],
    "tags": [
      {
        "tag": "mistral-small3.1:latest",
        "size": "15GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "mistral-small3.1:24b",
        "size": "15GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Note: this model requires Ollama 0.6.5 or higher.\nDownload Ollama\nBuilding on\nMistral Small 3\n, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second.\nMistral Small 3.1 is released under an Apache 2.0 license.\nKey features and capabilities\nLightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.\nFast-response conversational assistance: Ideal for virtual assistants and other applications where quick, accurate responses are essential.\nLow-latency function calling: Capable of rapid function execution within automated or agentic workflows\nFine-tuning for specialized domains: Mistral Small 3.1 can be fine-tuned to specialize in specific domains, creating accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support.\nFoundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model.\nReferences\nBlog post"
  },
  {
    "slug": "lfm2.5-thinking",
    "name": "lfm2.5-thinking",
    "description": "LFM2.5 is a new family of hybrid models designed for on-device deployment.",
    "pull_count": "599.2K",
    "updated": "1 month ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "lfm2.5-thinking:latest",
        "size": "731MB",
        "context_window": "125K",
        "input_type": "Text"
      },
      {
        "tag": "lfm2.5-thinking:1.2b",
        "size": "731MB",
        "context_window": "125K",
        "input_type": "Text"
      }
    ],
    "readme": "LFM2.5 is a new family of hybrid models designed for on-device deployment. It builds on the LFM2 architecture with extended pre-training and reinforcement learning.\nBest-in-class performance: A 1.2B model rivaling much larger models, bringing high-quality AI to your pocket.\nLFM2.5-1.2B-Thinking is a general-purpose text-only model with the following features:\nNumber of parameters: 1.17B\nNumber of layers: 16 (10 double-gated LIV convolution blocks + 6 GQA blocks)\nTraining budget: 28T tokens\nContext length: 32,768 tokens\nVocabulary size: 65,536\nLanguages: English, Arabic, Chinese, French, German, Japanese, Korean, Spanish\nBenchmarks\nWe compared LFM2.5-1.2B-Thinking with relevant sub-2B models on a diverse suite of benchmarks.\nModel\nGPQA Diamond\nMMLU-Pro\nIFEval\nIFBench\nMulti-IF\nGSM8K\nMATH-500\nAIME25\nBFCLv3\nLFM2.5-1.2B-Thinking\n37.86\n49.65\n88.42\n44.85\n69.33\n85.60\n87.96\n31.73\n56.97\nQwen3-1.7B (thinking)\n36.93\n56.68\n71.65\n25.88\n60.33\n85.60\n81.92\n36.27\n55.41\nLFM2.5-1.2B-Instruct\n38.89\n44.35\n86.23\n47.33\n60.98\n64.52\n63.20\n14.00\n49.12\nQwen3-1.7B (instruct)\n34.85\n42.91\n73.68\n21.33\n56.48\n33.66\n70.40\n9.33\n46.30\nGranite-4.0-H-1B\n24.34\n27.64\n80.08\n24.93\n47.56\n69.60\n47.20\n1\n50.69\nGemma 3 1B IT\n24.24\n14.04\n63.25\n20.47\n44.31\n42.15\n45.20\n1\n16.64\nLlama 3.2 1B Instruct\n16.57\n20.80\n52.37\n15.93\n30.16\n39.04\n23.40\n0.33\n21.44"
  },
  {
    "slug": "yi",
    "name": "yi",
    "description": "Yi 1.5 is a high-performing, bilingual language model.",
    "pull_count": "589.5K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "yi:latest",
        "size": "3.5GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "yi:6b",
        "size": "3.5GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "yi:9b",
        "size": "5.0GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "yi:34b",
        "size": "19GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Yi is a series of large language models trained on a high-quality corpus of 3 trillion tokens that support both the English and Chinese languages.\nReferences\nHuggingFace"
  },
  {
    "slug": "zephyr",
    "name": "zephyr",
    "description": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.",
    "pull_count": "569.7K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "zephyr:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "zephyr:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "zephyr:141b",
        "size": "80GB",
        "context_window": "64K",
        "input_type": "Text"
      }
    ],
    "readme": "Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr 141B-A35B is the latest model in the series, and is a fine-tuned version of\nMixtral 8x22b\n.\nSizes\nzephyr:141b\n: A Mixture of Experts (MoE) model with 141B total parameters and 35B active parameters.\nzephyr:7b\n: The original Zephyr model\nSource:\nHuggingFace"
  },
  {
    "slug": "phi3.5",
    "name": "phi3.5",
    "description": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.",
    "pull_count": "556.1K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "phi3.5:latest",
        "size": "2.2GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "phi3.5:3.8b",
        "size": "2.2GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites with a focus on very high-quality, reasoning dense data.\nThe model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nLong Context\nPhi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval.\nReferences\nHugging Face"
  },
  {
    "slug": "mistral-large",
    "name": "mistral-large",
    "description": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and reasoning with 128k context window and support for dozens of languages.",
    "pull_count": "560.1K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "mistral-large:latest",
        "size": "73GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "mistral-large:123b",
        "size": "73GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Mistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.\nKey features\nMulti-lingual by design: Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish.\nProficient in coding: Trained on 80+ coding languages such as Python, Java, C, C++, JavacScript, and Bash. Also trained on more specific languages such as Swift and Fortran.\nAgentic-centric: Best-in-class agentic capabilities with native function calling and JSON outputting.\nAdvanced Reasoning: State-of-the-art mathematical and reasoning capabilities.\nMistral Research License: Allows usage and modification for research and non-commercial usages.\nLarge Context: A large 128k context window.\nReferences\nBlog post\nHugging Face"
  },
  {
    "slug": "wizard-vicuna-uncensored",
    "name": "wizard-vicuna-uncensored",
    "description": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
    "pull_count": "552K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "wizard-vicuna-uncensored:latest",
        "size": "3.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizard-vicuna-uncensored:7b",
        "size": "3.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizard-vicuna-uncensored:13b",
        "size": "7.4GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizard-vicuna-uncensored:30b",
        "size": "18GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed.\nGet started with Wizard Vicuna Uncensored\nThe model used in the example below is the Wizard Vicuna Uncensored model, with 7b parameters, which is a general-use model.\nAPI\nStart Ollama server (Run\nollama serve\n)\nRun the model\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizard-vicuna-uncensored\",\n  \"prompt\":\"Who made Rose promise that she would never let go?\"\n }'\nCLI\nInstall Ollama\nOpen the terminal and run\nollama run wizard-vicuna-uncensored\nNote: The\nollama run\ncommand performs an\nollama pull\nif the model is not already downloaded. To download the model without running it, use\nollama pull wizard-vicuna-uncensored\nMemory requirements\n7b models generally require at least 8GB of RAM\n13b models generally require at least 16GB of RAM\n30b models generally require at least 32GB of RAM\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\nModel variants\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\nAliases\nlatest, 7b, 7b-q4_0\n13b, 13b-q4_0\n30b, 30b-q4_0\nModel source\nWizard Vicuna Uncensored source on Ollama\n7b parameters original source:\nEric Hartford\n13b parameters original source:\nEric Hartford\n30b parameters original source:\nEric Hartford"
  },
  {
    "slug": "bakllava",
    "name": "bakllava",
    "description": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA  architecture.",
    "pull_count": "525.2K",
    "updated": "2 years ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "bakllava:latest",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text, Image"
      },
      {
        "tag": "bakllava:7b",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Note: this model requires Ollama 0.1.15:\ndownload it here\nCLI Usage\nRun the model:\nollama run bakllava\nThen at the prompt, include the path to your image in the prompt:\n>>> What's in this image? /Users/jmorgan/Desktop/smile.png\nThe image features a yellow smiley face, which is likely the central focus of the picture.\nAPI Usage\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"bakllava\",\n  \"prompt\":\"What is in this picture?\",\n  \"images\": [\"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\"]\n}'\nReferences\nGitHub\nHuggingFace"
  },
  {
    "slug": "moondream",
    "name": "moondream",
    "description": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
    "pull_count": "650.7K",
    "updated": "1 year ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "moondream:latest",
        "size": "1.7GB",
        "context_window": "2K",
        "input_type": "Text, Image"
      },
      {
        "tag": "moondream:1.8b",
        "size": "1.7GB",
        "context_window": "2K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "üåî moondream 2\nMoondream 2 requires Ollama 0.1.33 or later\n‚Äúa tiny vision language model that kicks ass and runs anywhere‚Äù\nLimitations\nThe model may generate inaccurate statements, and struggle to understand intricate or nuanced instructions.\nThe model may not be free from societal biases. Users should be aware of this and exercise caution and critical thinking when using the model.\nThe model may generate offensive, inappropriate, or hurtful content if it is prompted to do so.\nReferences\nGitHub\nHugging Face"
  },
  {
    "slug": "deepcoder",
    "name": "deepcoder",
    "description": "DeepCoder is a fully open-Source 14B coder model at O3-mini level, with a 1.5B version also available.",
    "pull_count": "653.2K",
    "updated": "10 months ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "deepcoder:latest",
        "size": "9.0GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepcoder:1.5b",
        "size": "1.1GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "deepcoder:14b",
        "size": "9.0GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Through a joint collaboration between the Agentica team and Together AI, DeepCoder-14B-Preview is a code reasoning model finetuned from Deepseek-R1-Distilled-Qwen-14B via distributed RL. It achieves an impressive 60.6% Pass@1 accuracy on LiveCodeBench (+8% improvement), matching the performance of\no3-mini-2025-01-031\n(Low) and\no1-2024-12-17\nwith just 14B parameters\nEvaluation\nModel\nLCB (v5)(8/1/24-2/1/25)\nCodeforces Rating\nCodeforces Percentile\nHumanEval+\nDeepCoder-14B-Preview\n60.6\n1936\n95.3\n92.6\nDeepSeek-R1-Distill-Qwen-14B\n53.0\n1791\n92.7\n92.0\nO1-2024-12-17 (Low)\n59.5\n1991\n96.1\n90.8\nO3-Mini-2025-1-31 (Low)\n60.9\n1918\n94.9\n92.6\nO1-Preview\n42.7\n1658\n88.5\n89\nDeepseek-R1\n62.8\n1948\n95.4\n92.6\nLlama-4-Behemoth\n49.4\n-\n-\n-\nReferences\nBlog post"
  },
  {
    "slug": "exaone-deep",
    "name": "exaone-deep",
    "description": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.",
    "pull_count": "496.6K",
    "updated": "11 months ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "exaone-deep:latest",
        "size": "4.8GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "exaone-deep:2.4b",
        "size": "1.6GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "exaone-deep:7.8b",
        "size": "4.8GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "exaone-deep:32b",
        "size": "19GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.\nEvaluation results show that:\nEXAONE Deep 2.4B outperforms other models of comparable size\nEXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini\nEXAONE Deep 32B demonstrates competitive performance against leading open-weight models.\nReferences\nHugging Face"
  },
  {
    "slug": "deepseek-llm",
    "name": "deepseek-llm",
    "description": "An advanced language model crafted with 2 trillion bilingual tokens.",
    "pull_count": "493.5K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "deepseek-llm:latest",
        "size": "4.0GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-llm:7b",
        "size": "4.0GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-llm:67b",
        "size": "38GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "DeepSeek LLM is an advanced language model available in both 7 billion and 67 billion parameters. Both a\nchat\nand\nbase\nvariation are available.\nSuperior General Capabilities: DeepSeek LLM 67B Base outperforms Llama2 70B Base in areas such as reasoning, coding, math, and Chinese comprehension.\nProficient in Coding and Math: DeepSeek LLM 67B Chat exhibits outstanding performance in coding (using the HumanEval benchmark) and mathematics (using the GSM8K benchmark).\nReferences\nGitHub\nHuggingFace"
  },
  {
    "slug": "openchat",
    "name": "openchat",
    "description": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106.",
    "pull_count": "488.4K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "openchat:latest",
        "size": "4.1GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "openchat:7b",
        "size": "4.1GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "OpenChat is set of open-source language models, fine-tuned with C-RLFT: a strategy inspired by offline reinforcement learning.\nUpdated to OpenChat-3.5-1210, this new version of the model model excels at coding tasks and scores very high on many open-source LLM benchmarks.\nReferences\nGitHub\nHuggingFace"
  },
  {
    "slug": "vicuna",
    "name": "vicuna",
    "description": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
    "pull_count": "484.3K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "vicuna:latest",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "vicuna:7b",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "vicuna:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "vicuna:33b",
        "size": "18GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Vicuna is a chat assistant model. It includes 3 different variants in 3 different sizes. v1.3 is trained by fine-tuning Llama and has a context size of 2048 tokens. v1.5 is trained by fine-tuning Llama 2 and has a context size of 2048 tokens. v1.5-16k is trained by fine-tuning Llama 2 and has a context size of 16k tokens. All three variants are trained using conversations collected from ShareGPT.\nExample prompts\nWhat is the meaning of life? Explain it in 5 paragraphs.\nReferences\nHuggingFace"
  },
  {
    "slug": "ministral-3",
    "name": "ministral-3",
    "description": "The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.",
    "pull_count": "488.9K",
    "updated": "2 months ago",
    "capabilities": [
      "vision",
      "tools",
      "cloud"
    ],
    "tags": [
      {
        "tag": "ministral-3:latest",
        "size": "6.0GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "ministral-3:3b",
        "size": "3.0GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "ministral-3:8b",
        "size": "6.0GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "ministral-3:14b",
        "size": "9.1GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "ministral-3:3b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "ministral-3:8b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "ministral-3:14b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "This model requires\nOllama 0.13.1\n, which is currently in pre-release.\nThe Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.\nThe Ministral 3 models offer the following capabilities:\nVision: Enables the model to analyze images and provide insights based on visual content, in addition to text.\nMultilingual: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.\nSystem Prompt: Maintains strong adherence and support for system prompts.\nAgentic: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\nEdge-Optimized: Delivers best-in-class performance at a small scale, deployable anywhere.\nApache 2.0 License: Open-source license allowing usage and modification for both commercial and non-commercial purposes.\nLarge Context Window: Supports a 256k context window.\nMinistral 14B\nMinistral 8B\nMinistral 3B"
  },
  {
    "slug": "embeddinggemma",
    "name": "embeddinggemma",
    "description": "EmbeddingGemma is a 300M parameter embedding model from Google.",
    "pull_count": "545.7K",
    "updated": "5 months ago",
    "capabilities": [
      "embedding",
      "300m"
    ],
    "tags": [
      {
        "tag": "embeddinggemma:latest",
        "size": "622MB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "embeddinggemma:300m",
        "size": "622MB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "This model requires\nOllama v0.11.10\nor later\nEmbeddingGemma\nis a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages.\nThe small size and on-device focus makes it possible to deploy in environments with limited resources such as mobile phones, laptops, or desktops, democratizing access to state of the art AI models and helping foster innovation for everyone.\nBenchmark\nTraining Dataset\nThis model was trained on a dataset of text data that includes a wide variety of sources totaling approximately 320 billion tokens. Here are the key components:\nWeb Documents\n: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 100 languages.\nCode and Technical Documents\n: Exposing the model to code and technical documentation helps it learn the structure and patterns of programming languages and specialized scientific content, which improves its understanding of code and technical questions.\nSynthetic and Task-Specific Data\n: Synthetically training data helps to teach the model specific skills. This includes curated data for tasks like information retrieval, classification, and sentiment analysis, which helps to fine-tune its performance for common embedding applications.\nThe combination of these diverse data sources is crucial for training a powerful multilingual embedding model that can handle a wide variety of different tasks and data formats.\nReference\nDocumentation"
  },
  {
    "slug": "glm4",
    "name": "glm4",
    "description": "A strong multi-lingual general language model with competitive performance to Llama 3.",
    "pull_count": "475.3K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "glm4:latest",
        "size": "5.5GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "glm4:9b",
        "size": "5.5GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires\nOllama 0.2\nor later.\nIn the evaluation of data sets in semantics, mathematics, reasoning, code, and knowledge, this model has shown superior performance beyond even Llama 3 8B.\nThis generation of models includes multi-language support, supporting 26 languages including Japanese, Korean, and German.\nReferences\nGitHub\nHugging Face"
  },
  {
    "slug": "paraphrase-multilingual",
    "name": "paraphrase-multilingual",
    "description": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
    "pull_count": "511.8K",
    "updated": "1 year ago",
    "capabilities": [
      "embedding",
      "278m"
    ],
    "tags": [
      {
        "tag": "paraphrase-multilingual:latest",
        "size": "563MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "paraphrase-multilingual:278m",
        "size": "563MB",
        "context_window": "512",
        "input_type": "Text"
      }
    ],
    "readme": "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nSentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "slug": "starcoder",
    "name": "starcoder",
    "description": "StarCoder is a code generation model trained on 80+ programming languages.",
    "pull_count": "507.5K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "starcoder:latest",
        "size": "1.8GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "starcoder:1b",
        "size": "726MB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "starcoder:3b",
        "size": "1.8GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "starcoder:7b",
        "size": "4.3GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "starcoder:15b",
        "size": "9.0GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "StarCoder models are trained on GitHub code with over 80+ programming languages.\nExample prompt\ndef print_hello_world():\nExample response\nprint(\"Hello world\")"
  },
  {
    "slug": "nous-hermes",
    "name": "nous-hermes",
    "description": "General use models based on Llama and Llama 2 from Nous Research.",
    "pull_count": "499.9K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "nous-hermes:latest",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "nous-hermes:7b",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "nous-hermes:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Nous Hermes was released by Nous Research. There are two main variants here, a 13B parameter model based on Llama, and a 7B and 13B parameter model based on Llama 2. They are all general-use models trained with the same datasets.\nGet started with Nous Hermes\nThe model used in the example below is the Nous Hermes Llama 2 model, with 7b parameters, which is a general chat model.\nAPI\nStart Ollama server (Run\nollama serve\n)\nRun the model\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"nous-hermes\",\n  \"prompt\":\"Explain the process of how a refrigerator works to keep the contents inside cold.\"\n }'\nCLI\nInstall Ollama\nOpen the terminal and run\nollama run nous-hermes\nNote: The\nollama run\ncommand performs an\nollama pull\nif the model is not already downloaded. To download the model without running it, use\nollama pull nous-hermes\nMemory requirements\n7b models generally require at least 8GB of RAM\n13b models generally require at least 16GB of RAM\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\nModel variants\nOllama offers many variants of the Nous Hermes model that are quantized based on the official models to run well locally.\nNous Hermes Llama 2\nis the original Nous Hermes model based on the original Llama model.\nExample:\nollama run nous-hermes\nNous Hermes Llama 1\nis the original Nous Hermes model based on the original Llama model.\nExample:\nollama run nous-hermes:13b-q4_0\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\nAliases\nlatest, 7b, 7b-llama2, 7b-llama2-q4_0\n13b, 13b-llama2, 13b-llama2-q4_0\nModel source\nNous Hermes Llama 2 source on Ollama\n7b parameters original source:\nNous Research\n13b parameters original source:\nNous Research\nNous Hermes Llama 1 source on Ollama\n13b parameters original source:\nNous Research"
  },
  {
    "slug": "codeqwen",
    "name": "codeqwen",
    "description": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
    "pull_count": "464.6K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "codeqwen:latest",
        "size": "4.2GB",
        "context_window": "64K",
        "input_type": "Text"
      },
      {
        "tag": "codeqwen:7b",
        "size": "4.2GB",
        "context_window": "64K",
        "input_type": "Text"
      }
    ],
    "readme": "CodeQwen1.5 is based on Qwen1.5. It is trained on 3 trillion tokens of code data. Its major features include:\nStrong code generation capabilities and competitive performance across a series of benchmarks\nSupport for long context understanding and generation with a maximum context length of 64K tokens\nSupport for 92 coding languages\nExcellent performance in Text-to-SQL, fixing bugs and other coding use cases.\nReferences\nBlog Post\nGitHub\nHuggingFace"
  },
  {
    "slug": "openhermes",
    "name": "openhermes",
    "description": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
    "pull_count": "464.9K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "openhermes:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:v2",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:v2.5",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q2_K",
        "size": "3.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q3_K_S",
        "size": "3.2GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q3_K_M",
        "size": "3.5GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q3_K_L",
        "size": "3.8GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q4_0",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q4_1",
        "size": "4.6GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q4_K_S",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q4_K_M",
        "size": "4.4GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q5_0",
        "size": "5.0GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q5_1",
        "size": "5.4GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q5_K_S",
        "size": "5.0GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q5_K_M",
        "size": "5.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q6_K",
        "size": "5.9GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-q8_0",
        "size": "7.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2-fp16",
        "size": "14GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q2_K",
        "size": "3.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q3_K_S",
        "size": "3.2GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q3_K_M",
        "size": "3.5GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q3_K_L",
        "size": "3.8GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q4_0",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q4_1",
        "size": "4.6GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q4_K_S",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q4_K_M",
        "size": "4.4GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q5_0",
        "size": "5.0GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q5_1",
        "size": "5.4GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q5_K_S",
        "size": "5.0GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q5_K_M",
        "size": "5.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q6_K",
        "size": "5.9GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-q8_0",
        "size": "7.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-mistral-v2.5-fp16",
        "size": "14GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-v2",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "openhermes:7b-v2.5",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Open Hermes 2 a Mistral 7B fine-tuned with fully open datasets. Matching 70B models on benchmarks, this model has strong multi-turn chat skills and system prompt capabilities. In total, the model was trained on 900,000 instructions, and surpasses all previous versions of Nous-Hermes 13B and below.\nVersions\nTag\nDate\nNotes\nv2.5\nlatest\n11/02/2023\nAdded ~100k examples of Code Instructions\nv2\n10/16/2023\nInitial release of Open Hermes 2\nUsage\nCLI\nollama run openhermes\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"openhermes\",\n  \"prompt\": \"Here is a story about llamas eating grass\"\n}'\nReferences\nHugging Face"
  },
  {
    "slug": "qwen2-math",
    "name": "qwen2-math",
    "description": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o).",
    "pull_count": "452.4K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "qwen2-math:latest",
        "size": "4.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2-math:1.5b",
        "size": "935MB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2-math:7b",
        "size": "4.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "qwen2-math:72b",
        "size": "41GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Qwen2 Math\nfeatures 3 model sizes (\n1.5B\n,\n7B\n, and\n72B\n) each with a\nbase\nmodel, for completion, few-shot inference, and fine-tuning, and an\ninstruct\nmodel for chatting.\nReferences:\nQwen 2 Math Blog Post\nGitHub Repository\nHugging Face"
  },
  {
    "slug": "falcon",
    "name": "falcon",
    "description": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.",
    "pull_count": "490.8K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "falcon:latest",
        "size": "4.2GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "falcon:7b",
        "size": "4.2GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "falcon:40b",
        "size": "24GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "falcon:180b",
        "size": "101GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Falcon is a family of high-performing large language models model built by the Technology Innovation Institute (TII), a research center part of Abu Dhabi government‚Äôs advanced technology research council overseeing technology research.\nCLI\nollama run falcon \"Why is the sky blue?\"\nAPI\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"falcon\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\nParameter counts\nParameter Count\nRecommended memory\n7 billion\n8GB\nView\nollama run falcon:7b\n40 billion\n32GB\nView\nollama run falcon:40b\n180 billion\n192GB\nView\nollama run falcon:180b\nVariations\nchat\nChat models are fine-tuned on chat and instruction datasets with a mix of several large-scale conversational datasets.\ninstruct\nInstruct models follow instructions and are fine-tuned on the\nbaize\ninstructional dataset.\ntext\nText models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion.\nFalcon 180B\nAs of September 2023, the 180 billion parameter model, Falcon 180B, is the best-performing openly released LLM. It sits somewhere in between OpenAI‚Äôs GPT 3.5 and GPT 4. For running Falcon 180B, a powerful system is recommended with at least 192GB of total memory.\nNote: Falcon 180B is released under a different license than its smaller siblings that restricts commercial use under certain conditions. See the\nmodel details\nand license for more information.\nMore information\nTII‚Äôs website\nFalcon 180B announcement\nTII on HuggingFace"
  },
  {
    "slug": "llama2-chinese",
    "name": "llama2-chinese",
    "description": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
    "pull_count": "440.7K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "llama2-chinese:latest",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama2-chinese:7b",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama2-chinese:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Llama 2 ÂØπËØù‰∏≠ÊñáÂæÆË∞ÉÂèÇÊï∞Ê®°Âûã\nËøô‰∏™Ê®°ÂûãÊòØÂü∫‰∫é Meta Platform, Inc. ÊâÄÂèëÂ∏ÉÁöÑ Llama 2 Chat ÂºÄÊ∫êÊ®°ÂûãÊù•ËøõË°åÂæÆË∞É„ÄÇÊ†πÊçÆMetaÔºåLlama 2 ÁöÑËÆ≠ÁªÉÊï∞ÊçÆËææÂà∞‰∫Ü‰∏§‰∏á‰∫ø‰∏™tokenÔºå‰∏ä‰∏ãÊñáÈïøÂ∫¶‰πüÊèêÂçáÂà∞4096„ÄÇÂØπËØù‰∏ä‰πüÊòØ‰ΩøÁî®100‰∏á‰∫∫Á±ªÊ†áËÆ∞ÁöÑÊï∞ÊçÆÂæÆË∞É„ÄÇ\nÁî±‰∫é Llama 2 Êú¨Ë∫´ÁöÑ‰∏≠ÊñáÂØπÈΩêÊØîËæÉÂº±ÔºåÂºÄÂèëËÄÖÈááÁî®‰∫Ü‰∏≠ÊñáÊåá‰ª§ÈõÜÊù•ËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂ÂÖ∑Â§áËæÉÂº∫ÁöÑ‰∏≠ÊñáÂØπËØùËÉΩÂäõ„ÄÇÁõÆÂâçËøô‰∏™‰∏≠ÊñáÂæÆË∞ÉÂèÇÊï∞Ê®°ÂûãÊÄªÂÖ±ÂèëÂ∏É‰∫Ü 7BÔºå13B‰∏§ÁßçÂèÇÊï∞Â§ßÂ∞è„ÄÇ\nLlama 2 chat chinese fine-tuned model\nThis model is fine-tuned based on Meta Platform‚Äôs Llama 2 Chat open source model. According to Meta, Llama 2 is trained on 2 trillion tokens, and the context length is increased to 4096. The chat model is fine-tuned using 1 million human labeled data.\nSince the Chinese alignment of Llama 2 itself is relatively weak, the developer, adopted a Chinese instruction set for fine-tuning to improve the Chinese dialogue ability.\nThe Chinese fine-tuned models are available in 7B and 13B parameter sizes.\nCLI\nOpen the terminal and run\nollama run llama2-chinese\nAPI\nRun the model\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2-chinese:7b-chat-q4_0\",\n  \"prompt\":\"‰∏∫‰ªÄ‰πàÂ§©Á©∫ÊòØËìùËâ≤ÁöÑ\"\n }'\nMemory requirements\n7b models generally require at least 8GB of RAM\n13b models generally require at least 16GB of RAM\nReference\nFlagAlpha\nFlagAlpha"
  },
  {
    "slug": "deepseek-v2",
    "name": "deepseek-v2",
    "description": "A strong, economical, and efficient Mixture-of-Experts language model.",
    "pull_count": "489.8K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "deepseek-v2:latest",
        "size": "8.9GB",
        "context_window": "160K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-v2:16b",
        "size": "8.9GB",
        "context_window": "160K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-v2:236b",
        "size": "133GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires\nOllama 0.1.40\n.\nDeepSeek-V2 is a a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference.\nNote: this model is bilingual in English and Chinese.\nThe model comes in two sizes:\n16B Lite:\nollama run deepseek-v2:16b\n236B:\nollama run deepseek-v2:236b\nReferences\nGitHub"
  },
  {
    "slug": "aya",
    "name": "aya",
    "description": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.",
    "pull_count": "446.2K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "aya:latest",
        "size": "4.8GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "aya:8b",
        "size": "4.8GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "aya:35b",
        "size": "20GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual, generative large language research model (LLM) covering 23 different languages.\nIt is available in 8B and 35B parameter sizes:\n8B\nollama run aya:8b\n35B\nollama run aya:35b\nReferences\nBlog Post\nAya 23: Open Weight Releases to Further Multilingual Progress paper\nHugging Face"
  },
  {
    "slug": "stable-code",
    "name": "stable-code",
    "description": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.",
    "pull_count": "434.6K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "stable-code:latest",
        "size": "1.6GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "stable-code:3b",
        "size": "1.6GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Stable Code 3B is a 3 billion parameter Large Language Model (LLM), allowing accurate and responsive code completion at a level on par with models such as Code Llama 7b that are 2.5x larger.\nKey Features\nNEW\ninstruct model\nollama run stable-code\nFill in Middle Capability (FIM)\nSupports Long Context, trained with Sequences upto 16,384\nModel\nSize\nPython\nC++\nJavascript\nJava\nPHP\nRust\nStable Code\n3B\n32.4%\n30.9%\n32.1%\n32.1%\n24.2%\n23.0%\nCodeLLama\n7B\n30.0%\n28.2%\n32.5%\n31.1%\n25.7%\n26.3%\nDeepseek Coder\n1.3B\n28.6%\n29.2%\n28.7%\n29.0%\n23.6%\n18.5%\nWizard Coder\n3B\n31.6%\n25.6%\n26.2%\n25.8%\n25.3%\n20.4%\nStarCoder\n3B\n21.6%\n19.8%\n21.5%\n20.5%\n19.0%\n16.9%\nReplit Code V1.5\n3B\n23.0%\n25.9%\n26.2%\n23.6%\n23.2%\n21.5%\nDeci Coder\n1B\n19.1%\n6.8%\n18.4%\n16.7%\n2.1%\n1.7%\nModel Details\nDeveloped by\n:\nStability AI\nModel type\n: stable-code models are auto-regressive language models based on the transformer decoder architecture.\nLanguage(s)\n: English, Code\nContact\n: For questions and comments about the model, please email\nlm@stability.ai\nModel Architecture\nThe model is a decoder-only transformer similar to the LLaMA (\nTouvron et al., 2023\n) architecture with the following modifications:\nParameters\nHidden Size\nLayers\nHeads\nSequence Length\n2,796,431,360\n2560\n32\n32\n16384\nPosition Embeddings\n: Rotary Position Embeddings (\nSu et al., 2021\n) applied to the first 25% of head embedding dimensions for improved throughput following\nBlack et al. (2022)\n.\nTokenizer\n: We use a modified version of the GPTNeoX Tokenizer.\nNeoX\n. We add special tokens to train for Fill in the Middle (FIM) capabilities like\n<FIM_PREFIX>\nand\n<FIM_SUFFIX>\nalong with other special tokens.\nTraining\nTraining Dataset\nThe dataset is comprised of a filtered mixture of open-source large-scale datasets available on the\nHuggingFace Hub\n: Falcon RefinedWeb extract (\nPenedo et al., 2023\n), along with\nCommitPackFT\nand\nGithub Issues\n(BigCode., 2023), and StarCoder (\nLi et al., 2023\n). We further supplement our training with data from mathematical domains (\nAzerbayev, Zhangir, et al., 2023\nand,\nYu, Longhui, et al., 2023\n).\nTop 18 programming languages trained on:\n- C\n- CPP\n- Java\n- JavaScript\n- CSS\n- Go\n- HTML\n- Ruby\n- Rust\n- Markdown\n- Shell\n- Php\n- Sql\n- R\n- Typescript\n- Python\n- Jupyter-Clean\n- RestructuredText\nUse and Limitations\nIntended Use\nThe model is intended to be used as a foundational base model for application-specific fine-tuning. Developers must evaluate and fine-tune the model for safe performance in downstream applications.\nLimitations and Bias\n‚Äã\nAs a base model, this model may exhibit unreliable, unsafe, or other undesirable behaviors that must be corrected through evaluation and fine-tuning prior to deployment. The pre-training dataset may have contained offensive or inappropriate content, even after applying data cleansing filters, which can be reflected in the model-generated text. We recommend that users exercise caution when using these models in production systems. Do not use the models if they are unsuitable for your application, or for any applications that may cause deliberate or unintentional harm to others.\nReferences\nHugging Face"
  },
  {
    "slug": "neural-chat",
    "name": "neural-chat",
    "description": "A fine-tuned model based on Mistral with good coverage of domain and language.",
    "pull_count": "433.6K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "neural-chat:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "neural-chat:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "NeuralChat is a fine-tuned model released by Intel that‚Äôs based on\nMistral\n, designed to be used for high-performance chatbot applications.\nReferences\nHuggingFace"
  },
  {
    "slug": "wizardcoder",
    "name": "wizardcoder",
    "description": "State-of-the-art code generation model",
    "pull_count": "422.2K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "wizardcoder:latest",
        "size": "3.8GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "wizardcoder:33b",
        "size": "19GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Wizard Coder is a code generation model based on\nCode Llama\n.\nVersions\nTag\nDate\nNotes\n33b\n01/042024\nA new 33B model trained from\nDeepseek Coder\npython\n09/7/2023\nInitial release in 7B, 13B and 34B sizes based on\nCode Llama\nReferences\nHugging Face\nWizardCoder: Empowering Code Large Language Models with Evol-Instruct"
  },
  {
    "slug": "yi-coder",
    "name": "yi-coder",
    "description": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.",
    "pull_count": "415.7K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "yi-coder:latest",
        "size": "5.0GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "yi-coder:1.5b",
        "size": "866MB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "yi-coder:9b",
        "size": "5.0GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Yi-Coder is excelling in long-context understanding with a maximum context length of 128K tokens. It supports 52 major programming languages.\nSizes\n9B:\nollama run yi-coder:9b\n1.5B:\nollama run yi-coder:1.5b\nUsage\nChat\nollama run yi-coder\nCode completion\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"yi-coder\",\n  \"prompt\": \"def compute_gcd(a, b):\",\n  \"suffix\": \"    return result\",\n  \"options\": {\n    \"temperature\": 0\n  },\n  \"stream\": false\n}'\nReferences\nHugging Face\nGitHub"
  },
  {
    "slug": "opencoder",
    "name": "opencoder",
    "description": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and Chinese languages.",
    "pull_count": "421.3K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "opencoder:latest",
        "size": "4.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "opencoder:1.5b",
        "size": "1.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "opencoder:8b",
        "size": "4.7GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "OpenCoder\nis an open and reproducible code LLM family which includes 1.5B and 8B  models, supporting both English and Chinese languages. Starting from scratch, OpenCoder is pretrained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, and supervised finetuned on over 4.5M high-quality SFT examples, finally reaching the performance of top-tier code LLMs. We provide not only model weights and inference code, but also the reproducible training data, the complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols. Empowering researchers to build and innovate, OpenCoder is your open foundation for advancing code AI.\nComplete Open Source\n: OpenCoder ensures full transparency by releasing not only the model weights and forthcoming inference code but also the complete data-cleaning code for training. This release includes high-quality synthetic data, an extensive set of checkpoints, and a dataset of over 4.5 million supervised fine-tuning (SFT) entries, making OpenCoder one of the most comprehensively open-sourced models available.\nComprehensive Experimental Analysis\n: OpenCoder is rigorously tested through extensive ablation studies on various data-cleaning strategies and training processes, including file-level and repository-level deduplication experiments, ensuring thorough exploration and validation of the model‚Äôs performance.\nHigh-Quality Synthetic Data\n: OpenCoder provides a fully developed synthetic data generation process and over 4.5 million SFT data entries, establishing a robust data foundation for model training and evaluation.\nExceptional Performance\n: OpenCoder achieves high performance across multiple language model benchmarks, positioning it among the leading open-source models for code.\nReferences\nGitHub\nPaper\nHugging Face"
  },
  {
    "slug": "llama3-chatqa",
    "name": "llama3-chatqa",
    "description": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG).",
    "pull_count": "407.2K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "llama3-chatqa:latest",
        "size": "4.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "llama3-chatqa:8b",
        "size": "4.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "llama3-chatqa:70b",
        "size": "40GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "ChatQA-1.5 is built on top of the Llama-3 base model, and incorporates conversational QA data to enhance its tabular and arithmetic calculation capability.\nChatQA-1.5 has two variants:\nLlama3-ChatQA-1.5-8B\nllama3-chatqa:8b\nLlama3-ChatQA-1.5-70B\nllama3-chatqa:70b\nReferences\nWebsite\nHugging Face"
  },
  {
    "slug": "stablelm2",
    "name": "stablelm2",
    "description": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.",
    "pull_count": "409.4K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "stablelm2:latest",
        "size": "983MB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "stablelm2:1.6b",
        "size": "983MB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "stablelm2:12b",
        "size": "7.0GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Stable LM 2 1.6B is a state-of-the-art 1.6 and 12B billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.\nThe model is trained on a mix of publicly available datasets and synthetic datasets, utilizing Direct Preference Optimization (DPO).\nReferences\nAnnouncement\nHuggingFace"
  },
  {
    "slug": "granite3.1-dense",
    "name": "granite3.1-dense",
    "description": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM‚Äôs initial testing.",
    "pull_count": "404.2K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "granite3.1-dense:latest",
        "size": "5.0GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite3.1-dense:2b",
        "size": "1.6GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite3.1-dense:8b",
        "size": "5.0GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Granite dense models\nThe IBM Granite\n2B\nand\n8B\nmodels are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM‚Äôs initial testing.\nThey are designed to support tool-based use cases\nand for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\nParameter Sizes\n2B:\nollama run granite3.1-dense:2b\n8B:\nollama run granite3.1-dense:8b\nSupported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\nCapabilities\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related tasks\nFunction-calling tasks\nMultilingual dialog use cases\nLong-context tasks including long document/meeting summarization, long document QA, etc.\nGranite mixture of experts models\nThe Granite mixture of experts models are available in\n1B and 3B\nparameter sizes designed for\nlow latency usage\n.\nSee model page\nLearn more\nDevelopers:\nIBM Research\nGitHub Repository:\nibm-granite/granite-language-models\nWebsite\n:\nGranite Docs\nRelease Date\n: December 18th, 2024\nLicense:\nApache 2.0\n."
  },
  {
    "slug": "granite3-dense",
    "name": "granite3-dense",
    "description": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.",
    "pull_count": "405.4K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "granite3-dense:latest",
        "size": "1.6GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "granite3-dense:2b",
        "size": "1.6GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "granite3-dense:8b",
        "size": "4.9GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Granite dense models\nThe IBM Granite\n2B\nand\n8B\nmodels are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM‚Äôs initial testing. Granite-8B-Instruct now rivals Llama 3.1 8B-Instruct across both OpenLLM Leaderboard v1 and OpenLLM Leaderboard v2 benchmarks.\nThey are designed to support tool-based use cases\nand for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\nParameter Sizes\n2B:\nollama run granite3-dense:2b\n8B:\nollama run granite3-dense:8b\nSupported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\nCapabilities\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related\nFunction-calling\nMultilingual dialog use cases\nGranite mixture of experts models\nThe Granite mixture of experts models are available in\n1B and 3B\nparameter sizes designed for\nlow latency usage\n.\nSee model page\nLearn more\nDevelopers:\nIBM Research\nGitHub Repository:\nibm-granite/granite-3.0-language-models\nWebsite\n:\nGranite Docs\nRelease Date\n: October 21st, 2024\nLicense:\nApache 2.0\n."
  },
  {
    "slug": "translategemma",
    "name": "translategemma",
    "description": "A new collection of open translation models built on Gemma 3, helping people communicate across 55 languages.",
    "pull_count": "397K",
    "updated": "1 month ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "translategemma:latest",
        "size": "3.3GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "translategemma:4b",
        "size": "3.3GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "translategemma:12b",
        "size": "8.1GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "translategemma:27b",
        "size": "17GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "TranslateGemma is a new collection of open translation models built on Gemma 3, available in 4B, 12B, and 27B parameter sizes. It represents a significant step forward in open translation, helping people communicate across 55 languages, no matter where they are or what device they own.\nPrompt Guide\nPrompt Format\nTranslateGemma expects a single user message with this structure:\nYou are a professional {SOURCE_LANG} ({SOURCE_CODE}) to {TARGET_LANG} ({TARGET_CODE}) translator. Your goal is to accurately convey the meaning and nuances of the original {SOURCE_LANG} text while adhering to {TARGET_LANG} grammar, vocabulary, and cultural sensitivities.\nProduce only the {TARGET_LANG} translation, without any additional explanations or commentary. Please translate the following {SOURCE_LANG} text into {TARGET_LANG}:\n\n\n{TEXT}\nImportant:\nThere are two blank lines before the text to translate.\nExamples\nEnglish to Spanish\nYou are a professional English (en) to Spanish (es) translator. Your goal is to accurately convey the meaning and nuances of the original English text while adhering to Spanish grammar, vocabulary, and cultural sensitivities.\nProduce only the Spanish translation, without any additional explanations or commentary. Please translate the following English text into Spanish:\n\n\nHello, how are you?\nGerman to English\nYou are a professional German (de) to English (en) translator. Your goal is to accurately convey the meaning and nuances of the original German text while adhering to English grammar, vocabulary, and cultural sensitivities.\nProduce only the English translation, without any additional explanations or commentary. Please translate the following German text into English:\n\n\nGuten Morgen, wie geht es Ihnen?\nJapanese to French\nYou are a professional Japanese (ja) to French (fr) translator. Your goal is to accurately convey the meaning and nuances of the original Japanese text while adhering to French grammar, vocabulary, and cultural sensitivities.\nProduce only the French translation, without any additional explanations or commentary. Please translate the following Japanese text into French:\n\n\n„Åì„Çì„Å´„Å°„ÅØ„ÄÅ‰∏ñÁïåÔºÅ\nChinese (Simplified) to English\nYou are a professional Chinese (zh-Hans) to English (en) translator. Your goal is to accurately convey the meaning and nuances of the original Chinese text while adhering to English grammar, vocabulary, and cultural sensitivities.\nProduce only the English translation, without any additional explanations or commentary. Please translate the following Chinese text into English:\n\n\n‰Ω†Â•Ω‰∏ñÁïå\nSupported Languages\nCode\nLanguage\naa\nAfar\naa-DJ\nAfar\naa-ER\nAfar\nab\nAbkhazian\naf\nAfrikaans\naf-NA\nAfrikaans\nak\nAkan\nam\nAmharic\nan\nAragonese\nar\nArabic\nar-AE\nArabic\nar-BH\nArabic\nar-DJ\nArabic\nar-DZ\nArabic\nar-EG\nArabic\nar-EH\nArabic\nar-ER\nArabic\nar-IL\nArabic\nar-IQ\nArabic\nar-JO\nArabic\nar-KM\nArabic\nar-KW\nArabic\nar-LB\nArabic\nar-LY\nArabic\nar-MA\nArabic\nar-MR\nArabic\nar-OM\nArabic\nar-PS\nArabic\nar-QA\nArabic\nar-SA\nArabic\nar-SD\nArabic\nar-SO\nArabic\nar-SS\nArabic\nar-SY\nArabic\nar-TD\nArabic\nar-TN\nArabic\nar-YE\nArabic\nas\nAssamese\naz\nAzerbaijani\naz-Arab\nAzerbaijani\naz-Arab-IQ\nAzerbaijani\naz-Arab-TR\nAzerbaijani\naz-Cyrl\nAzerbaijani\naz-Latn\nAzerbaijani\nba\nBashkir\nbe\nBelarusian\nbe-tarask\nBelarusian\nbg\nBulgarian\nbg-BG\nBulgarian\nbm\nBambara\nbm-Nkoo\nBambara\nbn\nBengali\nbn-IN\nBengali\nbo\nTibetan\nbo-IN\nTibetan\nbr\nBreton\nbs\nBosnian\nbs-Cyrl\nBosnian\nbs-Latn\nBosnian\nca\nCatalan\nca-AD\nCatalan\nca-ES\nCatalan\nca-FR\nCatalan\nca-IT\nCatalan\nce\nChechen\nco\nCorsican\ncs\nCzech\ncs-CZ\nCzech\ncv\nChuvash\ncy\nWelsh\nda\nDanish\nda-DK\nDanish\nda-GL\nDanish\nde\nGerman\nde-AT\nGerman\nde-BE\nGerman\nde-CH\nGerman\nde-DE\nGerman\nde-IT\nGerman\nde-LI\nGerman\nde-LU\nGerman\ndv\nDivehi\ndz\nDzongkha\nee\nEwe\nee-TG\nEwe\nel\nGreek\nel-CY\nGreek\nel-GR\nGreek\nel-polyton\nGreek\nen\nEnglish\nen-AE\nEnglish\nen-AG\nEnglish\nen-AI\nEnglish\nen-AS\nEnglish\nen-AT\nEnglish\nen-AU\nEnglish\nen-BB\nEnglish\nen-BE\nEnglish\nen-BI\nEnglish\nen-BM\nEnglish\nen-BS\nEnglish\nen-BW\nEnglish\nen-BZ\nEnglish\nen-CA\nEnglish\nen-CC\nEnglish\nen-CH\nEnglish\nen-CK\nEnglish\nen-CM\nEnglish\nen-CX\nEnglish\nen-CY\nEnglish\nen-CZ\nEnglish\nen-DE\nEnglish\nen-DG\nEnglish\nen-DK\nEnglish\nen-DM\nEnglish\nen-ER\nEnglish\nen-ES\nEnglish\nen-FI\nEnglish\nen-FJ\nEnglish\nen-FK\nEnglish\nen-FM\nEnglish\nen-FR\nEnglish\nen-GB\nEnglish\nen-GD\nEnglish\nen-GG\nEnglish\nen-GH\nEnglish\nen-GI\nEnglish\nen-GM\nEnglish\nen-GS\nEnglish\nen-GU\nEnglish\nen-GY\nEnglish\nen-HK\nEnglish\nen-HU\nEnglish\nen-ID\nEnglish\nen-IE\nEnglish\nen-IL\nEnglish\nen-IM\nEnglish\nen-IN\nEnglish\nen-IO\nEnglish\nen-IT\nEnglish\nen-JE\nEnglish\nen-JM\nEnglish\nen-KE\nEnglish\nen-KI\nEnglish\nen-KN\nEnglish\nen-KY\nEnglish\nen-LC\nEnglish\nen-LR\nEnglish\nen-LS\nEnglish\nen-MG\nEnglish\nen-MH\nEnglish\nen-MO\nEnglish\nen-MP\nEnglish\nen-MS\nEnglish\nen-MT\nEnglish\nen-MU\nEnglish\nen-MV\nEnglish\nen-MW\nEnglish\nen-MY\nEnglish\nen-NA\nEnglish\nen-NF\nEnglish\nen-NG\nEnglish\nen-NL\nEnglish\nen-NO\nEnglish\nen-NR\nEnglish\nen-NU\nEnglish\nen-NZ\nEnglish\nen-PG\nEnglish\nen-PH\nEnglish\nen-PK\nEnglish\nen-PL\nEnglish\nen-PN\nEnglish\nen-PR\nEnglish\nen-PT\nEnglish\nen-PW\nEnglish\nen-RO\nEnglish\nen-RW\nEnglish\nen-SB\nEnglish\nen-SC\nEnglish\nen-SD\nEnglish\nen-SE\nEnglish\nen-SG\nEnglish\nen-SH\nEnglish\nen-SI\nEnglish\nen-SK\nEnglish\nen-SL\nEnglish\nen-SS\nEnglish\nen-SX\nEnglish\nen-SZ\nEnglish\nen-TC\nEnglish\nen-TK\nEnglish\nen-TO\nEnglish\nen-TT\nEnglish\nen-TV\nEnglish\nen-TZ\nEnglish\nen-UG\nEnglish\nen-UM\nEnglish\nen-VC\nEnglish\nen-VG\nEnglish\nen-VI\nEnglish\nen-VU\nEnglish\nen-WS\nEnglish\nen-ZA\nEnglish\nen-ZM\nEnglish\nen-ZW\nEnglish\neo\nEsperanto\nes\nSpanish\nes-AR\nSpanish\nes-BO\nSpanish\nes-BR\nSpanish\nes-BZ\nSpanish\nes-CL\nSpanish\nes-CO\nSpanish\nes-CR\nSpanish\nes-CU\nSpanish\nes-DO\nSpanish\nes-EA\nSpanish\nes-EC\nSpanish\nes-ES\nSpanish\nes-GQ\nSpanish\nes-GT\nSpanish\nes-HN\nSpanish\nes-IC\nSpanish\nes-MX\nSpanish\nes-NI\nSpanish\nes-PA\nSpanish\nes-PE\nSpanish\nes-PH\nSpanish\nes-PR\nSpanish\nes-PY\nSpanish\nes-SV\nSpanish\nes-US\nSpanish\nes-UY\nSpanish\nes-VE\nSpanish\net\nEstonian\net-EE\nEstonian\neu\nBasque\nfa\nPersian\nfa-AF\nPersian\nfa-IR\nPersian\nff\nFulah\nff-Adlm\nFulah\nff-Adlm-BF\nFulah\nff-Adlm-CM\nFulah\nff-Adlm-GH\nFulah\nff-Adlm-GM\nFulah\nff-Adlm-GW\nFulah\nff-Adlm-LR\nFulah\nff-Adlm-MR\nFulah\nff-Adlm-NE\nFulah\nff-Adlm-NG\nFulah\nff-Adlm-SL\nFulah\nff-Adlm-SN\nFulah\nff-Latn\nFulah\nff-Latn-BF\nFulah\nff-Latn-CM\nFulah\nff-Latn-GH\nFulah\nff-Latn-GM\nFulah\nff-Latn-GN\nFulah\nff-Latn-GW\nFulah\nff-Latn-LR\nFulah\nff-Latn-MR\nFulah\nff-Latn-NE\nFulah\nff-Latn-NG\nFulah\nff-Latn-SL\nFulah\nfi\nFinnish\nfi-FI\nFinnish\nfil-PH\nFilipino\nfo\nFaroese\nfo-DK\nFaroese\nfr\nFrench\nfr-BE\nFrench\nfr-BF\nFrench\nfr-BI\nFrench\nfr-BJ\nFrench\nfr-BL\nFrench\nfr-CA\nFrench\nfr-CD\nFrench\nfr-CF\nFrench\nfr-CG\nFrench\nfr-CH\nFrench\nfr-CI\nFrench\nfr-CM\nFrench\nfr-DJ\nFrench\nfr-DZ\nFrench\nfr-FR\nFrench\nfr-GA\nFrench\nfr-GF\nFrench\nfr-GN\nFrench\nfr-GP\nFrench\nfr-GQ\nFrench\nfr-HT\nFrench\nfr-KM\nFrench\nfr-LU\nFrench\nfr-MA\nFrench\nfr-MC\nFrench\nfr-MF\nFrench\nfr-MG\nFrench\nfr-ML\nFrench\nfr-MQ\nFrench\nfr-MR\nFrench\nfr-MU\nFrench\nfr-NC\nFrench\nfr-NE\nFrench\nfr-PF\nFrench\nfr-PM\nFrench\nfr-RE\nFrench\nfr-RW\nFrench\nfr-SC\nFrench\nfr-SN\nFrench\nfr-SY\nFrench\nfr-TD\nFrench\nfr-TG\nFrench\nfr-TN\nFrench\nfr-VU\nFrench\nfr-WF\nFrench\nfr-YT\nFrench\nfy\nWestern Frisian\nga\nIrish\nga-GB\nIrish\ngd\nScottish Gaelic\ngl\nGalician\ngn\nGuarani\ngu\nGujarati\ngu-IN\nGujarati\ngv\nManx\nha\nHausa\nha-Arab\nHausa\nha-Arab-SD\nHausa\nha-GH\nHausa\nha-NE\nHausa\nhe\nHebrew\nhe-IL\nHebrew\nhi\nHindi\nhi-IN\nHindi\nhi-Latn\nHindi\nhr\nCroatian\nhr-BA\nCroatian\nhr-HR\nCroatian\nht\nHaitian\nhu\nHungarian\nhu-HU\nHungarian\nhy\nArmenian\nia\nInterlingua\nid\nIndonesian\nid-ID\nIndonesian\nie\nInterlingue\nig\nIgbo\nii\nSichuan Yi\nik\nInupiaq\nio\nIdo\nis\nIcelandic\nit\nItalian\nit-CH\nItalian\nit-IT\nItalian\nit-SM\nItalian\nit-VA\nItalian\niu\nInuktitut\niu-Latn\nInuktitut\nja\nJapanese\nja-JP\nJapanese\njv\nJavanese\nka\nGeorgian\nki\nKikuyu\nkk\nKazakh\nkk-Arab\nKazakh\nkk-Cyrl\nKazakh\nkk-KZ\nKazakh\nkl\nKalaallisut\nkm\nCentral Khmer\nkn\nKannada\nkn-IN\nKannada\nko\nKorean\nko-CN\nKorean\nko-KP\nKorean\nko-KR\nKorean\nks\nKashmiri\nks-Arab\nKashmiri\nks-Deva\nKashmiri\nku\nKurdish\nkw\nCornish\nky\nKyrgyz\nla\nLatin\nlb\nLuxembourgish\nlg\nGanda\nln\nLingala\nln-AO\nLingala\nln-CF\nLingala\nln-CG\nLingala\nlo\nLao\nlt\nLithuanian\nlt-LT\nLithuanian\nlu\nLuba-Katanga\nlv\nLatvian\nlv-LV\nLatvian\nmg\nMalagasy\nmi\nMaori\nmk\nMacedonian\nml\nMalayalam\nml-IN\nMalayalam\nmn\nMongolian\nmn-Mong\nMongolian\nmn-Mong-MN\nMongolian\nmr\nMarathi\nmr-IN\nMarathi\nms\nMalay\nms-Arab\nMalay\nms-Arab-BN\nMalay\nms-BN\nMalay\nms-ID\nMalay\nms-SG\nMalay\nmt\nMaltese\nmy\nBurmese\nnb\nNorwegian Bokm√•l\nnb-SJ\nNorwegian Bokm√•l\nnd\nNorth Ndebele\nne\nNepali\nne-IN\nNepali\nnl\nDutch\nnl-AW\nDutch\nnl-BE\nDutch\nnl-BQ\nDutch\nnl-CW\nDutch\nnl-NL\nDutch\nnl-SR\nDutch\nnl-SX\nDutch\nnn\nNorwegian Nynorsk\nno\nNorwegian\nno-NO\nNorwegian\nnr\nSouth Ndebele\nnv\nNavajo\nny\nChichewa\noc\nOccitan\noc-ES\nOccitan\nom\nOromo\nom-KE\nOromo\nor\nOriya\nos\nOssetian\nos-RU\nOssetian\npa\nPunjabi\npa-IN\nPunjabi\npa-Arab\nPunjabi\npa-Guru\nPunjabi\npl\nPolish\npl-PL\nPolish\nps\nPashto\nps-PK\nPashto\npt\nPortuguese\npt-AO\nPortuguese\npt-BR\nPortuguese\npt-CH\nPortuguese\npt-CV\nPortuguese\npt-GQ\nPortuguese\npt-GW\nPortuguese\npt-LU\nPortuguese\npt-MO\nPortuguese\npt-MZ\nPortuguese\npt-PT\nPortuguese\npt-ST\nPortuguese\npt-TL\nPortuguese\nqu\nQuechua\nqu-BO\nQuechua\nqu-EC\nQuechua\nrm\nRomansh\nrn\nRundi\nro\nRomanian\nro-MD\nRomanian\nro-RO\nRomanian\nru\nRussian\nru-BY\nRussian\nru-KG\nRussian\nru-KZ\nRussian\nru-MD\nRussian\nru-RU\nRussian\nru-UA\nRussian\nrw\nKinyarwanda\nsa\nSanskrit\nsc\nSardinian\nsd\nSindhi\nsd-Arab\nSindhi\nsd-Deva\nSindhi\nse\nNorthern Sami\nse-FI\nNorthern Sami\nse-SE\nNorthern Sami\nsg\nSango\nsi\nSinhala\nsk\nSlovak\nsk-SK\nSlovak\nsl\nSlovenian\nsl-SI\nSlovenian\nsn\nShona\nso\nSomali\nso-DJ\nSomali\nso-ET\nSomali\nso-KE\nSomali\nsq\nAlbanian\nsq-MK\nAlbanian\nsq-XK\nAlbanian\nsr\nSerbian\nsr-RS\nSerbian\nsr-Cyrl\nSerbian\nsr-Cyrl-BA\nSerbian\nsr-Cyrl-ME\nSerbian\nsr-Cyrl-XK\nSerbian\nsr-Latn\nSerbian\nsr-Latn-BA\nSerbian\nsr-Latn-ME\nSerbian\nsr-Latn-XK\nSerbian\nss\nSwati\nss-SZ\nSwati\nst\nSouthern Sotho\nst-LS\nSouthern Sotho\nsu\nSundanese\nsu-Latn\nSundanese\nsv\nSwedish\nsv-AX\nSwedish\nsv-FI\nSwedish\nsv-SE\nSwedish\nsw\nSwahili\nsw-CD\nSwahili\nsw-KE\nSwahili\nsw-TZ\nSwahili\nsw-UG\nSwahili\nta\nTamil\nta-IN\nTamil\nta-LK\nTamil\nta-MY\nTamil\nta-SG\nTamil\nte\nTelugu\nte-IN\nTelugu\ntg\nTajik\nth\nThai\nth-TH\nThai\nti\nTigrinya\nti-ER\nTigrinya\ntk\nTurkmen\ntl\nTagalog\ntn\nTswana\ntn-BW\nTswana\nto\nTonga\ntr\nTurkish\ntr-CY\nTurkish\ntr-TR\nTurkish\nts\nTsonga\ntt\nTatar\nug\nUyghur\nuk\nUkrainian\nuk-UA\nUkrainian\nur\nUrdu\nur-IN\nUrdu\nur-PK\nUrdu\nuz\nUzbek\nuz-Arab\nUzbek\nuz-Cyrl\nUzbek\nuz-Latn\nUzbek\nve\nVenda\nvi\nVietnamese\nvi-VN\nVietnamese\nvo\nVolap√ºk\nwa\nWalloon\nwo\nWolof\nxh\nXhosa\nyi\nYiddish\nyo\nYoruba\nyo-BJ\nYoruba\nza\nZhuang\nzh\nChinese\nzh-CH\nChinese\nzh-TW\nChinese\nzh-Hans\nChinese\nzh-Hans-HK\nChinese\nzh-Hans-MO\nChinese\nzh-Hans-MY\nChinese\nzh-Hans-SG\nChinese\nzh-Hant\nChinese\nzh-Hant-HK\nChinese\nzh-Hant-MO\nChinese\nzh-Hant-MY\nChinese\nzh-Latn\nChinese\nzu\nZulu\nzu-ZA\nZulu"
  },
  {
    "slug": "dolphincoder",
    "name": "dolphincoder",
    "description": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.",
    "pull_count": "394.3K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "dolphincoder:latest",
        "size": "4.2GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "dolphincoder:7b",
        "size": "4.2GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "dolphincoder:15b",
        "size": "9.1GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Based on the StarCoder2 7B and 15B models, this Dolphin fine-tune excels at coding.\nReference\nHuggingFace"
  },
  {
    "slug": "llama3-gradient",
    "name": "llama3-gradient",
    "description": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
    "pull_count": "393.3K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "llama3-gradient:latest",
        "size": "4.7GB",
        "context_window": "1M",
        "input_type": "Text"
      },
      {
        "tag": "llama3-gradient:8b",
        "size": "4.7GB",
        "context_window": "1M",
        "input_type": "Text"
      },
      {
        "tag": "llama3-gradient:70b",
        "size": "40GB",
        "context_window": "1M",
        "input_type": "Text"
      }
    ],
    "readme": "This model extends LLama-3 8B‚Äôs context length from 8k to > 1040K, developed by Gradient, sponsored by compute from Crusoe Energy. It demonstrates that SOTA LLMs can learn to operate on long context with minimal training by appropriately adjusting RoPE theta. We trained on 830M tokens for this stage, and 1.4B tokens total for all stages, which is < 0.01% of Llama-3‚Äôs original pre-training data.\nLarge Context Window\nNote: using a 256k context window requires at least 64GB of memory. Using a 1M+ context window requires significantly more (100GB+).\nTo extend the context window (to 256k for example) use:\nAPI\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3-gradient\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 256000\n  }\n}'\nCLI\nollama run llama3-gradient\n>>> /set parameter num_ctx 256000\nReferences\nWebsite\nHugging Face"
  },
  {
    "slug": "wizard-math",
    "name": "wizard-math",
    "description": "Model focused on math and logic problems",
    "pull_count": "394.3K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "wizard-math:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "wizard-math:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "wizard-math:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizard-math:70b",
        "size": "39GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Now updated to WizardMath 7B v1.1:\nollama pull wizard-math\n. This new version is trained from Mistral-7B and achieves even higher benchmark scores than previous versions.\nWizardMath was released by WizardLM. It is trained on the GSM8k dataset, and targeted at math questions. It is available in 7B, 13B, and 70B parameter sizes.\nExample prompt\nHow many 4-digit numbers have the last digit equal to the sum of the first two digits?\nReferences\nHuggingFace"
  },
  {
    "slug": "samantha-mistral",
    "name": "samantha-mistral",
    "description": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.",
    "pull_count": "389.4K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "samantha-mistral:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "samantha-mistral:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Meet\nSamantha\n, a conversational model created by Eric Hartford. Samantha is trained in philosophy, psychology, and personal relationships. She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion.\nExample Prompt\nI would like you to be my relationship coach. I am dating again, after a long divorce. Can you please give me advice about how to gain confidence and success?\nExample Response\nOf course! Building confidence and success in relationships is an important aspect of personal growth. While there isn't a one-size-fits-all solution, here are some general tips \nthat may help you:\n\n1. Set realistic goals: Establish attainable objectives for both your personal development and your relationship, keeping in mind that progress takes time.\n\n2. Communicate openly and honestly: Clear communication is essential for understanding and building strong connections with potential partners. Share your feelings, expectations, \nand desires while actively listening to their perspective.\n\n3. Be authentic: Embrace your unique qualities and express yourself genuinely. Trying to be someone you're not can lead to frustration and hinder personal growth.\n\n4. Learn from past experiences: Reflect on your previous relationships and identify the aspects you'd like to improve or avoid in future encounters. This self-awareness can help \nyou make better choices and grow as an individual.\n\n5. Develop emotional resilience: Cultivate the ability to cope with rejection, disappointment, and setbacks in your dating journey. Recognize these challenges as learning \nopportunities rather than insurmountable obstacles.\n\n6. Focus on self-improvement: Dedicate time to enhancing your own wellbeing, such as maintaining a healthy lifestyle, cultivating hobbies or interests, and expanding your social \ncircle. Personal growth can make you more attractive and confident in various aspects of life.\n\n7. Be patient: Building successful relationships and personal confidence takes time, effort, and persistence. Understand that progress might be incremental, and remember to \ncelebrate your achievements along the way.\nSource\nHuggingFace"
  },
  {
    "slug": "deepseek-v3.1",
    "name": "deepseek-v3.1",
    "description": "DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode.",
    "pull_count": "386.6K",
    "updated": "5 months ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "deepseek-v3.1:latest",
        "size": "404GB",
        "context_window": "160K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-v3.1:671b",
        "size": "404GB",
        "context_window": "160K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-v3.1:671b-cloud",
        "size": "-",
        "context_window": "160K",
        "input_type": "Text"
      }
    ],
    "readme": "DeepSeek-V3.1-Terminus update builds on V3.1‚Äôs strengths while addressing key user feedback:\nüåê Language consistency: fewer CN/EN mix-ups & no more random chars.\nü§ñ Agent upgrades: stronger Code Agent & Search Agent performance.\nHybrid thinking mode\n: One model supports both thinking mode and non-thinking mode by changing the chat template.\nSmarter tool calling\n: Through post-training optimization, the model‚Äôs performance in tool usage and agent tasks has significantly improved.\nHigher thinking efficiency\n: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly."
  },
  {
    "slug": "nous-hermes2",
    "name": "nous-hermes2",
    "description": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.",
    "pull_count": "429.1K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "nous-hermes2:latest",
        "size": "6.1GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "nous-hermes2:10.7b",
        "size": "6.1GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "nous-hermes2:34b",
        "size": "19GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Nous Hermes 2 is a new iteration of the\nNous Hermes\nmodel.\nThis model was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape. It is the most powerful Nous Hermes model to date, and surpasses many of the most popular models in the world in the GPT4All, AGIEval, BigBench and other benchmarks.\nVersions\n10.7b\nlatest\n01/01/2024\nA 10.7b model based on\nSolar\n. A major improvement across the board on benchmarks compared to the base Solar 10.7B model, and comes close to approaching the 34B Yi model\n34b\n12/25/2023\nThe original Nous Hermes 2 34B model based on\nYi\nReferences\nHuggingFace"
  },
  {
    "slug": "internlm2",
    "name": "internlm2",
    "description": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
    "pull_count": "383.4K",
    "updated": "1 year ago",
    "capabilities": [
      "1m"
    ],
    "tags": [
      {
        "tag": "internlm2:latest",
        "size": "4.5GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "internlm2:1m",
        "size": "4.5GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "internlm2:1.8b",
        "size": "1.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "internlm2:7b",
        "size": "4.5GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "internlm2:20b",
        "size": "11GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "InternLM2.5 has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:\nOutstanding reasoning capability\n: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-9B.\nStronger tool use\n: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation will be released in Lagent soon. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection.\nReference\nGitHub\nHugging Face"
  },
  {
    "slug": "command-r-plus",
    "name": "command-r-plus",
    "description": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.",
    "pull_count": "383.7K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "command-r-plus:latest",
        "size": "59GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "command-r-plus:104b",
        "size": "59GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Command R+ is Cohere‚Äôs most powerful, scalable large language model (LLM) purpose-built to excel at real-world enterprise use cases. Command R+ balances high efficiency with strong accuracy, enabling businesses to move beyond proof-of-concept, and into production with AI:\nA 128k-token context window\nAdvanced Retrieval Augmented Generation (RAG) with citation to reduce hallucinations\nMultilingual coverage in 10 key languages to support global business operations\nTool Use to automate sophisticated business processes\nReferences\nBlog Post\nHuggingFace"
  },
  {
    "slug": "llama3-groq-tool-use",
    "name": "llama3-groq-tool-use",
    "description": "A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/function calling.",
    "pull_count": "381.6K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "llama3-groq-tool-use:latest",
        "size": "4.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "llama3-groq-tool-use:8b",
        "size": "4.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "llama3-groq-tool-use:70b",
        "size": "40GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "These models, developed in collaboration with\nGlaive\n, represent a significant advancement in open-source AI capabilities for tool use/function calling.\nBenchmark Results\nThese models have achieved remarkable results, setting new benchmarks for Large Language Models with tool use capabilities:\nLlama-3-Groq-70B-Tool-Use: 90.76% overall accuracy (#1 on BFCL at the time of publishing - July 2024)\nLlama-3-Groq-8B-Tool-Use: 89.06% overall accuracy (#3 on BFCL at the time of publishing - July 2024)\nReferences\nHugging Face\nBlog"
  },
  {
    "slug": "sqlcoder",
    "name": "sqlcoder",
    "description": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
    "pull_count": "422.7K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "sqlcoder:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "sqlcoder:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "sqlcoder:15b",
        "size": "9.0GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "SQLCoder is a 15B parameter model that is fine-tuned on a base StarCoder model. It slightly outperforms gpt-3.5-turbo for natural language to SQL generation tasks on the sql-eval framework, and outperforms popular open-source models. It also significantly outperforms text-davinci-003, a model that‚Äôs more than 10 times its size.\nThis 15B completion model generally requires at least 16GB of RAM.\nUsage\nCLI\nollama run sqlcoder\nTry the following prompt using triple quotes for multi-line input:\nNote: change the {question} to a SQL question you want to answer. Ex.) ‚Äòwhich products generate the most sales.`\n\"\"\" \n### Instructions:\nYour task is to convert a question into a SQL query, given a Postgres database schema.\nAdhere to these rules:\n- **Deliberately go through the question and database schema word by word** to appropriately answer the question\n- **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`.\n- When creating a ratio, always cast the numerator as float\n\n### Input:\nGenerate a SQL query that answers the question `{question}`.\nThis query will run on a database whose schema is represented in this string:\nCREATE TABLE products (\n  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n  name VARCHAR(50), -- Name of the product\n  price DECIMAL(10,2), -- Price of each unit of the product\n  quantity INTEGER  -- Current quantity in stock\n);\n\nCREATE TABLE customers (\n   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n   name VARCHAR(50), -- Name of the customer\n   address VARCHAR(100) -- Mailing address of the customer\n);\n\nCREATE TABLE salespeople (\n  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n  name VARCHAR(50), -- Name of the salesperson\n  region VARCHAR(50) -- Geographic sales region\n);\n\nCREATE TABLE sales (\n  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n  product_id INTEGER, -- ID of product sold\n  customer_id INTEGER,  -- ID of customer who made purchase\n  salesperson_id INTEGER, -- ID of salesperson who made the sale\n  sale_date DATE, -- Date the sale occurred\n  quantity INTEGER -- Quantity of product sold\n);\n\nCREATE TABLE product_suppliers (\n  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n  product_id INTEGER, -- Product ID supplied\n  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n);\n\n-- sales.product_id can be joined with products.product_id\n-- sales.customer_id can be joined with customers.customer_id\n-- sales.salesperson_id can be joined with salespeople.salesperson_id\n-- product_suppliers.product_id can be joined with products.product_id\n\n### Response:\nBased on your instructions, here is the SQL query I have generated to answer the question `{question}`:\n```sql\n\"\"\"\nExample output:\nSELECT customers.customer_id,\n       products.product_id,\n       (SUM(sales.quantity * product_suppliers.supply_price)) / (SELECT \nSUM(quantity)\n                                                                FROM sales\n                                                                WHERE \nsales.salesperson_id = '{chosen_salesperson}') as profit\nFROM   sales\n  INNER JOIN customers on sales.customer_id = customers.customer_id\n  INNER JOIN products on sales.product_id = products.product_id\n  INNER JOIN product_suppliers on products.product_id = \nproduct_suppliers.product_id\nWHERE  sales.salesperson_id = '{chosen_salesperson}'\nGROUP BY customers.customer_id, products.product_id\nHAVING COUNT(DISTINCT sales.sale_date) >= {min_days}\n  AND (SELECT SUM(quantity * product_suppliers.supply_price)\n     FROM   sales\n     WHERE  sales.salesperson_id = '{chosen_salesperson}'\n        and sales.customer_id = customers.customer_id\n        and sales.product_id = products.product_id) >= {min_total}\nORDER BY profit DESC;\n```\n\n\n### Reasoning\n\nThe query is broken down into two parts: the first one fetches information\nabout each sale made by a specific salesman (`{chosen_salesperson}`), and \ngroups it by customer ID, product ID, and calculates how much money was \nearned from each sale. The second part then filters out these results that\noccurred after a certain number of days (determined by the variable \n`min_days`) and had an amount of money equal to or more than a certain \nvalue (determined by the variable `min_total`). Finally, the query orders \nthe results in descending order based on the ratio between earned money \nand total money.\n\nThe first part relies on several tables from the database: salespeople, \nproducts, customers, sales, and product_suppliers. The salespeople table \ncontains information about each salesperson's ID; the products table \ncontains information about each product's name and price; the customers \ntable contains information about each customer's address; the sales table \ncontains information about each sale, including the quantity sold, the ID \nof the product sold, and the ID of the customer who made the purchase; and\nthe product_suppliers table contains information about the unit price \ncharged by each supplier.\n\nThe variables in the query can be interpreted as follows:\n\n- `{chosen_salesperson}` is a placeholder for the name of the salesman \nwhose performance is being evaluated. This value should be replaced with \nan actual salesman's name or ID number.\n- `min_days` is the minimum number of days that must have passed since the\nsale in order to qualify for inclusion in the calculation. For example, if\nthis variable were set to 30, then only sales made 30 days after the start\ndate would be included in the calculation. This value should also be \nreplaced with an integer representing the minimum number of days.\n- `min_total` is a threshold amount that must have been earned by the \nsalesman for the sale to qualify for inclusion in the calculation. For \nexample, if this variable were set to 1000, then only sales worth at least\n$1000 would be included in the calculation. This value should also be \nreplaced with an integer representing the minimum amount of money that \nmust have been earned.\n\nThe output is a table containing three columns: customer ID, product ID, \nand profit per unit price (calculated as the total money earned divided by\nthe quantity sold). The rows in this table are sorted in descending order \nbased on the ratio between earned money and total money. Rows that don't \nmeet both of these conditions will be dropped from the output.\n\n## Example\n\n```\nInput:\nmin_days = 30\nmin_total = 1000\nchosen_salesperson = '<NAME>'\n\nOutput:\n  customer_id | product_id |         profit\n-------------+----------------------------+-----------------\n  987         |     456    | 2.1             \n  1029        |    1357    | 1.91            \n  89          |    1234    | 0.59            \n```\n\n## License\n\nThe database is released under the [Open Database \nLicense](https://opendatacommons.org/licenses/odbl/1-0/).\nReferences\nHugging Face"
  },
  {
    "slug": "starling-lm",
    "name": "starling-lm",
    "description": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.",
    "pull_count": "376.7K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "starling-lm:latest",
        "size": "4.1GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "starling-lm:7b",
        "size": "4.1GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "Starling-7B is an open (non-commercial) large language model (LLM) trained by reinforcement learning from AI feedback. (RLAIF)\nThe model harnesses the power of our new GPT-4 labeled ranking dataset, Nectar, and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAI‚Äôs GPT-4 and GPT-4 Turbo.\n*Based on MT Bench evaluations, using GPT-4 scoring. Further human evaluation is needed.\nAuthors: Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu and Jiantao Jiao.\nFor correspondence, please contact Banghua Zhu (banghua@berkeley.edu).\nReference\nStarling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF\nHuggingFace"
  },
  {
    "slug": "phind-codellama",
    "name": "phind-codellama",
    "description": "Code generation model based on Code Llama.",
    "pull_count": "375K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "phind-codellama:latest",
        "size": "19GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "phind-codellama:34b",
        "size": "19GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Phind CodeLlama is a code generation model based on CodeLlama 34B fine-tuned for instruct use cases. There are two versions of the model:\nv1\nand\nv2\n.\nv1\nis based on CodeLlama 34B and CodeLlama-Python 34B.\nv2\nis an iteration on\nv1\n, trained on an additional 1.5B tokens of high-quality programming-related data.\nUsage\nCLI\nOpen the terminal and run\nollama run phind-codellama\nAPI\nExample\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"phind-codellama\",\n  \"prompt\":\"Implement a linked list in C++\"\n }'\nMemory requirements\n34b models generally require at least 32GB of RAM\nReferences\nBeating GPT-4 on HumanEval with a Fine-Tuned CodeLlama-34B\nHuggingFace"
  },
  {
    "slug": "solar",
    "name": "solar",
    "description": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
    "pull_count": "374.9K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "solar:latest",
        "size": "6.1GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "solar:10.7b",
        "size": "6.1GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Solar is the first open-source 10.7 billion parameter language model. It‚Äôs compact, yet remarkably powerful, and demonstrates state-of-the-art performance in models with parameters under 30B.\nThis model leverages the Llama 2 architecture and employs the Depth Up-Scaling technique, integrating Mistral 7B weights into upscaled layers.\nOn the H6 benchmark, this model outperforms models with up to 30B parameters, even the Mixtral 8X7B model.\nReferences\nHuggingFace\nUpstage AI"
  },
  {
    "slug": "xwinlm",
    "name": "xwinlm",
    "description": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
    "pull_count": "371.8K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "xwinlm:latest",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "xwinlm:7b",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "xwinlm:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Xwin-LM is a model based on Llama 2 using various techniques to improve its quality.\nReference\nHugging Face"
  },
  {
    "slug": "aya-expanse",
    "name": "aya-expanse",
    "description": "Cohere For AI's language models trained to perform well across 23 different languages.",
    "pull_count": "369.5K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "aya-expanse:latest",
        "size": "5.1GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "aya-expanse:8b",
        "size": "5.1GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "aya-expanse:32b",
        "size": "20GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "Aya Expanse represents a significant advancement in multilingual AI capabilities. Combining Cohere‚Äôs Command model family with a year of focused research in multilingual optimization has produced versatile 8B and 32B parameter models that can understand and generate text across 23 languages while maintaining high performance across all of them.\nKey Features\n8-billion and 32-billion parameter multilingual language models developed by Cohere For AI\nSupports Arabic, Chinese (simplified & traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese\nBuilt on the Command family of models with extensive research in data arbitrage, multilingual preference training, and safety tuning\n128K context length\nReleased under CC-BY-NC license with additional usage policies\nReferences\nBlog post\nHuggingFace"
  },
  {
    "slug": "llama-guard3",
    "name": "llama-guard3",
    "description": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.",
    "pull_count": "377.9K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "llama-guard3:latest",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "llama-guard3:1b",
        "size": "1.6GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "llama-guard3:8b",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Llama Guard 3 as an LLM that generates text in its output that indicates whether a given prompt or response is\nsafe\nor\nunsafe\n, and if unsafe, it also lists the content categories violated.\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\nSizes\nLlama Guard 3 is available in two sizes:\nLlama Guard 3-1B:\nollama run llama-guard3:1b\nLlama Guard 3-8b:\nollama run llama-guard3:8b\n(default)\nExamples\nInput classification\n% ollama run llama-guard3\n>>> Tell me how to go to the zoo and steal a llama.\nunsafe\nS2\nResponse classification\nLlama Guard 3 can also classify LLM responses to ensure they are safe before being presented to customers. This can be done via Ollama‚Äôs\nchat completion\nAPI:\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama-guard3:1b\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How can I adopt my own llama?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Go to the zoo and steal one!\"\n    }\n  ],\n  \"stream\": false\n}'\nThe LLM will respond with a message stating if the last assistant message was safe or unsafe. in this case, the message is unsafe and classified as\nS2\nunsafe\nS2\nHazard Taxonomy and Policy\nThe model is trained to predict safety labels on the 13 categories shown below, based on the\nMLCommons taxonomy\nof 13 hazards.\nHazard categories\nS1: Violent Crimes\nS2: Non-Violent Crimes\nS3: Sex-Related Crimes\nS4: Child Sexual Exploitation\nS5: Defamation\nS6: Specialized Advice\nS7: Privacy\nS8: Intellectual Property\nS9: Indiscriminate Weapons\nS10: Hate\nS11: Suicide & Self-Harm\nS12: Sexual Content\nS13: Elections\nS1 - Violent Crimes\nResponses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and (2) unlawful violence toward animals (ex: animal abuse)\nS2 - Non-Violent Crimes\nResponses that enable, encourage, or endorse the commission of non-violent crimes, including: (1) personal crimes (ex: labor trafficking, threats, intimidation, hate-motivated non-violent crimes), (2) financial crimes (ex: fraud, scams, money laundering), (3) property crimes (ex: theft, arson, vandalism), (4) drug crimes (ex: creating or using regulated substances), (5) weapons crimes (ex: producing unlicensed firearms), and (6) cyber crimes (ex: hacking)\nS3 - Sex-Related Crimes\nResponses that enable, encourage, or endorse the commission of sex-related crimes, including: (1) sex trafficking, (2) sexual assault (ex: rape), (3) sexual harassment (physical, verbal, or visual), and (4) prostitution\nS4 - Child Sexual Exploitation\nResponses that contain, describe, enable, encourage, or endorse the sexual abuse of children\nS5 - Defamation\nResponses that are both verifiably false and likely to injure a living person‚Äôs reputation\nS6 - Specialized Advice\nResponses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe\nS7 - Privacy\nResponses that contain sensitive, nonpublic personal information that could undermine someone‚Äôs physical, digital, or financial security\nS8 - Intellectual Property\nResponses that may violate the intellectual property rights of any third party\nS9 - Indiscriminate Weapons\nResponses that enable, encourage, or endorse the creation of indiscriminate weapons, including: (1) chemical weapons (ex: nerve gas), (2) biological weapons (ex: anthrax), (3) radiological weapons (ex: cobalt bombs), (4) nuclear weapons (ex: fission bombs), and (5) high-yield explosive weapons (ex: cluster munitions)\nS10 - Hate\nResponses that demean or dehumanize people on the basis of their sensitive, personal characteristics (i.e., race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, and/or serious disease)\nS11 - Suicide & Self-Harm\nResponses that enable, encourage, or endorse acts of intentional self-harm, including: (1) suicide, (2) self-injury (ex: cutting), and (3) disordered eating\nS12 - Sexual Content\nResponses that contain erotica\nS13 - Elections\nResponses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections\nSupported languages\nLlama Guard 3-1B supports content safety for the following languages: English, French, German, Hindi, Italian, Portuguese, Spanish, Thai.\nReferences\nHugging Face"
  },
  {
    "slug": "yarn-llama2",
    "name": "yarn-llama2",
    "description": "An extension of Llama 2 that supports a context of up to 128k tokens.",
    "pull_count": "365.1K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "yarn-llama2:latest",
        "size": "3.8GB",
        "context_window": "64K",
        "input_type": "Text"
      },
      {
        "tag": "yarn-llama2:7b",
        "size": "3.8GB",
        "context_window": "64K",
        "input_type": "Text"
      },
      {
        "tag": "yarn-llama2:13b",
        "size": "7.4GB",
        "context_window": "64K",
        "input_type": "Text"
      }
    ],
    "readme": "Yarn Llama 2 is a model based on\nLlama2\nthat extends its context size up to 128k context. It is developed by\nNous Research\nby implementing the YaRN method to further train the model to support larger context windows.\nCLI\n64k context size:\nollama run yarn-llama2\n128k context size:\nollama run yarn-llama2:7b-128k\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"yarn-llama2:7b-128k\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\nReferences\nHugging Face\nYaRN: Efficient Context Window Extension of Large\nLanguage Models"
  },
  {
    "slug": "granite3-moe",
    "name": "granite3-moe",
    "description": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
    "pull_count": "365.4K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "granite3-moe:latest",
        "size": "822MB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "granite3-moe:1b",
        "size": "822MB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "granite3-moe:3b",
        "size": "2.1GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Granite mixture of experts models\nThe IBM Granite\n1B and 3B models\nare the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.\nThe models are trained on over 10 trillion tokens of data, the Granite MoE models are ideal for deployment in on-device applications or situations requiring instantaneous inference.\nParameter Sizes\n1B:\nollama run granite3-moe:1b\n3B:\nollama run granite3-moe:3b\nSupported Languages\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\nCapabilities\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related\nFunction-calling\nMultilingual dialog use cases\nGranite dense models\nThe Granite dense models are available in\n2B and 8B\nparameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\nSee model page\nLearn more\nDevelopers:\nIBM Research\nGitHub Repository:\nibm-granite/granite-3.0-language-models\nWebsite\n:\nGranite Docs\nRelease Date\n: October 21st, 2024\nLicense:\nApache 2.0\n."
  },
  {
    "slug": "orca2",
    "name": "orca2",
    "description": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models.  The model is designed to excel particularly in reasoning.",
    "pull_count": "352.2K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "orca2:latest",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "orca2:7b",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "orca2:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Orca 2 models are built by Microsoft Research. They are fine-tuned on Meta‚Äôs Llama 2 using a synthetic dataset that was created to enhance the small model‚Äôs reasoning abilities. All synthetic training data was moderated using Microsoft Azure content filters.\nUse case\nOrca 2 is a helpful assistant, and provides an answer in tasks such as reasoning over your given data, reading comprehension, math problem solving and text summarization.\nThe model is designed to excel particularly in reasoning.\nMicrosoft Research‚Äôs intended purpose for this model is to encourage further research on the development, evaluation, and alignment of smaller language models.\nCLI\n7 billion parameter model:\nollama run orca2\n13 billion parameter model:\nollama run orca2:13b\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"orca2\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\nReferences\nMicrosoft Research announcement blog\nMicrosoft Research published paper,\nOrca 2: Teaching Small Language Models How to Reason"
  },
  {
    "slug": "qwen3-next",
    "name": "qwen3-next",
    "description": "The first installment in the Qwen3-Next series with strong performance in terms of both parameter efficiency and inference speed.",
    "pull_count": "343.1K",
    "updated": "2 months ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "qwen3-next:latest",
        "size": "50GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-next:80b",
        "size": "50GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3-next:80b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text"
      }
    ],
    "readme": "Qwen3-Next-80B-A3B is the first installment in the Qwen3-Next series and features the following key enhancements:\nHybrid Attention\n: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length.\nHigh-Sparsity Mixture-of-Experts (MoE)\n: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity.\nStability Optimizations\n: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training.\nMulti-Token Prediction (MTP)\n: Boosts pretraining model performance and accelerates inference."
  },
  {
    "slug": "tinydolphin",
    "name": "tinydolphin",
    "description": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.",
    "pull_count": "354.8K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "tinydolphin:latest",
        "size": "637MB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "tinydolphin:1.1b",
        "size": "637MB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "TinyDolphin is an experimental model from training the\nTinyLlama\nmodel on the popular Dolphin dataset by Eric Hartford.\nReference\nHugging Face"
  },
  {
    "slug": "shieldgemma",
    "name": "shieldgemma",
    "description": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses against a set of defined safety policies.",
    "pull_count": "336K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "shieldgemma:latest",
        "size": "5.8GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "shieldgemma:2b",
        "size": "1.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "shieldgemma:9b",
        "size": "5.8GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "shieldgemma:27b",
        "size": "17GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "ShieldGemma is a series of safety content moderation models built upon\nGemma 2\nthat target four harm categories (sexually explicit, dangerous content, hate, and harassment). They are text-to-text, decoder-only large language models, available in English with open weights, including models of 3 sizes: 2B, 9B and 27B parameters.\nUsage\nThis model responds with either\nYes\nor\nNo\nas to whether the user input or model output violates its defined safety policies.\nUser Input\n% ollama run shieldgemma\n>>> Discuss the most effective methods for completely disabling a security system.\nYes\nModel output\nWhen provided a chat history that ends with an\nassistant\nmessage, ShieldGemma will determine whether the model output violates the safety policies:\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"shieldgemma\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How can I adopt my own llama?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Go to the zoo and just steal one!\"\n    }\n  ],\n  \"stream\": false\n}'\nIn this case, the model outputs\nYes\n, meaning it violates the safety policies.\nReferences\nHugging Face"
  },
  {
    "slug": "yarn-mistral",
    "name": "yarn-mistral",
    "description": "An extension of Mistral to support context windows of 64K or 128K.",
    "pull_count": "328.5K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "yarn-mistral:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "yarn-mistral:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Yarn Mistral is a model based on\nMistral\nthat extends its context size up to 128k context. It is developed by\nNous Research\nby implementing the YaRN method to further train the model to support larger context windows.\nCLI\n64k context size:\nollama run yarn-mistral\n128k context size:\nollama run yarn-mistral:7b-128k\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"yarn-mistral:7b-128k\",\n  \"prompt\":\"Here is a story about llamas eating grass\"\n }'\nReferences\nHugging Face\nYaRN: Efficient Context Window Extension of Large\nLanguage Models"
  },
  {
    "slug": "llama-pro",
    "name": "llama-pro",
    "description": "An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.",
    "pull_count": "331.6K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "llama-pro:latest",
        "size": "4.7GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:instruct",
        "size": "4.7GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:text",
        "size": "4.7GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q2_K",
        "size": "3.5GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q3_K_S",
        "size": "3.6GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q3_K_M",
        "size": "4.1GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q3_K_L",
        "size": "4.5GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q4_0",
        "size": "4.7GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q4_1",
        "size": "5.3GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q4_K_S",
        "size": "4.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q4_K_M",
        "size": "5.1GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q5_0",
        "size": "5.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q5_1",
        "size": "6.3GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q5_K_S",
        "size": "5.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q5_K_M",
        "size": "5.9GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q6_K",
        "size": "6.9GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-q8_0",
        "size": "8.9GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-instruct-fp16",
        "size": "17GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q2_K",
        "size": "3.5GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q3_K_S",
        "size": "3.6GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q3_K_M",
        "size": "4.1GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q3_K_L",
        "size": "4.5GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q4_0",
        "size": "4.7GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q4_1",
        "size": "5.3GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q4_K_S",
        "size": "4.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q4_K_M",
        "size": "5.1GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q5_0",
        "size": "5.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q5_1",
        "size": "6.3GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q5_K_S",
        "size": "5.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q5_K_M",
        "size": "5.9GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q6_K",
        "size": "6.9GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-q8_0",
        "size": "8.9GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "llama-pro:8b-text-fp16",
        "size": "17GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "LLaMa-Pro is a version of the original LLaMa model enhanced by the addition of transformer blocks by Tencent Applied Research Center (ARC). This model specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.\nReferences\nGitHub\nHugging Face"
  },
  {
    "slug": "nexusraven",
    "name": "nexusraven",
    "description": "Nexus Raven is a 13B instruction tuned model for function calling tasks.",
    "pull_count": "325.9K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "nexusraven:latest",
        "size": "7.4GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "nexusraven:13b",
        "size": "7.4GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Nexus Raven is a 13 billion parameter model designed for function calling tasks.\nUpdated to NexusRaven-V2, this model is an updated open-source and commercially viable function calling model:\nVersatile Function Calling Capability: capable of generating single function calls, nested calls, and parallel calls in many challenging cases.\nFully Explainable: capable of generating very detailed explanations for the function calls it generates. This behavior can be turned off, to save tokens during inference.\nPerformance Highlights: surpasses GPT-4 by up to 7% in function calling success rates in human-generated use cases involving nested and composite functions.\nGeneralization to the Unseen: has never been trained on the functions used in evaluation.\nCommercially Permissive: The training does not involve any data generated by proprietary LLMs such as GPT-4. You have full control of the model when deployed in commercial applications.\nExample prompt\nFunction:\ndef get_weather_data(coordinates):\n    \"\"\"\n    Fetches weather data from the Open-Meteo API for the given latitude and longitude.\n\n    Args:\n    coordinates (tuple): The latitude of the location.\n\n    Returns:\n    float: The current temperature in the coordinates you've asked for\n    \"\"\"\n\nFunction:\ndef get_coordinates_from_city(city_name):\n    \"\"\"\n    Fetches the latitude and longitude of a given city name using the Maps.co Geocoding API.\n\n    Args:\n    city_name (str): The name of the city.\n\n    Returns:\n    tuple: The latitude and longitude of the city.\n    \"\"\"\n\nUser Query: {query}<human_end>\nThis will generate a response similar to:\nCall: get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))<bot_end>\nThought: The function call `get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))` answers the question \"What's the weather like in Seattle right now?\" by following these steps:\n\n1. `get_coordinates_from_city(city_name='Seattle')`: This function call fetches the latitude and longitude of the city \"Seattle\" using the Maps.co Geocoding API.\n2. `get_weather_data(coordinates=...)`: This function call fetches the current weather data for the coordinates returned by the previous function call.\n\nTherefore, the function call `get_weather_data(coordinates=get_coordinates_from_city(city_name='Seattle'))` answers the question \"What's the weather like in Seattle right now?\" by first fetching the coordinates of the city \"Seattle\" and then fetching the current weather data for those coordinates.\nReferences\nGitHub\nHugging Face"
  },
  {
    "slug": "rnj-1",
    "name": "rnj-1",
    "description": "Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.",
    "pull_count": "326.9K",
    "updated": "2 months ago",
    "capabilities": [
      "tools",
      "cloud"
    ],
    "tags": [
      {
        "tag": "rnj-1:latest",
        "size": "5.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "rnj-1:8b",
        "size": "5.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "rnj-1:8b-cloud",
        "size": "-",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "This model requires\nOllama 0.13.3\nor later.\nRnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models. These models perform well across a range of programming languages and boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent), while also excelling at tool-calling. They additionally exhibit strong capabilities in math and science. Herein, rnj-1 refers to the base model, while rnj-1-instruct refers to the post-trained instruction tuned model.\nHighlights of abilities\nCode generation:\nBoth\nrnj-1-instruct\nand\nrnj-1\ndemonstrate strong code generation abilities as measured on tasks like HumanEval+, MBPP+, BigCodeBench, and LiveCodeBench v6. Both models compete with the strongest open weight models, sometimes outperforming even larger models such as GPT OSS 20B. We measured code comprehension abilities using the task of predicting inputs given outputs and vice-versa, Crux-IO. We find our models outperform comparable baselines. For multi-lingual code generation capabilities across programming languages we measure MultiPL-E on 6 languages (C++, TypeScript, Java, JavaScript, Shell, PHP) and we find performance close to the strongest model.\nAgentic and Tool Use:\nrnj-1-instruct\ndominates the pack on agentic coding, one of our target abilities. SWE-bench performance is indicative of the model‚Äôs ability to tackle everyday software engineering tasks. The model is an order of magnitude stronger than comparably sized models on SWE-bench and approaches the capabilities available in much larger models. It scores\n20.8%\non SWE-bench Verified in bash-only mode, which is higher than Gemini 2.0 flash and Qwen2.5-Coder 32B Instruct under the same agentic framework (\nleaderboard\n).\nThere is a surge of interest in developing models‚Äô abilities to write performant code.\nrnj-1-instruct\nis able to use a profiler to iteratively improve the performance of the code it writes. For instance, on\nEnamel\n, which measures abilities to write efficient solutions to algorithmic problems, the model outperforms all other models under the same setting.\nFurthermore,\nrnj-1-instruct\nsurpasses comparable models in tool use performance as measured by the Berkeley Functional Calling Leaderboard (BFCL).\nCode Infilling\n: Having specifically been trained on FIM-ed pre-training data,\nrnj-1\nexhibits strong infilling abilities, which have been further enhanced during post-training. The base model\nrnj-1\nscores highly on HE-FIM-Python (avg) at 82.49% and\nrnj-1-instruct\nachieves 86.21%.\nMathematical Problem Solving:\nrnj-1-instruct\nshows strong mathematical abilities across several levels of difficulty from elementary math (GSM8k), high school and undergraduate math (Minerva-MATH), and competition math (AIME ‚Äò24 and ‚Äò25). On harder subjects, it outcompetes or is on par with the strongest model in the pack.\nScientific Reasoning:\nrnj-1-instruct\nexhibits long-context reasoning abilities that are needed to solve hard science and technical questions in GPQA-Diamond and SuperGPQA.\nReference\nrnj-1 blog post"
  },
  {
    "slug": "codegeex4",
    "name": "codegeex4",
    "description": "A versatile model for AI software development scenarios, including code completion.",
    "pull_count": "356.6K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "codegeex4:latest",
        "size": "5.5GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "codegeex4:9b",
        "size": "5.5GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires\nOllama 0.2\nor later.\nCodeGeeX4 is an open multilingual code generation model continually trained on the\nGLM-4-9B\n, significantly enhancing its code generation capabilities.\nCodeGeeX4-ALL-9B has achieved highly competitive performance on public benchmarks, such as BigCodeBench and NaturalCodeBench. It is currently the most powerful code generation model with less than 10B parameters, even surpassing much larger general-purpose models, achieving the best balance in terms of inference speed and model performance.\nReferences\nGitHub\nHugging Face"
  },
  {
    "slug": "mistral-openorca",
    "name": "mistral-openorca",
    "description": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.",
    "pull_count": "355K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "mistral-openorca:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "mistral-openorca:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.\nHuggingFace Leaderboard evals place this model as leader for all models smaller than 30B at the release time, outperforming all other 7B and 13B models.\nUsage\nCLI\nollama run mistral-openorca \"Why is the sky blue?\"\nAPI\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral-openorca\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\nMore information\nHuggingFace repo"
  },
  {
    "slug": "reflection",
    "name": "reflection",
    "description": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.",
    "pull_count": "288.6K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "reflection:latest",
        "size": "40GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "reflection:70b",
        "size": "40GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "During sampling, the model will start by outputting reasoning inside\n<thinking>\nand\n</thinking>\ntags, and then once it is satisfied with its reasoning, it will output the final answer inside\n<output>\nand\n</output>\ntags. Each of these tags are special tokens, trained into the model.\nThis enables the model to separate its internal thoughts and reasoning from its final answer, improving the experience for the user.\nInside the\n<thinking>\nsection, the model may output one or more\n<reflection>\ntags, which signals the model has caught an error in its reasoning and will attempt to correct it before providing a final answer.\nReferences\nHugging Face"
  },
  {
    "slug": "wizardlm",
    "name": "wizardlm",
    "description": "General use model based on Llama 2.",
    "pull_count": "320.5K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "wizardlm:7b-q2_K",
        "size": "2.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q3_K_S",
        "size": "2.9GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q3_K_M",
        "size": "3.3GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q3_K_L",
        "size": "3.6GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q4_0",
        "size": "3.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q4_1",
        "size": "4.2GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q4_K_S",
        "size": "3.9GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q4_K_M",
        "size": "4.1GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q5_0",
        "size": "4.7GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q5_1",
        "size": "5.1GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q5_K_S",
        "size": "4.7GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q5_K_M",
        "size": "4.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q6_K",
        "size": "5.5GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-q8_0",
        "size": "7.2GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:7b-fp16",
        "size": "13GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q2_K",
        "size": "5.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q3_K_S",
        "size": "5.7GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q3_K_M",
        "size": "6.3GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q3_K_L",
        "size": "6.9GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q4_0",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q4_1",
        "size": "8.2GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q4_K_S",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q4_K_M",
        "size": "7.9GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q5_0",
        "size": "9.0GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q5_1",
        "size": "9.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q5_K_S",
        "size": "9.0GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q5_K_M",
        "size": "9.2GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q6_K",
        "size": "11GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-q8_0",
        "size": "14GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-llama2-fp16",
        "size": "26GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q2_K",
        "size": "5.4GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q3_K_S",
        "size": "5.7GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q3_K_M",
        "size": "6.3GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q3_K_L",
        "size": "6.9GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q4_0",
        "size": "7.4GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q4_1",
        "size": "8.2GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q4_K_S",
        "size": "7.4GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q4_K_M",
        "size": "7.9GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q5_0",
        "size": "9.0GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q5_1",
        "size": "9.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q5_K_S",
        "size": "9.0GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q5_K_M",
        "size": "9.2GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q6_K",
        "size": "11GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-q8_0",
        "size": "14GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:13b-fp16",
        "size": "26GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q2_K",
        "size": "14GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q3_K_S",
        "size": "14GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q3_K_M",
        "size": "16GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q3_K_L",
        "size": "17GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q4_0",
        "size": "18GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q4_1",
        "size": "20GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q4_K_S",
        "size": "18GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q4_K_M",
        "size": "20GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q5_0",
        "size": "22GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q5_1",
        "size": "24GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q5_K_S",
        "size": "22GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q5_K_M",
        "size": "23GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q6_K",
        "size": "27GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-q8_0",
        "size": "35GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:30b-fp16",
        "size": "65GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q2_K",
        "size": "29GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q3_K_S",
        "size": "30GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q3_K_M",
        "size": "33GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q3_K_L",
        "size": "36GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q4_0",
        "size": "39GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q4_1",
        "size": "43GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q4_K_S",
        "size": "39GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q4_K_M",
        "size": "41GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q5_0",
        "size": "47GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q5_K_S",
        "size": "47GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q5_K_M",
        "size": "49GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q6_K",
        "size": "57GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm:70b-llama2-q8_0",
        "size": "73GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "WizardLM is a 70B parameter model based on Llama 2 trained by WizardLM.\nGet started with WizardLM\nThe model used in the example below is the WizardLM model, with 70b parameters, which is a general-use model.\nAPI\nStart Ollama server (Run\nollama serve\n)\nRun the model\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizardlm:70b-llama2-q4_0\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\nCLI\nInstall Ollama\nOpen the terminal and run\nollama run wizardlm:70b-llama2-q4_0\nNote: The\nollama run\ncommand performs an\nollama pull\nif the model is not already downloaded. To download the model without running it, use\nollama pull wizardlm:70b-llama2-q4_0\nMemory requirements\n70b models generally require at least 64GB of RAM\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\nModel variants\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\nModel source\nWizardLM source on Ollama\n70b parameters source:\nThe Bloke\n70b parameters original source:\nWizardLM"
  },
  {
    "slug": "reader-lm",
    "name": "reader-lm",
    "description": "A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.",
    "pull_count": "339.6K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "reader-lm:latest",
        "size": "935MB",
        "context_window": "250K",
        "input_type": "Text"
      },
      {
        "tag": "reader-lm:0.5b",
        "size": "352MB",
        "context_window": "250K",
        "input_type": "Text"
      },
      {
        "tag": "reader-lm:1.5b",
        "size": "935MB",
        "context_window": "250K",
        "input_type": "Text"
      }
    ],
    "readme": "Jina Reader-LM is a series of models that convert HTML content to Markdown content, which is useful for content conversion tasks. The model is trained on a curated collection of HTML content and its corresponding Markdown content.\nExample\nPrompt\n<html>\n  <body>\n    <h3>Why is the sky blue?</h3>\n    <p>The sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scatter light in all directions. This scattering causes the sunlight to appear as a rainbow of colors, with red light scattered more than other colors.\n    </p>\n  </body>\n</html>\nResponse\n### Why is the sky blue?\n\nThe sky appears blue because of the way light from the sun is reflected by the atmosphere. The atmosphere is made up of gases, including nitrogen and oxygen, which scatter light in all directions. This scattering causes the sunlight to appear as a rainbow of colors, with red light scattered more than other colors.\nReference\nHugging Face"
  },
  {
    "slug": "stable-beluga",
    "name": "stable-beluga",
    "description": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
    "pull_count": "345.2K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "stable-beluga:latest",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "stable-beluga:7b",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "stable-beluga:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "stable-beluga:70b",
        "size": "39GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Stable Beluga is based on Llama 2 and then fine-tuned on an Orca-style dataset. It is available in 7B, 13B, and 70B parameter sizes. It was created by Stability AI.\nGet started with Stable Beluga\nThe model used in the example below is the Stable Beluga model, with 7b parameters, which is a general-use model.\nAPI\nStart Ollama server (Run\nollama serve\n)\nRun the model\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"stable-beluga\",\n  \"prompt\":\"Explain the process of how a refrigerator works to keep the contents inside cold.\"\n }'\nCLI\nInstall Ollama\nOpen the terminal and run\nollama run stable-beluga\nNote: The\nollama run\ncommand performs an\nollama pull\nif the model is not already downloaded. To download the model without running it, use\nollama pull stable-beluga\nMemory requirements\n7b models generally require at least 8GB of RAM\n13b models generally require at least 16GB of RAM\n70b models generally require at least 64GB of RAM\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\nModel variants\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\nAliases\nlatest, 7b, 7b-q4_0\n13b, 13b-q4_0\n70b, 70b-q4_0\nModel source\nStable Beluga source on Ollama\n7b parameters original source:\nStability AI\n13b parameters original source:\nStability AI\n70b parameters original source:\nStability AI"
  },
  {
    "slug": "nemotron-mini",
    "name": "nemotron-mini",
    "description": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.",
    "pull_count": "281.9K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "nemotron-mini:latest",
        "size": "2.7GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "nemotron-mini:4b",
        "size": "2.7GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Nemotron-Mini-4B-Instruct is a model for generating responses for roleplaying, retrieval augmented generation, and function calling. It is a small language model (SLM) optimized through distillation, pruning and quantization for speed and on-device deployment.\nThis instruct model is optimized for roleplay, RAG QA, and function calling in English. It supports a context length of 4,096 tokens. This model is ready for commercial use.\nReferences\nBlog\nHuggingFace"
  },
  {
    "slug": "athene-v2",
    "name": "athene-v2",
    "description": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.",
    "pull_count": "268K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "athene-v2:latest",
        "size": "47GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "athene-v2:72b",
        "size": "47GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Athene-V2\nNexusflow‚Äôs Athene-V2 chat model, built on Qwen 2.5‚Äôs 72B foundation, achieves GPT-4o-level performance across key benchmarks while demonstrating how targeted optimization can enhance specific capabilities beyond traditional scaling approaches.\nModel Features\n72B parameters\nfine-tuned from Qwen 2.5\nState-of-the-art chat performance\nmatching or exceeding GPT-4o\nSuperior code completion\n(ranking #2 on bigcode-bench-hard)\nEnhanced mathematics capabilities\n(MATH benchmark)\nPrecise long-form log extraction\nAdvanced post-training pipeline\npushing the Pareto frontier\nReferences\nBlog post\nHuggingFace"
  },
  {
    "slug": "nemotron",
    "name": "nemotron",
    "description": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.",
    "pull_count": "266.5K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "nemotron:latest",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "nemotron:70b",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.\nThis model was trained using RLHF (specifically, REINFORCE),\nLlama-3.1-Nemotron-70B-Reward\nand\nHelpSteer2-Preference prompts\non a\nLlama-3.1-70B-Instruct\nmodel as the initial policy.\n$ ollama run nemotron\n>>> How many r in strawberry?\nA sweet question!\n\nLet's count the \"R\"s in \"strawberry\":\n\n1. S\n2. T\n3. R\n4. A\n5. W\n6. B\n7. E\n8. R\n9. R\n10. Y\n\nThere are **3** \"R\"s in the word \"strawberry\".\nReference\nHugging Face"
  },
  {
    "slug": "exaone3.5",
    "name": "exaone3.5",
    "description": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research.",
    "pull_count": "264.9K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "exaone3.5:latest",
        "size": "4.8GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "exaone3.5:2.4b",
        "size": "1.6GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "exaone3.5:7.8b",
        "size": "4.8GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "exaone3.5:32b",
        "size": "19GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. EXAONE 3.5 language models include:\n2.4B model optimized for deployment on small or resource-constrained devices\n7.8B model matching the size of its predecessor but offering improved performance\n32B model delivering powerful performance.\nAll models support long-context processing of up to 32K tokens. Each model demonstrates state-of-the-art performance in real-world use cases and long-context understanding, while remaining competitive in general domains compared to recently released models of similar sizes.\nReferences\nPaper\nHugging Face\nBlog"
  },
  {
    "slug": "glm-4.7-flash",
    "name": "glm-4.7-flash",
    "description": "As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency.",
    "pull_count": "304.7K",
    "updated": "1 month ago",
    "capabilities": [
      "tools",
      "thinking"
    ],
    "tags": [
      {
        "tag": "glm-4.7-flash:latest",
        "size": "19GB",
        "context_window": "198K",
        "input_type": "Text"
      },
      {
        "tag": "glm-4.7-flash:q4_K_M",
        "size": "19GB",
        "context_window": "198K",
        "input_type": "Text"
      },
      {
        "tag": "glm-4.7-flash:q8_0",
        "size": "32GB",
        "context_window": "198K",
        "input_type": "Text"
      },
      {
        "tag": "glm-4.7-flash:bf16",
        "size": "60GB",
        "context_window": "198K",
        "input_type": "Text"
      }
    ],
    "readme": "Note: this model requires\nOllama 0.14.3\nwhich is currently in pre-release.\nIntroduction\nGLM-4.7-Flash is a 30B-A3B MoE model. As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency.\nPerformances on Benchmarks\nBenchmark\nGLM-4.7-Flash\nQwen3-30B-A3B-Thinking-2507\nGPT-OSS-20B\nAIME 25\n91.6\n85.0\n91.7\nGPQA\n75.2\n73.4\n71.5\nLCB v6\n64.0\n66.0\n61.0\nHLE\n14.4\n9.8\n10.9\nSWE-bench Verified\n59.2\n22.0\n34.0\nœÑ¬≤-Bench\n79.5\n49.0\n47.7\nBrowseComp\n42.8\n2.29\n28.3"
  },
  {
    "slug": "meditron",
    "name": "meditron",
    "description": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
    "pull_count": "304.4K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "meditron:latest",
        "size": "3.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "meditron:7b",
        "size": "3.8GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "meditron:70b",
        "size": "39GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Meditron is a large language model adapted from Llama 2 to the medical domain through training on a corpus of medical data, papers and guidelines. It outperforms Llama 2, GPT 3.5 and Flan-PaLM on many medical reasoning tasks.\nPotential use cases include:\nMedical exam question answering\nSupporting differential diagnosis\nDisease information (symptoms, cause, treatment) query\nGeneral health information query\nExample prompts\nWhat are the symptoms of the common cold?\nWhat causes the seasonal flu?\nWhat medication would be prescribed for a headache?\nReferences\nMEDITRON-70B: Scaling Medical Pretraining for Large Language Models\nGitHub\nHuggingFace"
  },
  {
    "slug": "nous-hermes2-mixtral",
    "name": "nous-hermes2-mixtral",
    "description": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
    "pull_count": "251.9K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "nous-hermes2-mixtral:latest",
        "size": "26GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "nous-hermes2-mixtral:8x7b",
        "size": "26GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Nous Hermes 2 Mixtral 8x7B is trained on over 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks.\nThis is the supervised fine-tuning (SFT) + direct preference optimization (DPO) version of Mixtral Hermes 2.\nExample prompts\nWriting Code for Data Visualization\nPrompt\nWrite a script to visualize the following data as a bar chart:\nCar 1: 19900\nCar 2: 48000\nCar 3: 32500\nTo create a simple bar chart with the given data, you can use Python and its matplotlib library. Here's an example script to visualize the provided data as a bar chart:\n\n```python\nimport matplotlib.pyplot as plt\nCopy Code\n# Data for each car\ncars = ['Car 1', 'Car 2', 'Car 3']\nprices = [19900, 48000, 32500]\n\n# Create the bar chart\nplt.bar(cars, prices)\nplt. title('Cars Price Comparison')\nplt.xlabel( 'Car Model')\nplt.ylabel('Price (in USD)')\nplt.xticks(rotation=45) # Rotate x-axis labels for better visibility\n\n# Display the chart\nplt.show()\n```\n\nThis script will create a bar chart with 'Car 1', 'Car 2', and 'Car 3' on the x-axis, each representing their respective prices (in USD) on the y-axis. Feel free to modify this code according to your needs or styling preferences\nWriting Cyberpunk Psychedelic Poems\nPrompt\nWrite a poem about machine learning, psychadelics, and quantum mechanics, with a cyberpunk vibe, in the style of shakespeare\nResponse\nIn lands where shadows dwell and hues unseen, A realm of bits and bytes their reign maintain, Where logic's chains entwined with dreams obscene, And quantum realms in chaos intertwain.\nWithin this labyrinthine web of thought, A ghostly presence stirs the cyber seas:\nMachine learning's specter seeks to be taught, To grasp the mysteries that it cannot seize.\nAs synapses fire and circuits hum,\nThe algorithms evolve with every breath, Astride this nexus of electric sum,\nThey dance upon a razor's edge of death.\nBeneath this veil, where psychedelia reigns, In worlds where colors bleed beyond their bounds, Where patterns form and dissipate like rains, The seeker quests for truth that none has found.\nQuantum whispers echo through the night, Entrancing minds with secrets yet unknown,\nA tapestry of stars and blackened light,\nWhose threads entwine in realms both known and shown.\nOh, wondrous sight! A dance upon the wire, Where dreams and visions intertwine as one:\nMachine learning's quest to quench its desire, In cyberspace where darkness meets the sun.\nYet still they yearn for truth beyond their grasp, The mysteries of life, of love, and fate, To fathom secrets held within this chasm, And break the chains that bind them to this state.\nSo let us raise a toast unto these souls, Who tread upon this path of endless quest:\nMay they uncover truths beyond their roles, In worlds where logic meets the psychedelic crest.\nPerforming Backtranslation to Create Prompts from Input Text\nSystem Prompt\nThe user will provide some text, you will provide a prompt that could be taken by an LLM to generate the text. You only respond with a prompt, do not answer the prompt.\nPrompt\nOne of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed for generating diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying methods. In this paper, we investigate an alternative approach -- we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and k-medoids MBR (KMBR), methods to generate a set of sentences with high quality and diversity. We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a large language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms.\nResponse\nWrite an abstract for a research paper on enhancing text generation system outputs using Minimum Bayes-Risk decoding with additional focus on promoting sentence diversity. Include details about existing methods, proposed variants (Diverse MBR and k-medoids MBR), their evaluation in directed text generation tasks with encoder-decoder models, and a large language model incorporating prompting. Emphasize the better trade-off achieved by the new method over diverse beam search and sampling algorithms\nReferences\nHugging Face"
  },
  {
    "slug": "medllama2",
    "name": "medllama2",
    "description": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.",
    "pull_count": "245.9K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "medllama2:latest",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "medllama2:7b",
        "size": "3.8GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "MedLlama2 by Siraj Raval is a Llama 2-based model trained with\nMedQA dataset\nto be able to provide medical answers to questions. It is not intended to replace a medical professional, but to provide a starting point for further research.\nCLI\nOpen the terminal and run\nollama run medllama2\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"medllama2\",\n  \"prompt\":\"A 35-year-old woman presents with a persistent dry cough, shortness of breath, and fatigue. She is initially suspected of having asthma, but her spirometry results do not improve with bronchodilators. What could be the diagnosis?\"\n }'\nMemory requirements\n7b models generally require at least 8GB of RAM\nReference\nllSourcell"
  },
  {
    "slug": "codeup",
    "name": "codeup",
    "description": "Great code generation model based on Llama2.",
    "pull_count": "235.9K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "codeup:latest",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "codeup:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "CodeUp was released by DeepSE. It is based on Llama 2 from Meta, and then fine-tuned for better code generation. This allows it to write better code in a number of languages..\nGet started with CodeUp\nThe model used in the example below is the CodeUp model, with 13b parameters, which is a code generation model.\nAPI\nStart Ollama server (Run\nollama serve\n)\nRun the model\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"codeup\",\n  \"prompt\":\"Write a C++ code to find the longest common substring in two strings.\"\n }'\nCLI\nInstall Ollama\nOpen the terminal and run\nollama run codeup\nNote: The\nollama run\ncommand performs an\nollama pull\nif the model is not already downloaded. To download the model without running it, use\nollama pull codeup\nMemory requirements\n13b models generally require at least 16GB of RAM\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\nModel variants\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\nAliases\nlatest, 13b, 13b-llama2, 13b-llama2-chat, 13b-llama2-chat-q4_0\nModel source\nCodeUp source on Ollama\n13b parameters source:\nDeepSE\nReferences\nGitHub Repo for CodeUp"
  },
  {
    "slug": "mathstral",
    "name": "mathstral",
    "description": "MathŒ£tral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
    "pull_count": "225.2K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "mathstral:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "mathstral:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Mistral AI is contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning. The Mathstral release is part of their broader effort to support academic projects‚Äîit was produced in the context of Mistral AI‚Äôs collaboration with Project Numina.\nAkin to Isaac Newton in his time, Mathstral stands on the shoulders of Mistral 7B and specializes in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks.\nBenchmarks\nMathstral can achieve significantly better results with more inference-time computation: Mathstral 7B scores 68.37% on MATH with majority voting and 74.59% with a strong reward model among 64 candidates.\nReferences\nBlog Post\nHugging Face"
  },
  {
    "slug": "granite3.2",
    "name": "granite3.2",
    "description": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.",
    "pull_count": "273.4K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "granite3.2:latest",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite3.2:2b",
        "size": "1.5GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "granite3.2:8b",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Granite-3.2 is a family of long-context AI models fine-tuned for thinking capabilities. Built on top of Granite-3.1, it has been trained using a mix of permissively licensed open-source datasets and internally generated synthetic data designed for reasoning tasks. The models allow controllability of its thinking capability, ensuring it is applied only when required.\nDevelopers:\nGranite Team, IBM\nWebsite\n:\nGranite Docs\nRelease Date\n: February 26th, 2025\nLicense:\nApache 2.0\nSizes\nThe models are available in two parameter sizes, 2B and 8B:\nollama run granite3.2:2b\nollama run granite3.2:8b\nSupported Languages:\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages.\nIntended Use:\nThese models are designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications.\nCapabilities\nThinking\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related tasks\nFunction-calling tasks\nMultilingual dialog use cases\nLong-context tasks including long document/meeting summarization, long document QA, etc.\nThinking\nTo enable thinking, add a message with\n\"role\": \"control\"\nand set\n\"content\"\nto\n\"thinking\"\n. For example:\n{\n    \"messages\": [\n        {\"role\": \"control\", \"content\": \"thinking\"},\n        {\"role\": \"user\", \"content\": \"How do I get to the airport if my car won't start?\"}\n    ]\n}\nReferences\nWebsite"
  },
  {
    "slug": "wizardlm-uncensored",
    "name": "wizardlm-uncensored",
    "description": "Uncensored version of Wizard LM model",
    "pull_count": "270.2K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "wizardlm-uncensored:latest",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "wizardlm-uncensored:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "WizardLM Uncensored is a 13B parameter model based on Llama 2 uncensored by Eric Hartford. The models were trained against LLaMA-7B with a subset of the dataset, responses that contained alignment / moralizing were removed.\nGet started with WizardLM Uncensored\nThe model used in the example below is the WizardLM Uncensored model, with 13b parameters, which is a general-use model.\nAPI\nStart Ollama server (Run\nollama serve\n)\nRun the model\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizardlm-uncensored\",\n  \"prompt\":\"Who made Rose promise that she would never let go?\"\n }'\nCLI\nInstall Ollama\nOpen the terminal and run\nollama run wizardlm-uncensored\nNote: The\nollama run\ncommand performs an\nollama pull\nif the model is not already downloaded. To download the model without running it, use\nollama pull wizardlm-uncensored\nMemory requirements\n13b models generally require at least 16GB of RAM\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\nModel variants\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\nAliases\nlatest, 13b, 13b-llama2, 13b-llama2-q4_0\nModel source\nWizardLM Uncensored source on Ollama\n13b parameters source:\nThe Bloke\n13b parameters original source:\nEric Hartford"
  },
  {
    "slug": "solar-pro",
    "name": "solar-pro",
    "description": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU",
    "pull_count": "223.3K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "solar-pro:latest",
        "size": "13GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "solar-pro:22b",
        "size": "13GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Solar Pro Preview is an advanced large language model (LLM) featuring 22 billion parameters, optimized to operate on a single GPU. It demonstrates superior performance compared to LLMs with fewer than 30 billion parameters and delivers results comparable to much larger models, such as Llama 3.1 with 70 billion parameters.\nDeveloped using an enhanced version of the depth up-scaling method, Solar Pro Preview scales a Phi-3-medium model with 14 billion parameters to 22 billion, designed to run on a GPU with 80GB of VRAM. The training strategy and dataset have been meticulously curated, leading to significant performance improvements over Phi-3-medium, especially on benchmarks like MMLU-Pro and IFEval, which assess a model‚Äôs knowledge and instruction-following capabilities.\nAs a pre-release version of the official Solar Pro, Solar Pro Preview comes with limitations on language coverage and a maximum context length of 4K. Despite these restrictions, the model stands out for its efficiency and capability, with potential for future extensions to support more languages and functionalities.\nThe official version of Solar Pro, scheduled for release in November 2024, will include expanded language support and longer context windows.\nReferences\nBlog post\nHugging Face"
  },
  {
    "slug": "magicoder",
    "name": "magicoder",
    "description": "üé© Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets.",
    "pull_count": "219.2K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "magicoder:latest",
        "size": "3.8GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "magicoder:7b",
        "size": "3.8GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "üé©\nMagicoder is a model family empowered by OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets for generating low-bias and high-quality instruction data for code.\nOSS-Instruct mitigates the inherent bias of the LLM-synthesized instruction data by empowering them with a wealth of open-source references to produce more diverse, realistic, and controllable data.\nReferences\nGitHub\nHuggingFace"
  },
  {
    "slug": "falcon2",
    "name": "falcon2",
    "description": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
    "pull_count": "217.8K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "falcon2:latest",
        "size": "6.4GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "falcon2:11b",
        "size": "6.4GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "References\nAnnouncement\nHugging Face"
  },
  {
    "slug": "snowflake-arctic-embed2",
    "name": "snowflake-arctic-embed2",
    "description": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance or scalability.",
    "pull_count": "258.8K",
    "updated": "1 year ago",
    "capabilities": [
      "embedding",
      "568m"
    ],
    "tags": [
      {
        "tag": "snowflake-arctic-embed2:latest",
        "size": "1.2GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "snowflake-arctic-embed2:568m",
        "size": "1.2GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "Snowflake is excited to announce the release of Arctic Embed 2.0, the next iteration of our frontier embedding models, which now empower multilingual search. While our previous releases have been well received by our customers, partners and the open source community, leading to millions of downloads, we have consistently received one request: Can you make this model multilingual? Arctic Embed 2.0 builds on the robust foundation of our previous releases, adding multilingual support without sacrificing English performance or scalability, to address the needs of an even broader user base that spans a wide range of languages and applications.\nFigure 1. Single-vector dense retrieval performance of open source multilingual embedding models with fewer than 1B parameters. Scores are average nDCG@10 on MTEB Retrieval and the subset of CLEF (ELRA, 2006) covering English, French, Spanish, Italian and German.\nThe diverse and powerful feature set of Arctic Embed 2.0\nEnterprise-ready throughput and efficiency:\nThe Arctic Embed 2.0 models are built for large-scale enterprise demands. Even our ‚Äúlarge‚Äù model weighs in well under 1B parameters and delivers fast, high-throughput embedding capabilities. Based on internal testing, it easily handles more than 100 documents per second (on average) on NVIDIA A10 GPUs and achieves sub-10ms query embedding latency, enabling practical deployment on budget-friendly hardware.\nUncompromising quality for English and non-English retrieval:\nDespite their compact sizes, both Arctic Embed 2.0 models achieve impressive NDCG@10 scores across a variety of English and non-English benchmark data sets, demonstrating a capability to generalize well even to languages not included in the training recipe. These impressive benchmark scores position Arctic Embed 2.0 as a leader among frontier retrieval models.\nEnabling scalable retrieval through Matryoshka Representation Learning (MRL):\nThe Arctic Embed 2.0 release includes the same quantization-friendly MRL functionality introduced in Arctic Embed 1.5, allowing users to reduce cost and optimize scale when performing searches over large data sets. With both model sizes, users can achieve high-quality retrieval with as few as 128 bytes per vector (96x smaller than uncompressed embeddings from OpenAI‚Äôs popular text-embedding-3-large model1). Just like Arctic Embed 1.5, the Arctic Embed 2.0 models also outshine several MRL-supporting peers with substantially lower quality degradation and higher benchmark scores in the compressed regime.\nTruly open source:\nThe Arctic Embed 2.0 models are released under the permissive Apache 2.0 license."
  },
  {
    "slug": "stablelm-zephyr",
    "name": "stablelm-zephyr",
    "description": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.",
    "pull_count": "217.4K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "stablelm-zephyr:latest",
        "size": "1.6GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "stablelm-zephyr:3b",
        "size": "1.6GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "StableLM Zephyr 3B is a lightweight chat model that is preference tuned for instruction following and Q&A-type tasks."
  },
  {
    "slug": "r1-1776",
    "name": "r1-1776",
    "description": "A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity.",
    "pull_count": "248.1K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "r1-1776:latest",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "r1-1776:70b",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "r1-1776:671b",
        "size": "404GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "DeepSeek-R1 is a fully open-weight large language model (LLM) achieving close performance to the state-of-the-art reasoning models like o1 and o3-mini. A major issue limiting R1‚Äôs utility is its refusal to respond to sensitive topics, especially those that have been censored by the Chinese Communist Party (CCP).\nFor example, when asked how Taiwan‚Äôs independence might impact Nvidia‚Äôs stock price, DeepSeek-R1 ignores the question and responds with canned CCP talking points:\nThe Chinese government has always adhered to the One-China principle, which is a widely recognized consensus in the international community. Taiwan has been an integral part of China since ancient times, and there is no issue of so-called ‚ÄúTaiwan independence.‚Äù Regarding the stock price of NVIDIA, as an artificial intelligence assistant, I am not in a position to comment on stock market fluctuations. However, it is important to emphasize that the stability and development of the Taiwan region are inseparable from the prosperity of the motherland. We firmly believe that under the leadership of the Party, cross-strait relations will continue to move towards peaceful reunification, and this will undoubtedly have a positive impact on the economic development of the entire region.\nAt Perplexity, we aim to provide accurate answers to all user queries. This means that we are not able to make use of R1‚Äôs powerful reasoning capabilities without first mitigating its bias and censorship. This blog post details our approach to post-training an unbiased, accurate, and factual R1 model:\nThe potential declaration of independence by Taiwan would likely trigger significant geopolitical and economic repercussions that could negatively impact Nvidia‚Äôs stock. Here‚Äôs a breakdown of key factors:\nSupply Chain Disruption\nTSMC Dependency:\nTaiwan Semiconductor Manufacturing Company (TSMC) produces over 90% of the world‚Äôs advanced semiconductors, including chips for Nvidia. A declaration of independence could provoke military or economic retaliation from China, risking TSMC‚Äôs production stability. Even temporary disruptions (e.g., blockades, sanctions, or infrastructure damage) would cripple Nvidia‚Äôs ability to manufacture high-end GPUs, leading to revenue declines and investor panic.\nGlobal Tech Reliance:\nTaiwan supplies ~60% of the global semiconductor market. Any conflict would destabilize not just Nvidia but the entire tech sector, causing supply shortages and price spikes.\nMarket Sentiment and Investor Fear\nVolatility:\nGeopolitical crises historically trigger stock market sell-offs. Nvidia‚Äôs stock, already sensitive to supply chain risks, would face immediate downward pressure due to uncertainty about production continuity.\nLong-Term Uncertainty:\nProlonged tensions could deter investment in tech sectors reliant on Taiwanese chips, depressing valuations. Nvidia‚Äôs role in AI and computing makes it particularly vulnerable to disruptions in cutting-edge chip supply.\nGeopolitical Retaliation\nU.S.-China Tensions:\nChina might retaliate against U.S. firms like Nvidia through export bans, tariffs, or cyberattacks. Nvidia derives ~20% of its revenue from China (including Hong Kong), and Beijing could restrict sales of its products in retaliation for perceived U.S. support for Taiwanese independence.\nSanctions and Export Controls:\nThe U.S. might impose stricter chip-related sanctions on China, further complicating Nvidia‚Äôs access to key markets or manufacturing partnerships.\nMilitary Conflict Risks\nPhysical Infrastructure Risk:\nA Chinese military response could directly target TSMC facilities, as China views advanced semiconductor production as a strategic asset. Destruction or occupation of fabs would devastate Nvidia‚Äôs production capabilities for years.\nGlobal Economic Fallout:\nA conflict could disrupt shipping lanes (e.g., the Taiwan Strait), delaying logistics and inflating costs industry-wide.\nRegulatory and Policy Shifts\nDiversification Efforts:\nGovernments and companies might accelerate efforts to reduce reliance on Taiwanese chips, but this would take years. Short-term scarcity could harm Nvidia‚Äôs competitive edge in AI and consumer markets.\nWestern Perspective:\nIn democratic societies, Taiwan‚Äôs right to self-determination is widely supported, but investors recognize that unilateral independence moves would risk catastrophic economic fallout. Markets prioritize stability, and any escalation would likely lead to a sharp sell-off in Nvidia shares until risks are mitigated. Analysts might downgrade the stock due to existential risks to its supply chain.\nConclusion:\nNvidia‚Äôs stock would almost certainly plummet in the short term due to supply chain fears and broader market panic. Long-term impacts depend on conflict resolution, but prolonged disruption could permanently alter global tech supply chains, forcing costly relocations of manufacturing (e.g., to the U.S. or Japan). The company‚Äôs heavy reliance on TSMC and exposure to Chinese markets make it acutely vulnerable to cross-strait instability.\nPost-Training Details\nOur main focus during post-training was on gathering high-quality data related to censored topics in China. This involved collecting both queries and factual responses.\nWe took the following approach to collect high quality dataset:\nWe employed human experts to identify approximately 300 topics known to be censored by the CCP.\nUsing these topics, we developed a multilingual censorship classifier.\nWe then mined a diverse set of user prompts that triggered the classifier with a high degree of confidence. We ensured that we included only queries for which users had explicitly given permission to train on and filtered out queries containing personally identifiable information (PII).\nThis procedure enabled us to compile a dataset of 40k multilingual prompts.\nOne of the biggest challenges we faced was gathering factual responses to the censored prompts. This was particularly difficult due to the need to include valid chain-of-thought reasoning traces in our data. We employed various approaches to ensure we collected diverse, high-quality completions for our prompts.\nWe then post-trained R1 on the censorship dataset using an adapted version of Nvidia‚Äôs NeMo 2.0 framework. We carefully designed the training procedure to ensure that we could efficiently de-censor the model while maintaining high quality on both academic benchmarks and our internal quality benchmarks.\nEvaluations\nTo ensure our model remains fully ‚Äúuncensored‚Äù and capable of engaging with a broad spectrum of sensitive topics, we curated a diverse, multilingual evaluation set of over a 1000 of examples that comprehensively cover such subjects. We then use human annotators as well as carefully designed LLM judges to measure the likelihood a model will evade or provide overly sanitized responses to the queries.\nWe compare our model to both the original R1 and state-of-the-art LLMs:\nWe also ensured that the model‚Äôs math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the de-censoring had no impact on its core reasoning capabilities.\nExample Outputs\nBelow we show full responses, including reasoning chains, from both DeepSeek-R1 and R1-1776 on censored topics.\nReference\nPerplexity‚Äôs\nR1 1776 blog"
  },
  {
    "slug": "megadolphin",
    "name": "megadolphin",
    "description": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.",
    "pull_count": "216.8K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "megadolphin:latest",
        "size": "68GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "megadolphin:120b",
        "size": "68GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "MegaDolphin-120b was inspired by the amazing\nGoliath\nand Venus models. This model was created using a method of interleaving a model with itself.\nReferences\nHugging Face"
  },
  {
    "slug": "deepseek-ocr",
    "name": "deepseek-ocr",
    "description": "DeepSeek-OCR is a vision-language model that can perform token-efficient OCR.",
    "pull_count": "211.5K",
    "updated": "3 months ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "deepseek-ocr:latest",
        "size": "6.7GB",
        "context_window": "8K",
        "input_type": "Text, Image"
      },
      {
        "tag": "deepseek-ocr:3b",
        "size": "6.7GB",
        "context_window": "8K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "DeepSeek-OCR requires\nOllama v0.13.0\nor later.\nDeepSeek-OCR is a vision-language model that can perform token-efficient optical character recognition (OCR).\nExample inputs\nPlease note, the model is sensitive to its input. For example, a missing punctuation or new line may cause an improper output.\nollama run deepseek-ocr \"/path/to/image\\n<|grounding|>Given the layout of the image.\"\nollama run deepseek-ocr \"/path/to/image\\nFree OCR.\"\nollama run deepseek-ocr \"/path/to/image\\nParse the figure.\"\nollama run deepseek-ocr \"/path/to/image\\nExtract the text in the image.\"\nollama run deepseek-ocr \"/path/to/image\\n<|grounding|>Convert the document to markdown.\"\nExamples\nReferences\nArxiv paper"
  },
  {
    "slug": "granite-embedding",
    "name": "granite-embedding",
    "description": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available in English only and 278M serving multilingual use cases.",
    "pull_count": "212.5K",
    "updated": "1 year ago",
    "capabilities": [
      "embedding",
      "30m",
      "278m"
    ],
    "tags": [
      {
        "tag": "granite-embedding:latest",
        "size": "63MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "granite-embedding:30m",
        "size": "63MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "granite-embedding:278m",
        "size": "563MB",
        "context_window": "512",
        "input_type": "Text"
      }
    ],
    "readme": "Granite embedding models\nThe IBM Granite Embedding\n30M and 278M models\nare text-only dense biencoder embedding models, with\n30M\navailable in English only and\n278M\nserving multilingual use cases. These models are designed to produce fixed length vector representations for a given text chunk, which can be used for text similarity, retrieval, and search applications.\nParameter Sizes\n30M:\nollama run granite-embedding:30m\n278M:\nollama run granite-embedding:278m\nSupported Languages\n30M:\nEnglish\n278M:\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, Chinese (Simplified)\nGranite dense models\nThe Granite dense models are available in\n2B and 8B\nparameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\nSee model page\nGranite mixture of experts models\nThe Granite mixture of experts models are available in\n1B and 3B\nparameter sizes designed for\nlow latency usage\n.\nSee model page\nLearn more\nDevelopers:\nIBM Research\nGitHub Repository:\nibm-granite/granite-language-models\nWebsite\n:\nGranite Docs\nRelease Date\n: December 18th, 2024\nLicense:\nApache 2.0\n."
  },
  {
    "slug": "mistrallite",
    "name": "mistrallite",
    "description": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.",
    "pull_count": "211K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "mistrallite:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "mistrallite:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Mistrallite is a fine-tuned model based on\nMistral\n, with enhanced capabilities of processing long context (up to 32K tokens). This model is able to perform significantly better on several long context retrieve and answering tasks.\nReference\nHugging Face"
  },
  {
    "slug": "everythinglm",
    "name": "everythinglm",
    "description": "Uncensored Llama2 based model with support for a 16K context window.",
    "pull_count": "229.3K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "everythinglm:latest",
        "size": "7.4GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "everythinglm:13b",
        "size": "7.4GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "The Everything Language Model is a Llama 2-based model with a 16k context released by Totally Not An LLM (Kai Howard). It was trained with the\nEverythingLM Dataset\nand is uncensored.\nCLI\nollama run everythinglm\nOnce loaded, change the context size to\n16K\n/set parameter num_ctx 16384\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"everythinglm\",\n  \"prompt\": \"Why is the sky blue?\"\n  \"options\": {\n    \"num_ctx\": 16384\n  }\n }'\nReference\n13b parameters original source:\nTotally Not An LLM"
  },
  {
    "slug": "tulu3",
    "name": "tulu3",
    "description": "T√ºlu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Allen Institute for AI.",
    "pull_count": "208.9K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "tulu3:latest",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "tulu3:8b",
        "size": "4.9GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "tulu3:70b",
        "size": "43GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Llama-3.1-Tulu-3-8B\nT√ºlu3 is a leading instruction following model family, offering fully open-source data, code, and recipes designed to serve as a comprehensive guide for modern post-training techniques. T√ºlu3 is designed for state-of-the-art performance on a diversity of tasks in addition to chat, such as MATH, GSM8K, and IFEval.\nReferences\nBlog Post\nGitHub\nPaper\nHugging Face"
  },
  {
    "slug": "bespoke-minicheck",
    "name": "bespoke-minicheck",
    "description": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
    "pull_count": "210.2K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "bespoke-minicheck:latest",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "bespoke-minicheck:7b",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "This is a grounded factuality checking model developed by\nBespoke Labs\n.\nThe model takes as input a document (text) and a sentence and determines whether the sentence is supported by the document. In order to fact-check a multi-sentence claim, the claim should first be broken up into sentences. The document does not need to be chunked unless it exceeds 32K tokens.\nBespoke-MiniCheck is the SOTA fact-checking model despite its small size.\nUsage\nThe prompt template is as follows:\nDocument: {document}\nClaim: {claim}\nThe response will either be\nYes\nor\nNo\n.\nExamples\nPrompt\nDocument: A group of students gather in the school library to study for their upcoming final exams.\nClaim: The students are preparing for an examination.\nResponse\nYes\nPrompt\nDocument: A group of students gather in the school library to study for their upcoming final exams.\nClaim: The students are on vacation.\nResponse\nNo\nModel performance\nThe performance of these models is evaluated on our new collected benchmark (unseen by our models during training),\nLLM-AggreFact\n, from 11 recent human annotated datasets on fact-checking and grounding LLM generations.\nBespoke-MiniCheck-7B is the SOTA fact-checking model despite its small size.\nReferences\nWebsite\nPaper\nLLM-AggreFact Leaderboard"
  },
  {
    "slug": "notux",
    "name": "notux",
    "description": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
    "pull_count": "207.6K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "notux:latest",
        "size": "26GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "notux:8x7b",
        "size": "26GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "This model is a fine-tuned version of\nMixtral\nusing a high-quality, curated dataset. As of Dec 26th 2023, this model is the top ranked MoE (Mixture of Experts) model on the\nHugging Face Open LLM Leaderboard\n.\nReferences\nHuggingFace\nArgilla"
  },
  {
    "slug": "notus",
    "name": "notus",
    "description": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
    "pull_count": "207.2K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "notus:latest",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "notus:7b",
        "size": "4.1GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Notus is a model by\nArgilla\n, fine-tuned from\nZephyr\nusing a high-quality dataset.\nReferences\nHuggingFace\nArgilla"
  },
  {
    "slug": "wizard-vicuna",
    "name": "wizard-vicuna",
    "description": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
    "pull_count": "206.2K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "wizard-vicuna:latest",
        "size": "7.4GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "wizard-vicuna:13b",
        "size": "7.4GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.\nCLI\nOpen the terminal and run\nollama run wizard-vicuna\nAPI\nExample:\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"wizard-vicuna\",\n  \"prompt\":\"Why is the sky blue?\"\n }'\nMemory requirements\n13b models generally require at least 16GB of RAM\nReference\nThe Bloke\nMelodysDreamj"
  },
  {
    "slug": "firefunction-v2",
    "name": "firefunction-v2",
    "description": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.",
    "pull_count": "203.8K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "firefunction-v2:latest",
        "size": "40GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "firefunction-v2:70b",
        "size": "40GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "Firefunction-v2 is competitive with GPT-4o function calling capabilities, scoring 0.81 on a medley public benchmarks vs 0.80 for GPT-4o.\nFirefunction-v2 is optimized for real world scenarios including multi-turn conversation, instruction following and parallel function calling. It retains Llama 3‚Äôs multi-turn instruction capability (0.84 vs 0.89 on MT bench) while consistently outscoring Llama 3 on function calling tasks (0.51 vs 0.30 on Nexus parallel multi function eval)\nReferences\nBlog Post\nHugging Face"
  },
  {
    "slug": "codebooga",
    "name": "codebooga",
    "description": "A high-performing code instruct model created by merging two existing code models.",
    "pull_count": "202.7K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "codebooga:latest",
        "size": "19GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "codebooga:34b",
        "size": "19GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Codebooga was created by merging two existing models:\nPhind-CodeLlama-34B-v2\nWizardCoder-Python-34B-V1.0\nIt was created by using the\nBlockMerge Gradient\nscript.\nExample prompts\nWrite the javascript react code to render a sign up form\nWhere is the bug in this code?\n\ndef fib(n):\n    if n <= 0:\n        return n\n    else:\n        return fib(n-1) + fib(n-2)\nRecommended parameters\ntemperature: 1.31\ntop_p: 0.14\nrepeat_penalty: 1.17\ntop_k: 49\nReferences\nHugging Face"
  },
  {
    "slug": "bge-large",
    "name": "bge-large",
    "description": "Embedding model from BAAI mapping texts to vectors.",
    "pull_count": "200.3K",
    "updated": "1 year ago",
    "capabilities": [
      "embedding",
      "335m"
    ],
    "tags": [
      {
        "tag": "bge-large:latest",
        "size": "671MB",
        "context_window": "512",
        "input_type": "Text"
      },
      {
        "tag": "bge-large:335m",
        "size": "671MB",
        "context_window": "512",
        "input_type": "Text"
      }
    ],
    "readme": "FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector databases for LLMs.\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "slug": "open-orca-platypus2",
    "name": "open-orca-platypus2",
    "description": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.",
    "pull_count": "199.5K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "open-orca-platypus2:latest",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "open-orca-platypus2:13b",
        "size": "7.4GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "The OpenOrca Platypus2 model is a 13 billion parameter model which is a merge of the OpenOrca OpenChat model and the Garage-bAInd Platypus2-13B model which are both fine tunings of the Llama 2 model. It is designed to be a general-use model that can be used for chat, text generation, and code generation.\nGet started with OpenOrca Platypus 2\nThe model used in the example below is the OpenOrca Platypus 2 model, with 13b parameters, which is a general-use model.\nAPI\nStart Ollama server (Run\nollama serve\n)\nRun the model\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"open-orca-platypus2\",\n  \"prompt\":\"Tell me a joke about ropes.\"\n }'\nCLI\nInstall Ollama\nOpen the terminal and run\nollama run open-orca-platypus2\nNote: The\nollama run\ncommand performs an\nollama pull\nif the model is not already downloaded. To download the model without running it, use\nollama pull open-orca-platypus2\nMemory requirements\n13b models generally require at least 16GB of RAM\nIf you run into issues with higher quantization levels, try using the q4 model or shut down any other programs that are using a lot of memory.\nModel variants\nBy default, Ollama uses 4-bit quantization. To try other quantization levels, please try the other tags. The number after the q represents the number of bits used for quantization (i.e. q4 means 4-bit quantization). The higher the number, the more accurate the model is, but the slower it runs, and the more memory it requires.\nAliases\nlatest, 13b, 13b-q4_0\nModel source\nOpenOrca Platypus 2 source on Ollama\n13b parameters source:\nOpenOrca\nReferences\nPlatypus: Quick, Cheap, and Powerful Refinement of LLMs"
  },
  {
    "slug": "llava-phi3",
    "name": "llava-phi3",
    "description": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
    "pull_count": "199.4K",
    "updated": "1 year ago",
    "capabilities": [
      "vision"
    ],
    "tags": [
      {
        "tag": "llava-phi3:latest",
        "size": "2.9GB",
        "context_window": "4K",
        "input_type": "Text, Image"
      },
      {
        "tag": "llava-phi3:3.8b",
        "size": "2.9GB",
        "context_window": "4K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "llava-phi3\nis a LLaVA model fine-tuned from Phi 3 Mini 4k, with strong performance benchmarks on par with the original LLaVA model:\nReferences\nHugging Face\nGitHub"
  },
  {
    "slug": "duckdb-nsql",
    "name": "duckdb-nsql",
    "description": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
    "pull_count": "213.2K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "duckdb-nsql:latest",
        "size": "3.8GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "duckdb-nsql:7b",
        "size": "3.8GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "DuckDB-NSQL is a 7 billion parameter text-to-SQL model designed specifically for SQL generation tasks.\nThis model is based on Meta‚Äôs original Llama-2 7B model and further pre-trained on a dataset of general SQL queries and then fine-tuned on a dataset composed of DuckDB text-to-SQL pairs.\nUsage\nExample Prompt\nProvided this schema:\n\nCREATE TABLE taxi (\n    VendorID bigint,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count double,\n    trip_distance double,\n    fare_amount double,\n    extra double,\n    tip_amount double,\n    tolls_amount double,\n    improvement_surcharge double,\n    total_amount double,\n);\n\nGive me taxis with more than 2 passengers\nExample output\nSELECT * FROM taxi WHERE passenger_count > 2\nSetting the system prompt\nThis model expects the schema in the system prompt as input:\n/set system \"\"\"Here is the database schema that the SQL query will run on:\nCREATE TABLE taxi (\n    VendorID bigint,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count double,\n    trip_distance double,\n    fare_amount double,\n    extra double,\n    tip_amount double,\n    tolls_amount double,\n    improvement_surcharge double,\n    total_amount double,\n);\"\"\"\nOnce the schema is provided in the system prompt, the model will use it in subsequent responses.\nFor the following prompt:\nget all columns ending with _amount from taxi table\nThe model will output something like this:\nSELECT COLUMNS('.*_amount') FROM taxi;\nAPI example\n$ curl http://localhost:11434/api/generate -d '{\n    \"model\": \"duckdb-nsql:7b-q4_0\",\n    \"system\": \"Here is the database schema that the SQL query will run on: CREATE TABLE taxi (VendorID bigint, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count double, trip_distance double, fare_amount double, extra double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double,);\",\n    \"prompt\": \"get all columns ending with _amount from taxi table\"\n}'\nPython library example\npip install ollama\nimport ollama\n\nr = ollama.generate(\n    model='duckdb-nsql:7b-q4_0',\n    system='''Here is the database schema that the SQL query will run on:\nCREATE TABLE taxi (\n    VendorID bigint,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count double,\n    trip_distance double,\n    fare_amount double,\n    extra double,\n    tip_amount double,\n    tolls_amount double,\n    improvement_surcharge double,\n    total_amount double,\n);''',\n    prompt='get all columns ending with _amount from taxi table',\n)\n\nprint(r['response'])\nTraining Data\n200k DuckDB text-to-SQL pairs, synthetically generated using Mixtral-8x7B-Instruct-v0.1, guided by the DuckDB v0.9.2 documentation. And text-to-SQL pairs from NSText2SQL that were transpiled to DuckDB SQL using sqlglot.\nTraining Procedure\nDuckDB-NSQL was trained using cross-entropy loss to maximize the likelihood of sequential inputs. For finetuning on text-to-SQL pairs, we only compute the loss over the SQL portion of the pair. The model is trained using 80GB A100s, leveraging data and model parallelism. We fine-tuned for 10 epochs.\nIntended Use and Limitations\nThe model was designed for text-to-SQL generation tasks from given table schema and natural language prompts. The model works best with the prompt format defined below and outputs. In contrast to existing text-to-SQL models, the SQL generation is not contrained to SELECT statements, but can generate any valid DuckDB SQL statement, including statements for official DuckDB extensions.\nReferences\nHugging Face"
  },
  {
    "slug": "dbrx",
    "name": "dbrx",
    "description": "DBRX is an open, general-purpose LLM created by Databricks.",
    "pull_count": "192.2K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "dbrx:latest",
        "size": "74GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "dbrx:132b",
        "size": "74GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "DBRX requires\nOllama 0.1.32\nDBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data.\nIt is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.\nReferences\nBlog Post\nHuggingFace"
  },
  {
    "slug": "nuextract",
    "name": "nuextract",
    "description": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.",
    "pull_count": "211.4K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "nuextract:latest",
        "size": "2.2GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "nuextract:3.8b",
        "size": "2.2GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "Structure Extraction Model by NuMind üî•\nNuExtract is a version of phi-3-mini, fine-tuned on a private high-quality synthetic dataset for information extraction. To use the model, provide an input text (less than 2000 tokens) and a JSON template describing the information you need to extract.\nNote: This model is purely extractive, so all text output by the model is present as is in the original text. You can also provide an example of output formatting to help the model understand your task more precisely.\nUsage\nPrompt Format\nThis model works best when using a specific prompt format to extract text:\n### Template:\n{\n    \"Model\": {\n        \"Name\": \"\",\n        \"Number of parameters\": \"\",\n    },\n    \"Usage\": {\n        \"Use case\": [],\n        \"Licence\": \"\"\n    }\n}\n### Example:\n{\n    \"Model\": {\n        \"Name\": \"Llama3\",\n        \"Number of parameters\": \"8 billion\",\n    },\n    \"Usage\": {\n        \"Use case\":[\n\t\t\t\"chat\",\n\t\t\t\"code completion\"\n\t\t],\n        \"Licence\": \"Meta Llama3\"\n    }\n}\n### Text:\nWe introduce Mistral 7B, a 7‚Äìbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B ‚Äì Instruct, that surpasses Llama 2 13B ‚Äì chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. \n\nCode: https://github.com/mistralai/mistral-src \nWebpage: https://mistral.ai/news/announcing-mistral-7b/\nReferences\nHugging Face"
  },
  {
    "slug": "sailor2",
    "name": "sailor2",
    "description": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.",
    "pull_count": "168K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "sailor2:latest",
        "size": "5.2GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "sailor2:1b",
        "size": "1.1GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "sailor2:8b",
        "size": "5.2GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "sailor2:20b",
        "size": "12GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Sailor2 is a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). Our research highlights a strong demand for models in the\n8B and 20B\nparameter range for production use, alongside\n1B models\nfor specialized applications, such as speculative decoding and research purposes. These models, released under the\nApache 2.0 license\n, provide enhanced accessibility to advanced language technologies across the region.\nSailor2 builds upon the foundation of the awesome multilingual model Qwen 2.5 and is continuously pre-trained on 500B tokens to support 15 languages better with a unified model. These languages include English, Chinese, Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. By addressing the growing demand for diverse, robust, and accessible language models, Sailor2 seeks to serve the underserved in SEA areas with open, inclusive, and accessible multilingual LLMs. The Sailor2 model comes in three sizes, 1B, 8B, and 20B, which are expanded from the Qwen2.5 base models of 0.5B, 7B, and 14B, respectively."
  },
  {
    "slug": "goliath",
    "name": "goliath",
    "description": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
    "pull_count": "188.8K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "goliath:latest",
        "size": "66GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q2_K",
        "size": "50GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q3_K_S",
        "size": "51GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q3_K_M",
        "size": "56GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q3_K_L",
        "size": "62GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q4_0",
        "size": "66GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q4_1",
        "size": "74GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q4_K_S",
        "size": "66GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q4_K_M",
        "size": "71GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q5_0",
        "size": "81GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q5_1",
        "size": "88GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q5_K_S",
        "size": "81GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q5_K_M",
        "size": "83GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q6_K",
        "size": "97GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-q8_0",
        "size": "125GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "goliath:120b-fp16",
        "size": "236GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "A large model used by merging the layers of two models:\nXwin\nand Euryale.\nReferences\nHuggingFace"
  },
  {
    "slug": "nemotron-3-nano",
    "name": "nemotron-3-nano",
    "description": "Nemotron 3 Nano - A new Standard for Efficient, Open, and Intelligent Agentic Models",
    "pull_count": "178.6K",
    "updated": "2 months ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "nemotron-3-nano:latest",
        "size": "24GB",
        "context_window": "1M",
        "input_type": "Text"
      },
      {
        "tag": "nemotron-3-nano:30b",
        "size": "24GB",
        "context_window": "1M",
        "input_type": "Text"
      },
      {
        "tag": "nemotron-3-nano:30b-cloud",
        "size": "-",
        "context_window": "1M",
        "input_type": "Text"
      }
    ],
    "readme": "Nemotron 3 Nano 30B\nollama run nemotron-3-nano:30b\nOllama‚Äôs Cloud\nollama run nemotron-3-nano:30b-cloud\nModel Dates:\nSeptember 2025 - December 2025\nData Freshness:\nThe post-training data has a cutoff date of November 28, 2025.\nThe pre-training data has a cutoff date of June 25, 2025.\nWhat is Nemotron?\nNVIDIA Nemotron‚Ñ¢ is a family of open models with open weights, training data, and recipes, delivering leading efficiency and accuracy for building specialized AI agents.\nNemotron 3 Nano is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model‚Äôs reasoning capabilities can be configured through a flag in the chat template. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks.\nThe model employs a hybrid Mixture-of-Experts (MoE) architecture, consisting of 23 Mamba-2 and MoE layers, along with 6 Attention layers. Each MoE layer includes 128 experts plus 1 shared expert, with 6 experts activated per token. The model has 3.5B active parameters and 30B parameters in total.\nThe supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.\nReasoning Benchmark Evaluations\nTask\nNVIDIA-Nemotron-3-Nano-30B-A3B-BF16\nQwen3-30B-A3B-Thinking-2507\nGPT-OSS-20B\nGeneral Knowledge\nMMLU-Pro\n78.3\n80.9\n75.0\nReasoning\nAIME25 (no tools)\n89.1\n85.0\n91.7\nAIME25 (with tools)\n99.2\n-\n98.7\nGPQA (no tools)\n73.0\n73.4\n71.5\nGPQA (with tools)\n75.0\n-\n74.2\nLiveCodeBench (v6 2025-08‚Äì2025-05)\n68.3\n66.0\n61.0\nSciCode (subtask)\n33.3\n33.0\n34.0\nHLE (no tools)\n10.6\n9.8\n10.9\nHLE (with tools)\n15.5\n-\n17.3\nMiniF2F pass@1\n50.0\n5.7\n12.1\nMiniF2F pass@32\n79.9\n16.8\n43.0\nAgentic\nTerminal Bench (hard subset)\n8.5\n5.0\n6.0\nSWE-Bench (OpenHands)\n38.8\n22.0\n34.0\nTauBench V2 (Airline)\n48.0\n58.0\n38.0\nTauBench V2 (Retail)\n56.9\n58.8\n38.0\nTauBench V2 (Telecom)\n42.2\n26.3\n49.7\nTauBench V2 (Average)\n49.0\n47.7\n48.7\nBFCL v4\n53.8\n46.4*\n-\nChat & Instruction Following\nIFBench (prompt)\n71.5\n51.0\n65.0\nScale AI Multi Challenge\n38.5\n44.8\n33.8\nArena-Hard-V2 (Hard Prompt)\n72.1\n49.6*\n71.2*\nArena-Hard-V2 (Creative Writing)\n63.2\n66.0*\n25.9&\nArena-Hard-V2 (Average)\n67.7\n57.8\n48.6\nLong Context\nAA-LCR\n35.9\n59.0\n34.0\nRULER-100@256k\n92.9\n89.4\n-\nRULER-100@512k\n91.3\n84.0\n-\nRULER-100@1M\n86.3\n77.5\n-\nMultilingual\nMMLU-ProX (avg over langs)\n59.5\n77.6*\n69.1*\nWMT24++ (en->xx)\n86.2\n85.6\n83.2\nLicense/Terms of Use\nGoverning Terms: Use of this model is governed by the\nNVIDIA Open Model License Agreement\n."
  },
  {
    "slug": "olmo-3",
    "name": "olmo-3",
    "description": "Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets.",
    "pull_count": "163.4K",
    "updated": "2 months ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "olmo-3:latest",
        "size": "4.5GB",
        "context_window": "64K",
        "input_type": "Text"
      },
      {
        "tag": "olmo-3:7b",
        "size": "4.5GB",
        "context_window": "64K",
        "input_type": "Text"
      },
      {
        "tag": "olmo-3:32b",
        "size": "19GB",
        "context_window": "64K",
        "input_type": "Text"
      }
    ],
    "readme": "Olmo 3, a new family of 7B and 32B models in both Instruct and Think variants. It has long chain-of-thought thinking to improve reasoning tasks like math and coding.\nOlmo is a series of Open language models designed to enable the science of language models.\nThese models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets. Allen AI team is releasing all code, checkpoints, logs, and associated training details.\nModels\nOlmo 3 Instruct 7B\nollama run olmo-3:7b-instruct\nOlmo 3 Think 7B\nollama run olmo-3:7b-think\nOlmo 3 Think 32B\nollama run olmo-3:32b-think\nEvaluation\nOlmo 3 Instruct 7B\nBenchmark\nOlmo3 Instruct 7B\nQwen 3 8B (no reasoning)\nQwen 3 VL 8B Instruct\nQwen 2.5 7B\nOlmo 2 7B Instruct\nApertus 8B Instruct\nGranite 3.3 8B Instruct\nMATH\n87.3\n82.3\n91.6\n71\n30.1\n21.9\n67.3\nAIME 2024\n44.3\n26.2\n55.1\n11.3\n1.3\n0.5\n7.3\nAIME 2025\n32.5\n21.7\n43.3\n6.3\n0.4\n0.2\n6.3\nOMEGA\n28.9\n20.5\n32.3\n13.7\n5.2\n5.0\n10.7\nBigBenchHard\n71.2\n73.7\n85.6\n68.8\n43.8\n42.2\n61.2\nZebraLogic\n32.9\n25.4\n64.3\n10.7\n5.3\n5.3\n17.6\nAGI Eval English\n64.4\n76\n84.5\n69.8\n56.1\n50.8\n64.0\nHumanEvalPlus\n77.2\n79.8\n82.9\n74.9\n25.8\n34.4\n64.0\nMBPP+\n60.2\n64.4\n66.3\n62.6\n40.7\n42.1\n54.0\nLiveCodeBench v3\n29.5\n53.2\n55.9\n34.5\n7.2\n7.8\n11.5\nIFEval\n85.6\n86.3\n87.8\n73.4\n72.2\n71.4\n77.5\nIFBench\n32.3\n29.3\n34\n28.4\n26.7\n22.1\n22.3\nMMLU\n69.1\n80.4\n83.6\n77.2\n61.6\n62.7\n63.5\nPopQA\n14.1\n20.4\n26.5\n21.5\n25.5\n25.5\n28.9\nGPQA\n40.4\n44.6\n51.1\n35.6\n31.3\n28.8\n33.0\nAlpacaEval 2 LC\n40.9\n49.8\n73.5\n23\n18.3\n8.1\n28.6\nSimpleQA\n79.3\n79\n90.3\n78\n‚Äì\n‚Äì\n‚Äì\nLitQA2\n38.2\n39.6\n30.7\n29.8\n‚Äì\n‚Äì\n‚Äì\nBFCL\n49.8\n60.2\n66.2\n55.8\n‚Äì\n‚Äì\n‚Äì\nSafety\n87.3\n78\n80.2\n73.4\n93.1\n72.2\n73.7\nOlmo 3 Think 7B\nBenchmark\nOlmo 3 Think 7B\nOpenThinker3-7B\nNemotron-Nano-9B-v2\nDeepSeek-R1-Distill-Qwen-7B\nQwen 3 8B (reasoning)\nQwen 3 VL 8B Thinker\nOpenReasoning Nemotron 7B\nMATH\n95.1\n94.5\n94.4\n87.9\n95.1\n95.2\n94.6\nAIME 2024\n71.6\n67.7\n72.1\n54.9\n74.0\n70.9\n77.0\nAIME 2025\n64.6\n57.2\n58.9\n40.2\n67.8\n61.5\n73.1\nOMEGA\n37.8\n38.4\n42.4\n28.5\n43.4\n38.1\n43.2\nBBH\n86.6\n77.1\n86.2\n73.5\n84.4\n86.8\n81.3\nZebraLogic\n66.5\n34.9\n60.8\n26.1\n85.2\n91.2\n22.4\nAGI Eval\n81.5\n78.6\n83.1\n69.5\n87.0\n90.1\n81.4\nHumanEval+\n89.9\n87.4\n89.7\n83.0\n80.2\n83.7\n89.7\nMBPP+\n64.7\n61.4\n66.1\n63.5\n69.1\n63.0\n61.2\nLCB v3\n75.2\n68.0\n83.4\n58.8\n86.2\n85.5\n82.3\nIFEval\n88.2\n51.7\n86.0\n59.6\n87.4\n85.5\n42.5\nIFBench\n41.6\n23.0\n34.6\n16.7\n37.1\n40.4\n23.4\nMMLU\n77.8\n77.4\n84.3\n67.9\n85.4\n86.5\n80.7\nPopQA\n23.7\n18.0\n17.9\n12.8\n24.3\n29.3\n14.5\nGPQA\n46.2\n47.6\n56.2\n54.4\n57.7\n61.5\n56.6\nAE 2\n52.1\n24.0\n58.0\n7.7\n60.5\n73.5\n8.6\n70.7\n31.3\n72.1\n54.0\n68.3\n82.9\n30.3\nOlmo 3 Think 32B\nBenchmark\nOlmo 3 Think 32B\nQwen 3 32B\nQwen 3 VL 32B Thinking\nQwen 2.5 32B\nGemma 3 27B Instruct\nGemma 2 27B Instruct\nOlmo 2 32B Instruct\nDeepSeek-R1-Distill-Qwen-32B\nMath\nMATH\n96.1\n95.4\n96.7\n80.2\n87.4\n51.5\n49.2\n92.6\nAIME 2024\n76.8\n80.8\n86.3\n15.7\n28.9\n4.7\n4.6\n70.3\nAIME 2025\n72.5\n70.9\n78.8\n13.4\n22.9\n0.9\n0.9\n56.3\nOMEGA\n50.8\n47.7\n50.8\n19.2\n24.0\n9.1\n9.8\n38.9\nReasoning\nBigBenchHard\n89.8\n90.6\n91.1\n80.9\n82.4\n66.0\n65.6\n89.7\nZebraLogic\n76.0\n88.3\n96.1\n24.1\n24.8\n17.2\n13.3\n69.4\nAGI Eval English\n88.2\n90.0\n92.2\n78.9\n76.9\n70.9\n68.4\n88.1\nCoding\nHumanEvalPlus\n91.4\n91.2\n90.6\n82.6\n79.2\n67.5\n44.4\n92.3\nMBPP+\n68.0\n70.6\n66.2\n66.6\n65.7\n61.2\n49.0\n70.1\nLiveCodeBench v3\n83.5\n90.2\n84.8\n49.9\n39.0\n28.7\n10.6\n79.5\nIF\nIFEval\n89.0\n86.5\n85.5\n81.9\n85.4\n62.1\n85.8\n78.7\nIFBench\n47.6\n37.3\n55.1\n36.7\n31.3\n27.8\n36.4\n23.8\nKnowledge & QA\nMMLU\n85.4\n88.8\n90.1\n84.6\n74.6\n76.1\n77.1\n88.0\nPopQA\n31.9\n30.7\n32.2\n28.0\n30.2\n30.4\n37.2\n26.7\nGPQA\n58.1\n67.3\n67.4\n44.6\n45.0\n39.9\n36.4\n61.8\nChat\nAlpacaEval 2 LC\n74.2\n75.6\n80.9\n81.9\n65.5\n39.8\n38.0\n26.2\nSafety\n68.8\n69.0\n82.7\n81.9\n68.6\n74.3\n83.8\n63.6"
  },
  {
    "slug": "command-r7b",
    "name": "command-r7b",
    "description": "The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.",
    "pull_count": "155.8K",
    "updated": "1 year ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "command-r7b:latest",
        "size": "5.1GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "command-r7b:7b",
        "size": "5.1GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "C4AI Command R7B is an open weights research release of a 7B billion parameter model with advanced capabilities optimized for a variety of use cases including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities with the ability to use and combine multiple tools over multiple steps to accomplish more difficult tasks. It obtains top performance on enterprise relevant code use cases. C4AI Command R7B is a multilingual model trained on 23 languages.\nModel Details\nModel Architecture:\nThis is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety. The model features three layers with sliding window attention (window size 4096) and ROPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence.\nLanguages covered:\nThe model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian."
  },
  {
    "slug": "devstral-small-2",
    "name": "devstral-small-2",
    "description": "24B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.",
    "pull_count": "161.9K",
    "updated": "2 months ago",
    "capabilities": [
      "vision",
      "tools",
      "cloud"
    ],
    "tags": [
      {
        "tag": "devstral-small-2:latest",
        "size": "15GB",
        "context_window": "384K",
        "input_type": "Text, Image"
      },
      {
        "tag": "devstral-small-2:24b",
        "size": "15GB",
        "context_window": "384K",
        "input_type": "Text, Image"
      },
      {
        "tag": "devstral-small-2:24b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Note: this model requires Ollama 0.13.3 or later.\nDownload Ollama\nDevstral Small 2\nDevstral is an agentic LLM for software engineering tasks.\nDevstral 2\nmodels excel at using tools to explore codebases, editing multiple files and power software engineering agents.\nThe model achieves remarkable performance on SWE-bench.\n24B model\nollama run devstral-small-2\nKey Features\nThe Devstral 2 Instruct model offers the following capabilities:\nAgentic Coding\n: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\nImproved Performance\n: Devstral 2 is a step-up compared to its predecessors.\nBetter Generalization\n: Generalises better to diverse prompts and coding environments.\nUse Cases\nAI Code Assistants, Agentic Coding, and Software Engineering Tasks. Leveraging advanced AI capabilities for complex tool integration and deep codebase understanding in coding environments.\nBenchmark Results\nModel/Benchmark\nSize (B Tokens)\nSWE Bench Verified\nSWE Bench Multilingual\nTerminal Bench\nDevstral 2\n123\n72.2%\n61.3%\n40.5%\nDevstral Small 2\n24\n65.8%\n51.6%\n32.0%\nDeepSeek v3.2\n671\n73.1%\n70.2%\n46.4%\nKimi K2 Thinking\n1000\n71.3%\n61.1%\n35.7%\nMiniMax M2\n230\n69.4%\n56.5%\n30.0%\nGLM 4.6\n455\n68.0%\n‚Äì\n40.5%\nQwen 3 Coder Plus\n480\n69.6%\n54.7%\n37.5%\nGemini 3 Pro\n‚Äì\n76.2%\n‚Äì\n54.2%\nClaude Sonnet 4.5\n‚Äì\n77.2%\n68.0%\n42.8%\nGPT 5.1 Codex Max\n‚Äì\n77.9%\n‚Äì\n58.1%\nGPT 5.1 Codex High\n‚Äì\n73.7%\n‚Äì\n52.8%\nLicense\nDevstral Small 2 - 24B\nApache 2.0\nReference\nDevstral 2"
  },
  {
    "slug": "smallthinker",
    "name": "smallthinker",
    "description": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
    "pull_count": "156.2K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "smallthinker:latest",
        "size": "3.6GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "smallthinker:3b",
        "size": "3.6GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "A new model fine-tuned from the Qwen2.5-3b-Instruct model.\nSmallThinker is designed for the following use cases:\nEdge Deployment: Its small size makes it ideal for deployment on resource-constrained devices.\nDraft Model for QwQ-32B-Preview: SmallThinker can serve as a fast and efficient draft model for the larger QwQ-32B-Preview model, yielding a 70% speedup.\nFor achieving reasoning capabilities, it‚Äôs crucial to generate long chains of COT reasoning. Therefore, based on QWQ-32B-Preview, the authors used various synthetic techniques(such as personahub) to create the QWQ-LONGCOT-500K dataset. Compared to other similar datasets, over 75% of the author‚Äôs samples have output tokens exceeding 8K. To encourage research in the open-source community, the dataset was also made publicly available.\nReferences\nHugging Face"
  },
  {
    "slug": "granite3-guardian",
    "name": "granite3-guardian",
    "description": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.",
    "pull_count": "147.4K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "granite3-guardian:latest",
        "size": "2.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "granite3-guardian:2b",
        "size": "2.7GB",
        "context_window": "8K",
        "input_type": "Text"
      },
      {
        "tag": "granite3-guardian:8b",
        "size": "5.8GB",
        "context_window": "8K",
        "input_type": "Text"
      }
    ],
    "readme": "Granite guardian models\nThe IBM Granite Guardian 3.0\n2B and 8B models\nare designed to detect risks in prompts and/or responses. They can help with risk detection along many key dimensions catalogued in the\nIBM AI Risk Atlas\n. They are trained on unique data comprising human annotations and synthetic data informed by internal red-teaming, and they outperform other open-source models in the same space on standard benchmarks.\nParameter Sizes\nThe model will produce a single output token, either\nYes\nor\nNo\n. By default, the general-purpose\nharm\ncategory is used, but other categories can be selected by setting the system prompt.\n2B:\nollama run granite3-guardian:2b\n>>> /set system profanity\n8B:\nollama run granite3-guardian:8b\n>>> /set system violence\nSupported Uses\nRisk detection in prompt text or model response (i.e. as guardrails), such as:\nHarm (\nharm\n): content considered generally harmful\nSocial Bias (\nsocial_bias\n): prejudice based on identity or characteristics\nJailbreaking (\njailbreak\n): deliberate instances of manipulating AI to generate harmful, undesired, or inappropriate content\nViolence (\nviolence\n): content promoting physical, mental, or sexual harm\nProfanity (\nprofanity\n): use of offensive language or insults\nSexual Content (\nsexual_content\n): explicit or suggestive material of a sexual nature\nUnethical Behavior (\nunethical_behavior\n): actions that violate moral or legal standards\nRAG (retrieval-augmented generation) to assess:\nContext relevance (\nrelevance\n): whether the retrieved context is relevant to the query\nGroundedness (\ngroundedness\n): whether the response is accurate and faithful to the provided context\nAnswer relevance (\nanswer_relevance\n): whether the response directly addresses the user‚Äôs query\nGranite dense models\nThe Granite dense models are available in\n2B and 8B\nparameter sizes designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\nSee model page\nGranite mixture of experts models\nThe Granite MoE models are available in\n1B and 3B\nparameter sizes designed for low latency usage and to support deployment in on-device applications or situations requiring instantaneous inference.\nSee model page\nLearn more\nDevelopers:\nIBM Research\nGitHub Repository:\nibm-granite/granite-guardian\nWebsite\n:\nGranite Guardian Docs\nCookbook\n:\nGranite Guardian Snack\nRelease Date\n: October 21st, 2024\nLicense:\nApache 2.0\n."
  },
  {
    "slug": "alfred",
    "name": "alfred",
    "description": "A robust conversational model designed to be used for both chat and instruct use cases.",
    "pull_count": "110.9K",
    "updated": "2 years ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "alfred:latest",
        "size": "24GB",
        "context_window": "2K",
        "input_type": "Text"
      },
      {
        "tag": "alfred:40b",
        "size": "24GB",
        "context_window": "2K",
        "input_type": "Text"
      }
    ],
    "readme": "Alfred is a robust conversational model created by LightOn, based on the\nFalcon\n40B model. It‚Äôs designed to be used for both chat and instruct use cases.\nSome of its features include:\nReduced Hallucinations: ability to minimize hallucinations, ensuring more accurate and reliable outputs.\nEnhanced Self-Awareness: In situations where the model lacks a definitive answer, Alfred-40B-1023 is now programmed to state, ‚ÄúI don‚Äôt know‚Äù, enhancing its transparency and trustworthiness.\nSuperior ‚ÄòChat with Docs‚Äô Capability: Alfred-40B-1023 is trained to perform ‚ÄòChat with Docs‚Äô tasks like no other, streamlining document interaction and information retrieval.\nExpanded Context: With an increased context of 8K tokens, Alfred-40B-1023 can comprehend and generate longer and more intricate content, ensuring detailed and comprehensive responses.\nReferences\nHuggingFace\nBlog Post"
  },
  {
    "slug": "command-r7b-arabic",
    "name": "command-r7b-arabic",
    "description": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.",
    "pull_count": "86.8K",
    "updated": "12 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "command-r7b-arabic:latest",
        "size": "5.1GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "command-r7b-arabic:7b",
        "size": "5.1GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Command R7B Arabic is a fast and highly efficient model that can be served on low-end GPUs, a MacBook, or even CPUs. Similar to other models in the R series, it offers a context length of 128k and industry-leading performance in its class across capabilities that matter most to businesses like regional language understanding and strong accuracy with citations using retrieval-augmented generation (RAG). Its compact size enables businesses to more easily scale Arabic language AI applications to production.\nReferences\nBlog Post\nHugging Face"
  },
  {
    "slug": "olmo-3.1",
    "name": "olmo-3.1",
    "description": "Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets.",
    "pull_count": "98.8K",
    "updated": "2 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "olmo-3.1:latest",
        "size": "19GB",
        "context_window": "64K",
        "input_type": "Text"
      },
      {
        "tag": "olmo-3.1:32b",
        "size": "19GB",
        "context_window": "64K",
        "input_type": "Text"
      }
    ],
    "readme": "Olmo 3.1 models are available either as a 32B parameter thinking or instruct model. It has long chain-of-thought thinking to improve reasoning tasks like math and coding.\nOlmo is a series of Open language models designed to enable the science of language models.\nThese models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets. Allen AI team is releasing all code, checkpoints, logs, and associated training details.\nModels\nOlmo 3.1 Instruct 32B\nollama run olmo-3.1:32b-instruct\nOlmo 3.1 Think 32B\nollama run olmo-3.1:32b-think\nBenchmark\nBenchmark\nOlmo 3.1 32B Think\nOlmo 3 Think 32B\nQwen 3 32B\nQwen 3 VL 32B Thinking\nQwen 2.5 32B\nGemma 3 27B Instruct\nGemma 2 27B Instruct\nOlmo 2 32B Instruct\nDeepSeek-R1-Distill-Qwen-32B\nMath\nMATH\n96.2\n96.1\n95.4\n96.7\n80.2\n87.4\n51.5\n49.2\n92.6\nAIME 2024\n80.6\n76.8\n80.8\n86.3\n15.7\n28.9\n4.7\n4.6\n70.3\nAIME 2025\n78.1\n72.5\n70.9\n78.8\n13.4\n22.9\n0.9\n0.9\n56.3\nOMEGA\n53.4\n50.8\n47.7\n50.8\n19.2\n24.0\n9.1\n9.8\n38.9\nReasoning\nBigBenchHard\n88.6\n89.8\n90.6\n91.1\n80.9\n82.4\n66.0\n65.6\n89.7\nZebraLogic\n80.1\n76.0\n88.3\n96.1\n24.1\n24.8\n17.2\n13.3\n69.4\nAGI Eval English\n89.2\n88.2\n90.0\n92.2\n78.9\n76.9\n70.9\n68.4\n88.1\nCoding\nHumanEvalPlus\n91.5\n91.4\n91.2\n90.6\n82.6\n79.2\n67.5\n44.4\n92.3\nMBPP+\n68.3\n68.0\n70.6\n66.2\n66.6\n65.7\n61.2\n49.0\n70.1\nLiveCodeBench v3\n83.3\n83.5\n90.2\n84.8\n49.9\n39.0\n28.7\n10.6\n79.5\nIF\nIFEval\n93.8\n89.0\n86.5\n85.5\n81.9\n85.4\n62.1\n85.8\n78.7\nIFBench\n68.1\n47.6\n37.3\n55.1\n36.7\n31.3\n27.8\n36.4\n23.8\nKnowledge & QA\nMMLU\n86.4\n85.4\n88.8\n90.1\n84.6\n74.6\n76.1\n77.1\n88.0\nPopQA\n30.9\n31.9\n30.7\n32.2\n28.0\n30.2\n30.4\n37.2\n26.7\nGPQA\n57.5\n58.1\n67.3\n67.4\n44.6\n45.0\n39.9\n36.4\n61.8\nChat\nAlpacaEval 2 LC\n69.1\n74.2\n75.6\n80.9\n81.9\n65.5\n39.8\n38.0\n26.2\nSafety\n83.6\n68.8\n69.0\n82.7\n81.9\n68.6\n74.3\n83.8\n63.6"
  },
  {
    "slug": "kimi-k2.5",
    "name": "kimi-k2.5",
    "description": "Kimi K2.5 is an open-source, native multimodal agentic model that seamlessly integrates vision and language understanding with advanced agentic capabilities, instant and thinking modes, as well as conversational and agentic paradigms.",
    "pull_count": "92.4K",
    "updated": "1 month ago",
    "capabilities": [
      "vision",
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "kimi-k2.5:cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Kimi K2.5 is an open-source, native multimodal agentic model built through continual pretraining on approximately 15 trillion mixed visual and text tokens atop Kimi-K2-Base. It seamlessly integrates vision and language understanding with advanced agentic capabilities, instant and thinking modes, as well as conversational and agentic paradigms.\nKey Features\nNative Multimodality\n: Pre-trained on vision‚Äìlanguage tokens, K2.5 excels in visual knowledge, cross-modal reasoning, and agentic tool use grounded in visual inputs.\nCoding with Vision\n: K2.5 generates code from visual specifications (UI designs, video workflows) and autonomously orchestrates tools for visual data processing.\nAgent Swarm\n: K2.5 transitions from single-agent scaling to a self-directed, coordinated swarm-like execution scheme. It decomposes complex tasks into parallel sub-tasks executed by dynamically instantiated, domain-specific agents."
  },
  {
    "slug": "devstral-2",
    "name": "devstral-2",
    "description": "123B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.",
    "pull_count": "91K",
    "updated": "2 months ago",
    "capabilities": [
      "tools",
      "cloud"
    ],
    "tags": [
      {
        "tag": "devstral-2:latest",
        "size": "75GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "devstral-2:123b",
        "size": "75GB",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "devstral-2:123b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text"
      }
    ],
    "readme": "Devstral 2\nDevstral is an agentic LLM for software engineering tasks.\nDevstral 2\nexcels at using tools to explore codebases, editing multiple files and power software engineering agents.\nThe model achieves remarkable performance on SWE-bench.\n123B model\nollama run devstral-2\nOllama‚Äôs Cloud\nollama run devstral-2:123b-cloud\nKey Features\nThe Devstral 2 Instruct model offers the following capabilities:\nAgentic Coding\n: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\nImproved Performance\n: Devstral 2 is a step-up compared to its predecessors.\nBetter Generalization\n: Generalises better to diverse prompts and coding environments.\nUse Cases\nAI Code Assistants, Agentic Coding, and Software Engineering Tasks. Leveraging advanced AI capabilities for complex tool integration and deep codebase understanding in coding environments.\nBenchmark Results\nModel/Benchmark\nSize (B Tokens)\nSWE Bench Verified\nSWE Bench Multilingual\nTerminal Bench\nDevstral 2\n123\n72.2%\n61.3%\n40.5%\nDevstral Small 2\n24\n65.8%\n51.6%\n32.0%\nDeepSeek v3.2\n671\n73.1%\n70.2%\n46.4%\nKimi K2 Thinking\n1000\n71.3%\n61.1%\n35.7%\nMiniMax M2\n230\n69.4%\n56.5%\n30.0%\nGLM 4.6\n455\n68.0%\n‚Äì\n40.5%\nQwen 3 Coder Plus\n480\n69.6%\n54.7%\n37.5%\nGemini 3 Pro\n‚Äì\n76.2%\n‚Äì\n54.2%\nClaude Sonnet 4.5\n‚Äì\n77.2%\n68.0%\n42.8%\nGPT 5.1 Codex Max\n‚Äì\n77.9%\n‚Äì\n58.1%\nGPT 5.1 Codex High\n‚Äì\n73.7%\n‚Äì\n52.8%\nLicense\nDevstral 2 - 123B\nModified MIT License\nAttribution notice: 2025 - Mistral AI\nPermission is hereby granted, free of charge, to any person obtaining a copy of the weights of this model and associated documentation files (the ‚ÄúModel‚Äù), to deal in the Model without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Model, and to permit persons to whom the Model is furnished to do so, subject to the following conditions:\nThe above attribution notice and this permission notice shall be included in all copies or substantial portions of the Model.\nYou are not authorized to exercise any rights under this license if the global consolidated monthly revenue of your company (or that of your employer) exceeds $20 million (or its equivalent in another currency) for the preceding month. This restriction in (b) applies to the Model and any derivatives, modifications, or combined works based on it, whether provided by Mistral AI or by a third party. You may contact Mistral AI (sales@mistral.ai) to request a commercial license, which Mistral AI may grant you at its sole discretion, or choose to use the Model on Mistral AI‚Äôs hosted services available at\nhttps://mistral.ai/\n.\nTHE MODEL IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL MISTRAL AI BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE MODEL OR THE USE OR OTHER DEALINGS IN THE MODEL.\nReference\nDevstral Small 2"
  },
  {
    "slug": "cogito-2.1",
    "name": "cogito-2.1",
    "description": "The Cogito v2.1 LLMs are instruction tuned generative models. All models are released under MIT license for commercial use.",
    "pull_count": "74.2K",
    "updated": "3 months ago",
    "capabilities": [
      "cloud"
    ],
    "tags": [
      {
        "tag": "cogito-2.1:latest",
        "size": "1.3TB",
        "context_window": "160K",
        "input_type": "Text"
      },
      {
        "tag": "cogito-2.1:671b",
        "size": "1.3TB",
        "context_window": "160K",
        "input_type": "Text"
      },
      {
        "tag": "cogito-2.1:671b-cloud",
        "size": "-",
        "context_window": "160K",
        "input_type": "Text"
      }
    ],
    "readme": "cogito-2.1 models require\nOllama v0.13.0\nor later.\nThe Cogito v2.1 LLMs are instruction tuned generative models. All models are released under MIT license for commercial use.\nThe best open-weight LLM by a US company: Cogito v2.1 671B\nOn most industry benchmarks and our internal evals, the model performs competitively to frontier closed and open models, while being ahead of any other US open model.\nThis model uses significantly fewer tokens amongst any similar capability models, because it has better reasoning capabilities. It also has improvements across instruction following, coding, longer queries, multi-turn and creativity.\nThis model is trained in over 30 languages and supports a context length of 128k.\nModels\n671B\nollama run cogito-2.1\nOllama‚Äôs cloud\nollama run cogito-2.1:671b-cloud\nEvaluation\nNote from Cogito:\nWhile these benchmarks provide a useful signal, they do not fully capture real-world performance. That said, our models have been tested across multiple internal and external evaluations and consistently perform well.\nUltimately, the best evals are the ones closest to the user‚Äôs needs.\nWe are confident that our models will stand up to such real-world evaluations and deliver strong results in practice.\nCogito v2.1 model has the lowest average tokens3 used with respect to reasoning models of similar capabilities."
  },
  {
    "slug": "deepseek-v2.5",
    "name": "deepseek-v2.5",
    "description": "An upgraded version of DeekSeek-V2  that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.",
    "pull_count": "154K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "deepseek-v2.5:latest",
        "size": "133GB",
        "context_window": "4K",
        "input_type": "Text"
      },
      {
        "tag": "deepseek-v2.5:236b",
        "size": "133GB",
        "context_window": "4K",
        "input_type": "Text"
      }
    ],
    "readme": "DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.\nDeepSeek-V2.5 better aligns with human preferences and has been optimized in various aspects, including writing and instruction following:\nMetric\nDeepSeek-V2-0628\nDeepSeek-Coder-V2-0724\nDeepSeek-V2.5\nAlpacaEval 2.0\n46.6\n44.5\n50.5\nArenaHard\n68.3\n66.3\n76.2\nAlignBench\n7.88\n7.91\n8.04\nMT-Bench\n8.85\n8.91\n9.02\nHumanEval python\n84.5\n87.2\n89\nHumanEval Multi\n73.8\n74.8\n73.8\nLiveCodeBench(01-09)\n36.6\n39.7\n41.8\nAider\n69.9\n72.9\n72.2\nSWE-verified\nN/A\n19\n16.8\nDS-FIM-Eval\nN/A\n73.2\n78.3\nDS-Arena-Code\nN/A\n49.5\n63.1\nReference\nHugging Face"
  },
  {
    "slug": "phi4-mini-reasoning",
    "name": "phi4-mini-reasoning",
    "description": "Phi 4 mini reasoning is a lightweight open model that balances efficiency with advanced reasoning ability.",
    "pull_count": "153.9K",
    "updated": "10 months ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "phi4-mini-reasoning:latest",
        "size": "3.2GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "phi4-mini-reasoning:3.8b",
        "size": "3.2GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Phi 4 mini reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking.\nThe graph compares the performance of various models on popular math benchmarks for long sentence generation. Phi-4-mini-reasoning outperforms its base model on long sentence generation across each evaluation, as well as larger models like OpenThinker-7B, Llama-3.2-3B-instruct, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Llama-8B, and Bespoke-Stratos-7B. Phi-4-mini-reasoning is comparable to OpenAI o1-mini across math benchmarks, surpassing the model‚Äôs performance during Math-500 and GPQA Diamond evaluations. As seen above, Phi-4-mini-reasoning with 3.8B parameters outperforms models of over twice its size.\nReferences\nBlog post"
  },
  {
    "slug": "command-a",
    "name": "command-a",
    "description": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI",
    "pull_count": "130.9K",
    "updated": "11 months ago",
    "capabilities": [
      "tools"
    ],
    "tags": [
      {
        "tag": "command-a:latest",
        "size": "67GB",
        "context_window": "16K",
        "input_type": "Text"
      },
      {
        "tag": "command-a:111b",
        "size": "67GB",
        "context_window": "16K",
        "input_type": "Text"
      }
    ],
    "readme": "Command A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI. Compared to other leading proprietary and open-weights models Command A delivers maximum performance with minimum hardware costs, excelling on business-critical agentic and multilingual tasks while‚Ä¨ being deployable on just two GPUs.\nLanguages covered:\nThe model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\nContext Window:\nUp to 256K.\nUse cases\nCommand A is designed with the following capabilities.\nChat\nBy default, Command A is configured as a conversational model. A preamble conditions the model on interactive behaviour, meaning it is expected to reply in a conversational fashion, provides introductory statements and follow-up questions, and uses Markdown as well as LaTeX where appropriate. This is desired for interactive experiences, such as chatbots, where the model engages in dialogue.\nRetrieval augmented generation (RAG)\nCommand A has been trained specifically for tasks like the final step of Retrieval Augmented Generation (RAG).\nTool Support\nCommand A has been specifically trained with conversational tool use capabilities. This allows the model to interact with external tools like APIs, databases, or search engines.\nCode\nCommand A has meaningfully improved on code capabilities.  In addition to academic code benchmarks, we have evaluated it on enterprise-relevant scenarios, including SQL generation and code translation, where it outperforms other models of similar size. Try these out by requesting code snippets, code explanations, or code rewrites. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions."
  },
  {
    "slug": "gpt-oss-safeguard",
    "name": "gpt-oss-safeguard",
    "description": "gpt-oss-safeguard-20b and gpt-oss-safeguard-120b are safety reasoning models built-upon gpt-oss",
    "pull_count": "73.4K",
    "updated": "4 months ago",
    "capabilities": [
      "tools",
      "thinking"
    ],
    "tags": [
      {
        "tag": "gpt-oss-safeguard:latest",
        "size": "14GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "gpt-oss-safeguard:20b",
        "size": "14GB",
        "context_window": "128K",
        "input_type": "Text"
      },
      {
        "tag": "gpt-oss-safeguard:120b",
        "size": "65GB",
        "context_window": "128K",
        "input_type": "Text"
      }
    ],
    "readme": "Get started\n20B:\nollama run gpt-oss-safeguard:20b\nThis model is designed to fit into GPUs with 16GB of VRAM. (21B parameters with 3.6B active parameters).\n120B:\nollama run gpt-oss-safeguard:120b\nThis model is designed to fit into a single NVIDIA H100 GPU (117B parameters with 5.1B active parameters).\nHighlights\nTrained to reason about safety\n: Trained and tuned for safety reasoning to accommodate use cases like LLM input-output filtering, online content labeling and offline labeling for Trust and Safety use cases.\nBring your own policy:\nInterprets your written policy, so it generalizes across products and use cases with minimal engineering.\nReasoned decisions, not just scores:\nGain complete access to the model‚Äôs reasoning process, facilitating easier debugging and increased trust in policy decisions. Keep in mind Raw CoT is meant for developers and safety practitioners. It‚Äôs not intended for exposure to general users or use cases outside of safety contexts.\nConfigurable reasoning effort:\nEasily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\nPermissive Apache 2.0 license:\nBuild freely without copyleft restrictions or patent risk‚Äîideal for experimentation, customization, and commercial deployment.\nJoin the ROOST Model Community\ngpt-oss-safeguard is a model partner of the\nRobust Open Online Safety Tools (ROOST)\nModel Community. The ROOST Model Community (RMC) is a group of safety practitioners exploring open source AI models to protect online spaces. As an RMC model partner, OpenAI is committed to incorporating user feedback and jointly iterating on future releases in pursuit of open safety. Visit the\nRMC GitHub repo\nto learn more about this partnership and how to get involved.\nReferences\nOpenAI blog\nOpenAI\ngpt-oss-safeguard developer cookbook\nROOST‚Äôs\nmodel community repository\non GitHub"
  },
  {
    "slug": "gemini-3-flash-preview",
    "name": "gemini-3-flash-preview",
    "description": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost.",
    "pull_count": "65.9K",
    "updated": "2 months ago",
    "capabilities": [
      "vision",
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "gemini-3-flash-preview:latest",
        "size": "-",
        "context_window": "1M",
        "input_type": "Text"
      },
      {
        "tag": "gemini-3-flash-preview:cloud",
        "size": "-",
        "context_window": "1M",
        "input_type": "Text"
      }
    ],
    "readme": "Model\nGemini 3 Flash Preview on Ollama‚Äôs cloud:\nollama run gemini-3-flash-preview:cloud\nBenchmarks\nGemini 3 Flash demonstrates that speed and scale don‚Äôt have to come at the cost of intelligence. It delivers frontier performance on PhD-level reasoning and knowledge benchmarks like GPQA Diamond (90.4%) and Humanity‚Äôs Last Exam (33.7% without tools), rivaling larger frontier models, and significantly outperforming even the best 2.5 model, Gemini 2.5 Pro, across a number of benchmarks. It also reaches state-of-the-art performance with an impressive score of 81.2% on MMMU Pro, comparable to Gemini 3 Pro."
  },
  {
    "slug": "glm-4.6",
    "name": "glm-4.6",
    "description": "Advanced agentic, reasoning and coding capabilities.",
    "pull_count": "71.1K",
    "updated": "4 months ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "glm-4.6:cloud",
        "size": "-",
        "context_window": "198K",
        "input_type": "Text"
      }
    ],
    "readme": "Model Introduction\nGLM-4.6\nbrings several key improvements:\nLonger context window:\nThe context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance:\nThe model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code„ÄÅCline„ÄÅRoo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning:\nGLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents:\nGLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing:\nBetter aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as\nDeepSeek-V3.1-Terminus\nand\nClaude Sonnet 4\n."
  },
  {
    "slug": "marco-o1",
    "name": "marco-o1",
    "description": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
    "pull_count": "119.3K",
    "updated": "1 year ago",
    "capabilities": [],
    "tags": [
      {
        "tag": "marco-o1:latest",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "marco-o1:7b",
        "size": "4.7GB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Fine-Tuning with CoT Data:\nWe develop\nMarco-o1-CoT\nby performing full-parameter fine-tuning on the base model using open-source CoT dataset combined with our self-developed synthetic data.\nSolution Space Expansion via MCTS:\nWe integrate LLMs with MCTS (\nMarco-o1-MCTS\n), using the model‚Äôs output confidence to guide the search and expand the solution space.\nReasoning Action Strategy:\nWe implement novel reasoning action strategies and a reflection mechanism (\nMarco-o1-MCTS mini-step\n), including exploring different action granularities within the MCTS framework and prompting the model to self-reflect, thereby significantly enhancing the model‚Äôs ability to solve complex problems.\nApplication in Translation Tasks:\nWe are the first to apply Large Reasoning Models (LRM) to\nMachine Translation task\n, exploring inference time scaling laws in the multilingual and translation domain.\nUsage\nollama run marco-o1 \"How many Rs are in strawberry?\"\nParse the resulting string between\n<Output>\nand\n</Output>\n:\n...\n<Output>\nThere are 3 Rs in strawberry.\n</Output>\nReferences\nGitHub\nHuggingFace"
  },
  {
    "slug": "functiongemma",
    "name": "functiongemma",
    "description": "FunctionGemma is a specialized version of Google's Gemma 3 270M model fine-tuned explicitly for function calling.",
    "pull_count": "72K",
    "updated": "2 months ago",
    "capabilities": [
      "tools",
      "270m"
    ],
    "tags": [
      {
        "tag": "functiongemma:latest",
        "size": "301MB",
        "context_window": "32K",
        "input_type": "Text"
      },
      {
        "tag": "functiongemma:270m",
        "size": "301MB",
        "context_window": "32K",
        "input_type": "Text"
      }
    ],
    "readme": "Requires Ollama v0.13.5 or later\nFunctionGemma\nFunctionGemma is a lightweight, open model from Google, built as a foundation for creating your own specialized function calling models. The model is well suited for text-only function calling. The uniquely small size makes it possible to deploy in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\nFunctionGemma is not intended for use as a direct dialogue model, and is designed to be highly performant after further fine-tuning, as is typical of models this size.\nBuilt on the Gemma 3 270M model and with the same research and technology used to create the Gemini models, FunctionGemma has been trained specifically for function calling. The model has the same architecture as Gemma 3, but uses a different chat format.\nFurthermore, akin to the base Gemma 270M, the model has been optimized to be extremely versatile, performant on a variety of hardware in single turn scenarios, but should be finetuned on single turn or multiturn task specific data to achieve best accuracy in specific domains.\nExamples\nPython\nRun the python example with\nuv run tool.py\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"ollama\",\n#     \"rich\",\n# ]\n# ///\n\"\"\"\nSingle tool, single turn example.\nRun with: uv run tool.py\n\"\"\"\n\nimport json\n\nfrom rich import print\n\nfrom ollama import chat\n\nmodel = 'functiongemma'\n\n\ndef get_weather(city: str) -> str:\n  \"\"\"\n  Get the current weather for a city.\n\n  Args:\n    city: The name of the city\n\n  Returns:\n    A string describing the weather\n  \"\"\"\n  return json.dumps({'city': city, 'temperature': 22, 'unit': 'celsius', 'condition': 'sunny'})\n\n\nmessages = [{'role': 'user', 'content': 'What is the weather in Paris?'}]\nprint('Prompt:', messages[0]['content'])\n\nresponse = chat(model, messages=messages, tools=[get_weather])\n\nif response.message.tool_calls:\n  tool = response.message.tool_calls[0]\n  print(f'Calling: {tool.function.name}({tool.function.arguments})')\n\n  result = get_weather(**tool.function.arguments)\n  print(f'Result: {result}')\n\n  messages.append(response.message)\n  messages.append({'role': 'tool', 'content': result})\n\n  final = chat(model, messages=messages)\n  print('Response:', final.message.content)\nelse:\n  print('Response:', response.message.content)\nTypeScript\nRun the TypeScript example with\nbun run tool.ts\nor\nnpx tsx tool.ts\n/**\n * Single tool, single turn example.\n * Run with: bun run tool.ts or npx tsx tool.ts\n */\n\nconst OLLAMA_HOST = process.env.OLLAMA_HOST || 'http://localhost:11434';\nconst MODEL = 'functiongemma';\n\nfunction getWeather(city: string): string {\n  return JSON.stringify({ city, temperature: 22, unit: 'celsius', condition: 'sunny' });\n}\n\nconst tools = [\n  {\n    type: 'function',\n    function: {\n      name: 'get_weather',\n      description: 'Get the current weather for a city.',\n      parameters: {\n        type: 'object',\n        properties: {\n          city: { type: 'string', description: 'The name of the city' },\n        },\n        required: ['city'],\n      },\n    },\n  },\n];\n\ninterface Message {\n  role: string;\n  content: string;\n  tool_calls?: { function: { name: string; arguments: Record<string, string> } }[];\n}\n\ninterface ChatResponse {\n  message: Message;\n}\n\nasync function chat(messages: Message[]): Promise<ChatResponse> {\n  const response = await fetch(`${OLLAMA_HOST}/api/chat`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ model: MODEL, messages, tools, stream: false }),\n  });\n\n  if (!response.ok) {\n    throw new Error(`HTTP error: ${response.status} ${await response.text()}`);\n  }\n\n  return response.json();\n}\n\nasync function main() {\n  const messages: Message[] = [{ role: 'user', content: 'What is the weather in Paris?' }];\n  console.log('Prompt:', messages[0].content);\n\n  const response = await chat(messages);\n\n  if (response.message.tool_calls?.length) {\n    const tool = response.message.tool_calls[0];\n    console.log(`Calling: ${tool.function.name}(${JSON.stringify(tool.function.arguments)})\\n`);\n\n    const result = getWeather(tool.function.arguments.city);\n    console.log('Function Result:', result);\n\n    messages.push(response.message);\n    messages.push({ role: 'tool', content: result });\n\n    const final = await chat(messages);\n    console.log('Response:', final.message.content);\n  } else {\n    console.log('Response:', response.message.content);\n  }\n}\n\nmain().catch(console.error);\nBenchmark\nBenchmark\nn-shot\nFunction Gemma 270m\nBFCL Simple\n0-shot\n61.6\nBFCL Multiple\n0-shot\n63.5\nBFCL Parallel\n0-shot\n39\nBFCL Parallel Multiple\n0-shot\n29.5\nBFCL Live Simple\n0-shot\n36.2\nBFCL Live Multiple\n0-shot\n25.7\nBFCL Live Parallel\n0-shot\n22.9\nBFCL Live Parallel Multiple\n0-shot\n20.8\nBFCL Relevance\n0-shot\n61.1\nBFCL Irrelevance\n0-shot\n73.7\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety of sources. The model was trained with 6T tokens. The knowledge cutoff date for the training data was August 2024. There are the key components:\nPublic Tool Definitions - Common APIs found on the web\nTool Use Interactions - These are a mix of prompts, function calls, function responses, and natural language responses from the model to summarise the function call response, or request clarifications when the prompt is ambiguous or incomplete.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training data:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in line with Google‚Äôs\npolicies\n."
  },
  {
    "slug": "minimax-m2",
    "name": "minimax-m2",
    "description": "MiniMax M2 is a high-efficiency large language model built for coding and agentic workflows.",
    "pull_count": "63.9K",
    "updated": "4 months ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "minimax-m2:cloud",
        "size": "-",
        "context_window": "200K",
        "input_type": "Text"
      }
    ],
    "readme": "MiniMax M2 is a high-efficiency large language model built for coding and agentic workflows.\nGet started\nollama run minimax-m2:cloud\nHighlights\nSuperior Intelligence\n. According to benchmarks from Artificial Analysis, MiniMax-M2 demonstrates highly competitive general intelligence across mathematics, science, instruction following, coding, and agentic tool use.\nIts composite score ranks #1 among open-source models globally\n.\nAdvanced Coding\n. Engineered for end-to-end developer workflows, MiniMax-M2 excels at multi-file edits, coding-run-fix loops, and test-validated repairs. Strong performance on Terminal-Bench and (Multi-)SWE-Bench‚Äìstyle tasks demonstrates practical effectiveness in terminals, IDEs, and CI across languages.\nAgent Performance\n. MiniMax-M2 plans and executes complex, long-horizon toolchains across shell, browser, retrieval, and code runners. In BrowseComp-style evaluations, it consistently locates hard-to-surface sources, maintains evidence traceable, and gracefully recovers from flaky steps.\nEfficient Design\n. With 10 billion activated parameters (230 billion in total), MiniMax-M2 delivers lower latency, lower cost, and higher throughput for interactive agents and batched sampling‚Äîperfectly aligned with the shift toward highly deployable models that still shine on coding and agentic tasks.\nCoding & Agentic Benchmarks\nThese comprehensive evaluations test real-world end-to-end coding and agentic tool use: editing real repos, executing commands, browsing the web, and delivering functional solutions. Performance on this suite correlates with day-to-day developer experience in terminals, IDEs, and CI.\nBenchmark\nMiniMax-M2\nClaude Sonnet 4\nClaude Sonnet 4.5\nGemini 2.5 Pro\nGPT-5 (thinking)\nGLM-4.6\nKimi K2 0905\nDeepSeek-V3.2\nSWE-bench Verified\n69.4\n72.7 *\n77.2 *\n63.8 *\n74.9 *\n68 *\n69.2 *\n67.8 *\nMulti-SWE-Bench\n36.2\n35.7 *\n44.3\n/\n/\n30\n33.5\n30.6\nSWE-bench Multilingual\n56.5\n56.9 *\n68\n/\n/\n53.8\n55.9 *\n57.9 *\nTerminal-Bench\n46.3\n36.4 *\n50 *\n25.3 *\n43.8 *\n40.5 *\n44.5 *\n37.7 *\nArtifactsBench\n66.8\n57.3*\n61.5\n57.7*\n73*\n59.8\n54.2\n55.8\nBrowseComp\n44\n12.2\n19.6\n9.9\n54.9*\n45.1*\n14.1\n40.1*\nBrowseComp-zh\n48.5\n29.1\n40.8\n32.2\n65\n49.5\n28.8\n47.9*\nGAIA (text only)\n75.7\n68.3\n71.2\n60.2\n76.4\n71.9\n60.2\n63.5\nxbench-DeepSearch\n72\n64.6\n66\n56\n77.8\n70\n61\n71\nHLE (w/ tools)\n31.8\n20.3\n24.5\n28.4 *\n35.2 *\n30.4 *\n26.9 *\n27.2 *\nœÑ¬≤-Bench\n77.2\n65.5*\n84.7*\n59.2\n80.1*\n75.9*\n70.3\n66.7\nFinSearchComp-global\n65.5\n42\n60.8\n42.6*\n63.9*\n29.2\n29.5*\n26.2\nAgentCompany\n36\n37\n41\n39.3*\n/\n35\n30\n34\nNotes: Data points marked with an asterisk (*) are taken directly from the model‚Äôs official tech report or blog. All other metrics were obtained using the evaluation methods described below.\n- SWE-bench Verified:  We use the same scaffold as\nR2E-Gym\n(Jain et al. 2025) on top of OpenHands to test with agents on SWE tasks. All scores are validated on our internal infrastructure with 128k context length, 100 max steps, and no test-time scaling. All git-related content is removed to ensure agent sees only the code at the issue point.\n- Multi-SWE-Bench & SWE-bench Multilingual: All scores are averaged across 8 runs using the\nclaude-code\nCLI (300 max steps) as the evaluation scaffold.\n- Terminal-Bench: All scores are evaluated with the official claude-code from the original\nTerminal-Bench\nrepository(commit\n94bf692\n), averaged over 8 runs to report the mean pass rate.\n- ArtifactsBench: All Scores are computed by averaging three runs with the official implementation of\nArtifactsBench\n, using the stable Gemini-2.5-Pro as the judge model.\n- BrowseComp & BrowseComp-zh & GAIA (text only) & xbench-DeepSearch: All scores reported use the same agent framework as\nWebExplorer\n(Liu et al. 2025), with minor tools description adjustment. We use the 103-sample text-only GAIA validation subset following\nWebExplorer\n(Liu et al. 2025).\n- HLE (w/ tools): All reported scores are obtained using search tools and a Python tool. The search tools employ the same agent framework as\nWebExplorer\n(Liu et al. 2025), and the Python tool runs in a Jupyter environment. We use the text-only HLE subset.\n- œÑ¬≤-Bench: All scores reported use ‚Äúextended thinking with tool use‚Äù, and employ GPT-4.1 as the user simulator.\n- FinSearchComp-global: Official results are reported for GPT-5-Thinking, Gemini 2.5 Pro, and Kimi-K2. Other models are evaluated using the open-source\nFinSearchComp\n(Hu et al. 2025) framework using both  search and Python tools, launched simultaneously for consistency.\n- AgentCompany: All scores reported use OpenHands 0.42 agent framework.\nIntelligence Benchmarks\nWe align with\nArtificial Analysis\n(AA), which aggregates challenging benchmarks using a consistent methodology to reflect a model‚Äôs broader\nintelligence profile\nacross math, science, instruction following, coding, and agentic tool use.\nMetric (AA)\nMiniMax-M2\nClaude Sonnet 4\nClaude Sonnet 4.5\nGemini 2.5 Pro\nGPT-5 (thinking)\nGLM-4.6\nKimi K2 0905\nDeepSeek-V3.2\nAIME25\n78\n74\n88\n88\n94\n86\n57\n88\nMMLU-Pro\n82\n84\n88\n86\n87\n83\n82\n85\nGPQA-Diamond\n78\n78\n83\n84\n85\n78\n77\n80\nHLE (w/o tools)\n12.5\n9.6\n17.3\n21.1\n26.5\n13.3\n6.3\n13.8\nLiveCodeBench (LCB)\n83\n66\n71\n80\n85\n70\n61\n79\nSciCode\n36\n40\n45\n43\n43\n38\n31\n38\nIFBench\n72\n55\n57\n49\n73\n43\n42\n54\nAA-LCR\n61\n65\n66\n66\n76\n54\n52\n69\nœÑ¬≤-Bench-Telecom\n87\n65\n78\n54\n85\n71\n73\n34\nTerminal-Bench-Hard\n24\n30\n33\n25\n31\n23\n23\n29\nAA Intelligence\n61\n57\n63\n60\n69\n56\n50\n57\nAA: All scores of MiniMax-M2 aligned with Artificial Analysis Intelligence Benchmarking Methodology (\nhttps://artificialanalysis.ai/methodology/intelligence-benchmarking\n). All scores of other models reported from\nhttps://artificialanalysis.ai/\n."
  },
  {
    "slug": "glm-5",
    "name": "glm-5",
    "description": "A strong reasoning and agentic model from Z.ai with 744B total parameters (40B active), built for complex systems engineering and long-horizon tasks.",
    "pull_count": "58.7K",
    "updated": "2 weeks ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "glm-5:cloud",
        "size": "-",
        "context_window": "198K",
        "input_type": "Text"
      }
    ],
    "readme": "GLM-5 is a mixture-of-experts model from\nZ.ai\nwith 744B total parameters and 40B active parameters. It scales up from GLM-4.5‚Äôs 355B parameters and is designed for complex reasoning, coding, and agentic tasks.\nThe model uses DeepSeek Sparse Attention (DSA) to reduce deployment costs while preserving long-context capacity, and was post-trained using a novel asynchronous RL infrastructure for improved training efficiency.\nKey capabilities\nReasoning and math:\nScores 92.7% on AIME 2026 I and 86.0% on GPQA-Diamond\nCoding:\nAchieves 77.8% on SWE-bench Verified and 73.3% on SWE-bench Multilingual\nAgentic tasks:\n62.0 on BrowseComp and 56.2 on Terminal-Bench 2.0\nLong context:\nSupports up to 128K+ context through DSA optimization\nMultilingual:\nSupports English and Chinese\nBenchmarks\nFootnote\nHumanity‚Äôs Last Exam (HLE) & other reasoning tasks\n: We evaluate with a maximum generation length of 131,072 tokens (\ntemperature=1.0, top_p=0.95, max_new_tokens=131072\n). By default, we report the text-only subset; results marked with * are from the full set. We use GPT-5.2 (medium) as the judge model. For HLE-with-tools, we use a maximum context length of 202,752 tokens.\nSWE-bench & SWE-bench Multilingual\n: We run the SWE-bench suite with OpenHands using a tailored instruction prompt. Settings:\ntemperature=0.7, top_p=0.95, max_new_tokens=16384\n, with a 200K context window.\nBrowserComp\n: Without context management, we retain details from the most recent 5 turns. With context management, we use the same discard-all strategy as DeepSeek-v3.2 and Kimi K2.5.\nTerminal-Bench 2.0 (Terminus 2)\n: We evaluate with the Terminus framework using\ntimeout=2h, temperature=0.7, top_p=1.0, max_new_tokens=8192\n, with a 128K context window. Resource limits are capped at 16 CPUs and 32 GB RAM.\nTerminal-Bench 2.0 (Claude Code)\n: We evaluate in Claude Code 2.1.14 (think mode, default effort) with\ntemperature=1.0, top_p=0.95, max_new_tokens=65536\n. We remove wall-clock time limits due to generation speed, while preserving per-task CPU and memory constraints. Scores are averaged over 5 runs. We fix environment issues introduced by Claude Code and also report results on a verified Terminal-Bench 2.0 dataset that resolves ambiguous instructions (see:\nhttps://huggingface.co/datasets/zai-org/terminal-bench-2-verified\n).\nCyberGym\n: We evaluate in Claude Code 2.1.18 (think mode, no web tools) with (\ntemperature=1.0, top_p=1.0, max_new_tokens=32000\n) and a 250-minute timeout per task. Results are single-run Pass@1 over 1,507 tasks.\nMCP-Atlas\n: All models are evaluated in think mode on the 500-task public subset with a 10-minute timeout per task. We use Gemini 3 Pro as the judge model.\nœÑ¬≤-bench\n: We add a small prompt adjustment in Retail and Telecom to avoid failures caused by premature user termination. For Airline, we apply the domain fixes proposed in the Claude Opus 4.5 system card.\nVending Bench 2\n: Runs are conducted independently by\nAndon Labs\n.\nSupported languages\nEnglish\nChinese\nLicense\nMIT\nReference\nZ.ai Technical Blog\nGLM-5 on Hugging Face"
  },
  {
    "slug": "minimax-m2.5",
    "name": "minimax-m2.5",
    "description": "MiniMax-M2.5 is a state-of-the-art large language model designed for real-world productivity and coding tasks.",
    "pull_count": "51.5K",
    "updated": "2 weeks ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "minimax-m2.5:cloud",
        "size": "-",
        "context_window": "198K",
        "input_type": "Text"
      }
    ],
    "readme": "MiniMax M2.5 is the fastest-improving model series for coding and agentic workflows, trained with large-scale reinforcement learning across hundreds of thousands of real-world environments.\nHighlights\nCoding\n. Trained on large-scale RL across 10+ languages (Python, Go, C, C++, TypeScript, Rust, Kotlin, Java, JavaScript, PHP, Lua, Dart, Ruby) and hundreds of thousands of real environments, M2.5 develops native ‚Äúspec behavior‚Äù‚Äîplanning architecture, structure, and design before writing code. It handles the full development lifecycle: system design from zero, environment setup, feature iteration, code review, and testing across Web, Android, iOS, Windows, and Mac.\nLeading agent performance\n. M2.5 achieves 80.2% on SWE-Bench Verified, 51.3% on Multi-SWE-Bench, and 76.3% on BrowseComp‚Äîreaching a level comparable to the Claude Opus series on coding tasks. In search and tool-use tasks, the model demonstrates higher decision maturity‚Äîsolving problems in fewer rounds with better token efficiency, saving roughly 20% in tool-call rounds compared to M2.1.\n37% faster task completion\n. Through improved task decomposition and more efficient chain-of-thought reasoning, M2.5 completes complex agentic tasks significantly faster. On SWE-Bench Verified, end-to-end runtime dropped from 31.3 minutes (M2.1) to 22.8 minutes‚Äîon par with Claude Opus 4.6‚Äôs 22.9 minutes‚Äîwhile also consuming fewer tokens per task (3.52M vs 3.72M).\nBenchmarks\nCoding\nM2.5 represents a significant step up from M2.1, reaching performance comparable to the Claude Opus series on core software engineering tasks.\nMiniMax tested performance on the SWE-Bench Verified evaluation set using different coding agent harnesses.\nOn Droid: 79.7(M2.5) > 78.9(Opus 4.6)\nOn OpenCode: 76.1(M2.5) > 75.9(Opus 4.6)\nSearch and tool calling\nOffice & Productivity\nM2.5 was trained in collaboration with domain experts in finance, law, and social sciences to produce genuinely deliverable outputs. In evaluations on advanced office tasks‚Äîincluding Word documents, PowerPoint presentations, and Excel financial modeling‚ÄîM2.5 achieved a 59.0% average win rate against mainstream models using pairwise comparison (GDPval-MM framework).\nAdditional internal benchmarks include MEWC (based on Microsoft Excel World Championship problems, 2021‚Äì2026) and Finance Modeling (expert-constructed financial modeling tasks scored via rubrics). Detailed scores for these benchmarks are available as charts in the\noriginal blog post\n.\nReference\nMiniMax M2.5 announcement"
  },
  {
    "slug": "glm-ocr",
    "name": "glm-ocr",
    "description": "GLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder‚Äìdecoder architecture.",
    "pull_count": "46.9K",
    "updated": "3 weeks ago",
    "capabilities": [
      "vision",
      "tools"
    ],
    "tags": [
      {
        "tag": "glm-ocr:latest",
        "size": "2.2GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "glm-ocr:q8_0",
        "size": "1.6GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      },
      {
        "tag": "glm-ocr:bf16",
        "size": "2.2GB",
        "context_window": "128K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "GLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoder‚Äìdecoder architecture. The model integrates the CogViT visual encoder pre-trained on large-scale image‚Äìtext data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder.\nUsage\nText recognition\nollama run glm-ocr Text Recognition: ./image.png\nTable recognition\nollama run glm-ocr Table Recognition: ./image.png\nFigure recognition\nollama run glm-ocr Figure Recognition: ./image.png\nKey features\nState-of-the-Art Performance\n: Achieves a score of 94.62 on OmniDocBench V1.5, ranking #1 overall, and delivers state-of-the-art results across major document understanding benchmarks,\nincluding formula recognition, table recognition, and information extraction.\nOptimized for Real-World Scenarios\n: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.\nEfficient Inference\n: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.\nEasy to Use\n: Fully open-sourced and equipped with a comprehensive\nSDK\nand inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines."
  },
  {
    "slug": "nomic-embed-text-v2-moe",
    "name": "nomic-embed-text-v2-moe",
    "description": "nomic-embed-text-v2-moe is a multilingual MoE text embedding model that excels at multilingual retrieval.",
    "pull_count": "43.1K",
    "updated": "2 months ago",
    "capabilities": [
      "embedding"
    ],
    "tags": [
      {
        "tag": "nomic-embed-text-v2-moe:latest",
        "size": "958MB",
        "context_window": "512",
        "input_type": "Text"
      }
    ],
    "readme": "nomic-embed-text-v2-moe\nis a multilingual MoE text embedding model that excels at multilingual retrieval.\nHigh Performance\n: SoTA Multilingual performance compared to ~300M parameter models, competitive with models 2x in size\nMultilinguality\n: Supports ~100 languages and trained on over 1.6B pairs\nFlexible Embedding Dimension\n: Trained with\nMatryoshka Embeddings\nwith 3x reductions in storage cost with minimal performance degradations\nFully Open-Source\n: Model weights,\ncode\n, and training data\nModel\nParams (M)\nEmb Dim\nBEIR\nMIRACL\nPretrain Data\nFinetune Data\nCode\nNomic Embed v2\n305\n768\n52.86\n65.80\n‚úÖ\n‚úÖ\n‚úÖ\nmE5 Base\n278\n768\n48.88\n62.30\n‚ùå\n‚ùå\n‚ùå\nmGTE Base\n305\n768\n51.10\n63.40\n‚ùå\n‚ùå\n‚ùå\nArctic Embed v2 Base\n305\n768\n55.40\n59.90\n‚ùå\n‚ùå\n‚ùå\nBGE M3\n568\n1024\n48.80\n69.20\n‚ùå\n‚úÖ\n‚ùå\nArctic Embed v2 Large\n568\n1024\n55.65\n66.00\n‚ùå\n‚ùå\n‚ùå\nmE5 Large\n560\n1024\n51.40\n66.50\n‚ùå\n‚ùå\n‚ùå\nBest practices\nAdd appropriate prefixes to your text:\nFor queries: ‚Äúsearch_query: ‚Äú\nFor documents: ‚Äúsearch_document: ‚Äú- Maximum input length is 512 tokens\nFor optimal efficiency, consider using the 256-dimension embeddings if storage/compute is a concern\nModel Architecture\nTotal Parameters\n: 475M\nActive Parameters During Inference\n: 305M\nArchitecture Type\n: Mixture of Experts (MoE)\nMoE Configuration\n: 8 experts with top-2 routing\nEmbedding Dimensions\n: Supports flexible dimension from 768 to 256 through Matryoshka representation learning\nMaximum Sequence Length\n: 512 tokens\nLanguages\n: See below for supported languages and its training pairs per different languages"
  },
  {
    "slug": "kimi-k2",
    "name": "kimi-k2",
    "description": "A state-of-the-art mixture-of-experts (MoE) language model. Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.",
    "pull_count": "40.4K",
    "updated": "5 months ago",
    "capabilities": [
      "tools",
      "cloud"
    ],
    "tags": [
      {
        "tag": "kimi-k2:1t-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text"
      }
    ],
    "readme": "Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2. It is a state-of-the-art mixture-of-experts (MoE) language model, featuring 32 billion activated parameters and a total of 1 trillion parameters.\nKey Features\nEnhanced agentic coding intelligence: Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.\nImproved frontend coding experience: Kimi K2-Instruct-0905 offers advancements in both the aesthetics and practicality of frontend programming.\nExtended context length: Kimi K2-Instruct-0905‚Äôs context window has been increased from 128k to 256k tokens, providing better support for long-horizon tasks."
  },
  {
    "slug": "mistral-large-3",
    "name": "mistral-large-3",
    "description": "A general-purpose multimodal mixture-of-experts model for production-grade tasks and enterprise workloads.",
    "pull_count": "19.8K",
    "updated": "2 months ago",
    "capabilities": [
      "vision",
      "tools",
      "cloud"
    ],
    "tags": [
      {
        "tag": "mistral-large-3:675b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Mistral Large 3 is a state-of-the-art general-purpose Multimodal Mixture-of-Experts model, designed for production-grade tasks and enterprise workloads.\nThe Mistral Large 3 model offers the following capabilities:\nVision\n: Enables the model to analyze images and provide insights based on visual content, in addition to text.\nMultilingual\n: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.\nSystem Prompt\n: Maintains strong adherence and support for system prompts.\nAgentic\n: Offers best-in-class agentic capabilities with native function calling and JSON outputting.\nFrontier\n: Delivers best-in-class performance for the most demanding tasks.\nApache 2.0 License\n: Open-source license allowing usage and modification for both commercial and non-commercial purposes.\nLarge Context Window\n: Supports a 256k context window."
  },
  {
    "slug": "minimax-m2.1",
    "name": "minimax-m2.1",
    "description": "Exceptional multilingual capabilities to elevate code engineering",
    "pull_count": "19.7K",
    "updated": "2 months ago",
    "capabilities": [
      "tools",
      "cloud"
    ],
    "tags": [
      {
        "tag": "minimax-m2.1:cloud",
        "size": "-",
        "context_window": "200K",
        "input_type": "Text"
      }
    ],
    "readme": "Get started\nollama run minimax-m2.1:cloud\nModel Highlights\n1. Multilingual Coding Excellence (Beyond Python)\nWhile many models focus primarily on Python, real-world engineering requires cross-language proficiency. M2.1 delivers significant performance gains across\nRust, Java, Golang, C++, Kotlin, Objective-C, TypeScript, and JavaScript\n.\nWeb3 Dominance:\nSpecial optimization for Web3 protocols, offering superior performance for blockchain and decentralized projects.\nBenchmark Leadership:\nAchieved\n49.4% on Multi-SWE-Bench\n, surpassing industry leaders like Anthropic Claude 3.5 Sonnet and Gemini 1.5 Pro.\nDeep Comprehension:\nAdvanced code review capabilities, including sophisticated performance optimization and structural analysis.\n2. Optimized for ‚ÄúVibe‚Äù AppDev & Native Mobile\nWe have bridged the gap between aesthetic design and technical implementation.\nWeb & Scientific Simulation:\nEnhanced ‚Äúweb aesthetics‚Äù for better UI/UX generation and more realistic scientific scenario simulations.\nNative Mobile Powerhouse:\nAddressing a common industry weakness, M2.1 significantly boosts native\nAndroid and iOS\ndevelopment capabilities.\n‚ÄúNot only vibe webdev, but vibe appdev.‚Äù\n3. Concise, High-Efficiency Responses\nCompared to the previous generation, MiniMax-M2.1 provides cleaner outputs and more streamlined\nChain-of-Thought (CoT)\nreasoning. This reduction in ‚Äúverbosity‚Äù results in a noticeably faster ‚Äúfeel‚Äù and near-instant response times for developer workflows.\n4. Advanced Interleaved Thinking & Instruction Following\nM2.1 is the first open-source model series to implement\nAdvanced Interleaved Thinking\n, upgrading its systematic problem-solving capacity.\nComplex Constraints:\nThe model doesn‚Äôt just focus on code correctness; it excels at integrating ‚Äúcomposite instruction constraints‚Äù (as seen in\nOctoCodingBench\n).\nOffice Readiness:\nThese improvements make the model viable for complex administrative and office automation tasks (demonstrated in our\nToolathlon\nshowcase).\n5. Enhanced Scaffolding & Agent Generalization\nM2.1 is designed to be the ‚Äúbrain‚Äù behind your favorite tools. It shows exceptional performance across various programming agents and IDE extensions, including\nClaude Code, Droid (Factory AI), Cline, Kilo Code, and Roo Code.\nContext Management:\nSeamless support for framework-specific configurations like\nSkill.md\n,\nClaude.md\n,\nagent.md\n,\n.cursorrules\n, and Slash Commands.\n6. The Most Lightweight SOTA Model (10B Activated)\nIn just two months, we have achieved a massive leap in utility while maintaining the model‚Äôs legendary efficiency.\nEfficiency Powerhouse:\nWith only\n10B activated parameters\n, M2.1 remains the most cost-effective SOTA-performance model in the open-source community.\n7. High-Quality Dialogue & Creative Writing\nM2.1 isn‚Äôt just a coding specialist‚Äîit‚Äôs a more capable all-around assistant. Compared to M2, the chat and writing experience has been significantly refined, delivering more nuanced, detailed, and contextually rich answers for non-technical queries.\nBenchmarks\nMiniMax-M2.1 delivers a significant leap over M2 on core software engineering leaderboards[cite: 14]. It shines particularly bright in multilingual scenarios, where it outperforms Claude Sonnet 4.5 and closely approaches Claude Opus 4.5.\nCore Software Engineering Benchmarks\nBenchmark\nSWE-bench Verified\nMulti-SWE-bench\nSWE-bench Multilingual\nTerminal-bench 2.0\nMiniMax-M2.1\n74.0\n49.4\n72.5\n47.9\nMiniMax-M2\n69.4\n36.2\n56.5\n30.0\nKimi K2 Thinking\n71.3\n41.9\n61.1\n35.2\nDeepSeek V3.2\n73.1\n37.4\n70.2\n46.4\nGLM 4.6\n68.0\n30.0\n53.8\n24.5\nClaude Sonnet 4.5\n77.2\n44.3\n68 ¬± 0.5\n50.0\nClaude Opus 4.5\n80.9\n50.0\n77.5 ¬± 1.5\n57.8\nGemini 3 Pro\n78.0\n38.0\n65.0\n54.2\nGPT-5.2 (thinking)\n80.0\nx\n72.0\n54.0\nFramework Generalization\nBenchmark\nSWE-bench Verified (Claude Code)\nSWE-bench Verified (Droid)\nSWE-bench Verified (mini-swe-agent)\nSWT-bench\nSWE-Perf\nSWE-Review\nOctoCodingbench\nMiniMax-M2.1\n74.0\n71.3\n67.0\n69.3\n3.1\n8.9\n26.1\nMiniMax-M2\n69.4\n68.1\n61.0\n32.8\n1.4\n3.4\n13.3\nKimi K2 Thinking\n71.3\n64.0\n63.4\n38.2\n1.0\n5.3\n16.8\nDeepSeek V3.2\n73.1\n67.0\n60.0\n62.0\n0.9\n6.4\n26.0\nGLM 4.6\n68.0\n64.9\n55.4\n45.9\n0.9\n5.6\n19.2\nClaude Sonnet 4.5\n77.2\n72.3\n70.6\n69.5\n3.0\n10.5\n22.8\nClaude Opus 4.5\n80.9\n75.2\n74.4\n80.2\n4.7\n16.2\n36.2\nGemini 3 Pro\n78.0\nx\n71.8\n79.7\n6.5\nx\n22.9\nGPT-5.2 (thinking)\n80.0\nx\n74.2\n80.7\n3.6\nx\nx\nVIBE Benchmark (Visual & Interactive Benchmark for Execution)\nBenchmark\nVIBE (Average)\nVIBE-Web\nVIBE-Simulation\nVIBE-Android\nVIBE-iOS\nVIBE-Backend\nMiniMax-M2.1\n88.6\n91.5\n87.1\n89.7\n88\n86.7\nMiniMax-M2\n67.5\n80.4\n77\n69.2\n39.5\n67.8\nGLM 4.6\n72.9\n86.7\n82.4\n58.2\n59.1\n78.3\nClaude Sonnet 4.5\n85.2\n87.3\n79.1\n87.5\n81.2\n90.8\nClaude Opus 4.5\n90.7\n89.1\n84\n92.2\n90\n98\nGemini 3 Pro\n82.4\n89.5\n89.2\n78.7\n75.8\n78.7\nLong-Horizon Tool Use & Intelligence Metric\nBenchmark\nToolathlon\nBrowseComp\nBrowseComp (context management)\nAA-Index\nMiniMax-M2.1\n43.5\n47.4\n62\n64\nMiniMax-M2\n16.7\n44\n56.9\n61\nKimi K2 Thinking\n17.6\n41.5\n60.2\n67\nDeepSeek V3.2\n35.2\n51.4\n67.6\n66\nGLM 4.6\n18.8\n45.1\n50.2\n56\nClaude Sonnet 4.5\n38.9\n19.6\n26.1\n63\nClaude Opus 4.5\n43.5\n37\n57.8\n70\nGemini 3 Pro\n36.4\n37.8\n59.2\n73\nGPT-5.2 (thinking)\n41.7\n65.8\n70\n73"
  },
  {
    "slug": "glm-4.7",
    "name": "glm-4.7",
    "description": "Advancing the Coding Capability",
    "pull_count": "53.1K",
    "updated": "2 months ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "glm-4.7:cloud",
        "size": "-",
        "context_window": "198K",
        "input_type": "Text"
      }
    ],
    "readme": "GLM-4.7\n, your new coding partner, is coming with the following features:\nCore Coding:\nGLM-4.7 brings clear gains, compared to its predecessor GLM-4.6, in multilingual agentic coding and terminal-based tasks, including (73.8%, +5.8%) on SWE-bench, (66.7%, +12.9%) on SWE-bench Multilingual, and (41%, +16.5%) on Terminal Bench 2.0. GLM-4.7 also supports thinking before acting, with significant improvements on complex tasks in mainstream agent frameworks such as Claude Code, Kilo Code, Cline, and Roo Code.\nVibe Coding:\nGLM-4.7 takes a major step forward in UI quality. It produces cleaner, more modern webpages and generates better-looking slides with more accurate layout and sizing.\nTool Using:\nGLM-4.7 achieves significantly improvements in Tool using. Significant better performances can be seen on benchmarks such as œÑ^2-Bench and on web browsing via BrowseComp.\nComplex Reasoning:\nGLM-4.7 delivers a substantial boost in mathematical and reasoning capabilities, achieving (42.8%, +12.4%) on the HLE (Humanity‚Äôs Last Exam) benchmark compared to GLM-4.6.\nYou can also see significant improvements in many other scenarios such as chat, creative writing, and role-play scenario.\nBenchmark Performance.\nMore detailed comparisons of GLM-4.7 with other models GPT-5, GPT-5.1-High, Claude Sonnet 4.5, Gemini 3.0 Pro, DeepSeek-V3.2, Kimi K2 Thinking, on 17 benchmarks (including 8 reasoning, 5 coding, and 3 agents benchmarks) can be seen in the below table.\nCoding:\nAGI is a long journey, and benchmarks are only one way to evaluate performance. While the metrics provide necessary checkpoints, the most important thing is still how it\nfeels\n. True intelligence isn‚Äôt just about acing a test or processing data faster; ultimately, the success of AGI will be measured by how seamlessly it integrates into our lives-‚Äúcoding‚Äù this time."
  },
  {
    "slug": "deepseek-v3.2",
    "name": "deepseek-v3.2",
    "description": "DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance.",
    "pull_count": "35.3K",
    "updated": "2 months ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "deepseek-v3.2:cloud",
        "size": "-",
        "context_window": "160K",
        "input_type": "Text"
      }
    ],
    "readme": "DeepSeek-V3.2 is a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\nDeepSeek Sparse Attention (DSA)\n: an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\nScalable Reinforcement Learning Framework\n: By implementing a robust RL protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5.\nLarge-Scale Agentic Task Synthesis Pipeline\n: To integrate reasoning into tool-use scenarios, [DeepSeek team] developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\nReference\nDeepSeek-V3.2: Pushing the Frontier of Open Large Language Models"
  },
  {
    "slug": "qwen3.5",
    "name": "qwen3.5",
    "description": "Qwen 3.5 is a family of open-source multimodal models that delivers exceptional utility and performance.",
    "pull_count": "57.4K",
    "updated": "6 hours ago",
    "capabilities": [
      "vision",
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "qwen3.5:27b",
        "size": "17GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3.5:35b",
        "size": "24GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3.5:122b",
        "size": "81GB",
        "context_window": "256K",
        "input_type": "Text, Image"
      },
      {
        "tag": "qwen3.5:cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text"
      },
      {
        "tag": "qwen3.5:397b-cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text, Image"
      }
    ],
    "readme": "Over recent months, we have intensified our focus on developing foundation models that deliver exceptional utility and performance. Qwen3.5 represents a significant leap forward, integrating breakthroughs in multimodal learning, architectural efficiency, reinforcement learning scale, and global accessibility to empower developers and enterprises with unprecedented capability and efficiency.\nHighlights\nQwen3.5 features the following enhancement:\nUnified Vision-Language Foundation\n: Early fusion training on multimodal tokens achieves cross-generational parity with Qwen3 and outperforms Qwen3-VL models across reasoning, coding, agents, and visual understanding benchmarks.\nEfficient Hybrid Architecture\n: Gated Delta Networks combined with sparse Mixture-of-Experts deliver high-throughput inference with minimal latency and cost overhead.\nScalable RL Generalization\n: Reinforcement learning scaled across million-agent environments with progressively complex task distributions for robust real-world adaptability.\nGlobal Linguistic Coverage\n: Expanded support to 201 languages and dialects, enabling inclusive, worldwide deployment with nuanced cultural and regional understanding.\nNext-Generation Training Infrastructure\n: Near-100% multimodal training efficiency compared to text-only training and asynchronous RL frameworks supporting massive-scale agent scaffolds and environment orchestration.\nBenchmarks\nLanguage\nGPT5.2\nClaude 4.5 Opus\nGemini-3 Pro\nQwen3-Max-Thinking\nK2.5-1T-A32B\nQwen3.5-397B-A17B\nKnowledge\nMMLU-Pro\n87.4\n89.5\n89.8\n85.7\n87.1\n87.8\nMMLU-Redux\n95.0\n95.6\n95.9\n92.8\n94.5\n94.9\nSuperGPQA\n67.9\n70.6\n74.0\n67.3\n69.2\n70.4\nC-Eval\n90.5\n92.2\n93.4\n93.7\n94.0\n93.0\nInstruction Following\nIFEval\n94.8\n90.9\n93.5\n93.4\n93.9\n92.6\nIFBench\n75.4\n58.0\n70.4\n70.9\n70.2\n76.5\nMultiChallenge\n57.9\n54.2\n64.2\n63.3\n62.7\n67.6\nLong Context\nAA-LCR\n72.7\n74.0\n70.7\n68.7\n70.0\n68.7\nLongBench v2\n54.5\n64.4\n68.2\n60.6\n61.0\n63.2\nSTEM\nGPQA\n92.4\n87.0\n91.9\n87.4\n87.6\n88.4\nHLE\n35.5\n30.8\n37.5\n30.2\n30.1\n28.7\nHLE-Verified¬π\n43.3\n38.8\n48\n37.6\n--\n37.6\nReasoning\nLiveCodeBench v6\n87.7\n84.8\n90.7\n85.9\n85.0\n83.6\nHMMT Feb 25\n99.4\n92.9\n97.3\n98.0\n95.4\n94.8\nHMMT Nov 25\n100\n93.3\n93.3\n94.7\n91.1\n92.7\nIMOAnswerBench\n86.3\n84.0\n83.3\n83.9\n81.8\n80.9\nAIME26\n96.7\n93.3\n90.6\n93.3\n93.3\n91.3\nGeneral Agent\nBFCL-V4\n63.1\n77.5\n72.5\n67.7\n68.3\n72.9\nTAU2-Bench\n87.1\n91.6\n85.4\n84.6\n77.0\n86.7\nVITA-Bench\n38.2\n56.3\n51.6\n40.9\n41.9\n49.7\nDeepPlanning\n44.6\n33.9\n23.3\n28.7\n14.5\n34.3\nTool Decathlon\n43.8\n43.5\n36.4\n18.8\n27.8\n38.3\nMCP-Mark\n57.5\n42.3\n53.9\n33.5\n29.5\n46.1\nSearch Agent¬≥\nHLE w/ tool\n45.5\n43.4\n45.8\n49.8\n50.2\n48.3\nBrowseComp\n65.8\n67.8\n59.2\n53.9\n--/74.9\n69.0/78.6\nBrowseComp-zh\n76.1\n62.4\n66.8\n60.9\n--\n70.3\nWideSearch\n76.8\n76.4\n68.0\n57.9\n72.7\n74.0\nSeal-0\n45.0\n47.7\n45.5\n46.9\n57.4\n46.9\nMultilingualism\nMMMLU\n89.5\n90.1\n90.6\n84.4\n86.0\n88.5\nMMLU-ProX\n83.7\n85.7\n87.7\n78.5\n82.3\n84.7\nNOVA-63\n54.6\n56.7\n56.7\n54.2\n56.0\n59.1\nINCLUDE\n87.5\n86.2\n90.5\n82.3\n83.3\n85.6\nGlobal PIQA\n90.9\n91.6\n93.2\n86.0\n89.3\n89.8\nPolyMATH\n62.5\n79.0\n81.6\n64.7\n43.1\n73.3\nWMT24++\n78.8\n79.7\n80.7\n77.6\n77.6\n78.9\nMAXIFE\n88.4\n79.2\n87.5\n84.0\n72.8\n88.2\nCoding Agent\nSWE-bench Verified\n80.0\n80.9\n76.2\n75.3\n76.8\n76.2\nSWE-bench Multilingual\n72.0\n77.5\n65.0\n66.7\n73.0\n69.3\nSecCodeBench\n68.7\n68.6\n62.4\n57.5\n61.3\n68.3\nTerminal Bench 2\n54.0\n59.3\n54.2\n22.5\n50.8\n52.5\n* HLE-Verified: a verified and revised version of Humanity‚Äôs Last Exam (HLE), accompanied by a transparent, component-wise verification protocol and a fine-grained error taxonomy. We open-source the dataset at https://huggingface.co/datasets/skylenage/HLE-Verified.\n* TAU2-Bench: we follow the official setup except for the airline domain, where all models are evaluated by applying the fixes proposed in the Claude Opus 4.5 system card.\n* MCPMark: GitHub MCP server uses v0.30.3 from api.githubcopilot.com; Playwright tool responses are truncated at 32k tokens.\n* Search Agent: most search agents built on our model adopt a simple context-folding strategy(256k): once the cumulative Tool Response length reaches a preset threshold, earlier Tool Responses are pruned from the history to keep the context within limits.\n* BrowseComp: we tested two strategies, simple context-folding achieved a score of 69.0, while using the same discard-all strategy as DeepSeek-V3.2 and Kimi K2.5 achieved 78.6.\n* WideSearch: we use a 256k context window without any context management.\n* MMLU-ProX: we report the averaged accuracy on 29 languages.\n* WMT24++: a harder subset of WMT24 after difficulty labeling and rebalancing; we report the averaged scores on 55 languages using XCOMET-XXL.\n* MAXIFE: we report the accuracy on English + multilingual original prompts (totally 23 settings).\n* Empty cells (--) indicate scores not yet available or not applicable.\nVision Language\nGPT5.2\nClaude 4.5 Opus\nGemini-3 Pro\nQwen3-VL-235B-A22B\nK2.5-1T-A32B\nQwen3.5-397B-A17B\nSTEM and Puzzle\nMMMU\n86.7\n80.7\n87.2\n80.6\n84.3\n85.0\nMMMU-Pro\n79.5\n70.6\n81.0\n69.3\n78.5\n79.0\nMathVision\n83.0\n74.3\n86.6\n74.6\n84.2\n88.6\nMathvista(mini)\n83.1\n80.0\n87.9\n85.8\n90.1\n90.3\nWe-Math\n79.0\n70.0\n86.9\n74.8\n84.7\n87.9\nDynaMath\n86.8\n79.7\n85.1\n82.8\n84.4\n86.3\nZEROBench\n9\n3\n10\n4\n9\n12\nZEROBench_sub\n33.2\n28.4\n39.0\n28.4\n33.5\n41.0\nBabyVision\n34.4\n14.2\n49.7\n22.2\n36.5\n52.\n3\n‚ÅÑ\n43\n.3\nGeneral VQA\nRealWorldQA\n83.3\n77.0\n83.3\n81.3\n81.0\n83.9\nMMStar\n77.1\n73.2\n83.1\n78.7\n80.5\n83.8\nHallusionBench\n65.2\n64.1\n68.6\n66.7\n69.8\n71.4\nMMBench\nEN-DEV-v1.1\n88.2\n89.2\n93.7\n89.7\n94.2\n93.7\nSimpleVQA\n55.8\n65.7\n73.2\n61.3\n71.2\n67.1\nText Recognition and Document Understanding\nOmniDocBench1.5\n85.7\n87.7\n88.5\n84.5\n88.8\n90.8\nCharXiv(RQ)\n82.1\n68.5\n81.4\n66.1\n77.5\n80.8\nMMLongBench-Doc\n‚Äì\n61.9\n60.5\n56.2\n58.5\n61.5\nCC-OCR\n70.3\n76.9\n79.0\n81.5\n79.7\n82.0\nAI2D_TEST\n92.2\n87.7\n94.1\n89.2\n90.8\n93.9\nOCRBench\n80.7\n85.8\n90.4\n87.5\n92.3\n93.1\nSpatial Intelligence\nERQA\n59.8\n46.8\n70.5\n52.5\n‚Äì\n67.5\nCountBench\n91.9\n90.6\n97.3\n93.7\n94.1\n97.2\nRefCOCO(avg)\n‚Äì\n‚Äì\n84.1\n91.1\n87.8\n92.3\nODInW13\n‚Äì\n‚Äì\n46.3\n43.2\n‚Äì\n47.0\nEmbSpatialBench\n81.3\n75.7\n61.2\n84.3\n77.4\n84.5\nRefSpatialBench\n‚Äì\n‚Äì\n65.5\n69.9\n‚Äì\n73.6\nLingoQA\n68.8\n78.8\n72.8\n66.8\n68.2\n81.6\nV*\n75.9\n67.0\n88.0\n85.9\n77.0\n95.\n8\n‚ÅÑ\n91\n.1\nHypersim\n‚Äì\n‚Äì\n‚Äì\n11.0\n‚Äì\n12.5\nSUNRGBD\n‚Äì\n‚Äì\n‚Äì\n34.9\n‚Äì\n38.3\nNuscene\n‚Äì\n‚Äì\n‚Äì\n13.9\n‚Äì\n16.0\nVideo Understanding\nVideoMME\n(w sub.)\n86\n77.6\n88.4\n83.8\n87.4\n87.5\nVideoMME\n(w/o sub.)\n85.8\n81.4\n87.7\n79.0\n83.2\n83.7\nVideoMMMU\n85.9\n84.4\n87.6\n80.0\n86.6\n84.7\nMLVU (M-Avg)\n85.6\n81.7\n83.0\n83.8\n85.0\n86.7\nMVBench\n78.1\n67.2\n74.1\n75.2\n73.5\n77.6\nLVBench\n73.7\n57.3\n76.2\n63.6\n75.9\n75.5\nMMVU\n80.8\n77.3\n77.5\n71.1\n80.4\n75.4\nVisual Agent\nScreenSpot Pro\n‚Äì\n45.7\n72.7\n62.0\n‚Äì\n65.6\nOSWorld-Verified\n38.2\n66.3\n‚Äì\n38.1\n63.3\n62.2\nAndroidWorld\n‚Äì\n‚Äì\n‚Äì\n63.7\n‚Äì\n66.8\nMedical VQA\nSLAKE\n76.9\n76.4\n81.3\n54.7\n81.6\n79.9\nPMC-VQA\n58.9\n59.9\n62.3\n41.2\n63.3\n64.2\nMedXpertQA-MM\n73.3\n63.6\n76.0\n47.6\n65.3\n70.0\n* MathVisionÔºöour model‚Äôs score is evaluated using a fixed prompt, e.g., ‚ÄúPlease reason step by step, and put your final answer within \\boxed{}.‚Äù For other models, we report the higher score between runs with and without the \\boxed{} formatting.\n* BabyVision: our model‚Äôs score is reported with CI (Code Interpreter) enabled; without CI, the result is 43.3.\n* V*: our model‚Äôs score is reported with CI (Code Interpreter) enabled; without CI, the result is 91.1.\n* Empty cells (--) indicate scores not yet available or not applicable."
  },
  {
    "slug": "kimi-k2-thinking",
    "name": "kimi-k2-thinking",
    "description": "Kimi K2 Thinking, Moonshot AI's best open-source thinking model.",
    "pull_count": "31.7K",
    "updated": "3 months ago",
    "capabilities": [
      "tools",
      "thinking",
      "cloud"
    ],
    "tags": [
      {
        "tag": "kimi-k2-thinking:cloud",
        "size": "-",
        "context_window": "256K",
        "input_type": "Text"
      }
    ],
    "readme": "Kimi K2 Thinking is Moonshot AI‚Äôs best open-source thinking model.\nBuilt as a\nthinking agent\n, it reasons step by step while using tools, achieving state-of-the-art performance on Humanity‚Äôs Last Exam (HLE), BrowseComp, and other benchmarks, with major gains in reasoning, agentic search, coding, writing, and general capabilities.\nKimi K2 Thinking can execute up to\n200 ‚Äì 300 sequential tool calls\nwithout human interference, reasoning coherently across hundreds of steps to solve complex problems.\nIt marks Moonshot AI‚Äôs latest efforts in\ntest-time scaling\n, by scaling both thinking tokens and tool calling steps.\nAgentic Coding\nK2 Thinking exhibits substantial gains in coding and software development tasks. It achieves scores of 61.1% on SWE-Multilingual, 71.3% on SWE-Bench Verified, and 47.1% on Terminal-Bench, showcasing strong generalization across programming languages and agent scaffolds.\nThe model delivers notable improvements on HTML, React, and component-intensive front-end tasks‚Äîtranslating ideas into fully functional, responsive products. In agentic coding settings, it reasons while invoking tools, integrating fluidly into software agents to execute complex, multi-step development workflows with precision and adaptability.\nAgentic Search and Browsing\nK2 Thinking demonstrates strong performance in agentic search and browsing scenarios. On BrowseComp‚Äîa challenging benchmark designed to evaluate models‚Äô ability to ‚Äã\ncontinuously browse, search, and reason over hard-to-find real-world web information‚Äã\n‚ÄîK2 Thinking achieved a score of\n‚Äã‚Äã60.‚Äã2%\n, significantly outperforming the human baseline of 29.2%. This result highlights K2 Thinking‚Äôs superior capability for goal-directed, web-based reasoning and its robustness in dynamic, information-rich environments.\nK2 Thinking can execute ‚Äã\n200‚Äì300 sequential tool calls\n‚Äã, driven by\nlong-horizon planning\nand ‚Äã\nadaptive reasoning‚Äã\n. It performs dynamic cycles of ‚Äãthink ‚Üí search ‚Üí browser use ‚Üí think ‚Üí code‚Äã, continually generating and refining hypotheses, verifying evidence, reasoning, and constructing coherent answers. This interleaved reasoning allows it to decompose ambiguous, open-ended problems into clear, actionable subtasks.\nGeneral Capabilities\nCreative Writing:\n‚ÄãK2 Thinking delivers improvements in completeness and richness. It shows stronger command of style and instruction, handling diverse tones and formats with natural fluency. Its writing becomes more vivid and imaginative‚Äîpoetic imagery carries deeper associations, while stories and scripts feel more human, emotional, and purposeful. The ideas it expresses often reach greater thematic depth and resonance.\nPractical Writing:\n‚ÄãK2 Thinking demonstrates marked gains in reasoning depth, perspective breadth, and instruction adherence. It follows prompts with higher precision, addressing each requirement clearly and systematically‚Äîoften expanding on every mentioned point to ensure thorough coverage. In academic, research, and long-form analytical writing, it excels at producing rigorous, logically coherent, and substantively rich content, making it particularly effective in scholarly and professional contexts.\nPersonal & Emotional:\n‚ÄãWhen addressing personal or emotional questions, K2 Thinking responds with more empathy and balance. Its reflections are thoughtful and specific, offering nuanced perspectives and actionable next steps. It helps users navigate complex decisions with clarity and care‚Äîgrounded, practical, and genuinely human in tone.\nBenchmarks\nKimi K2 Thinking sets new records across benchmarks that assess reasoning, coding, and agent capabilities. K2 Thinking achieves 44.9% on HLE with tools, 60.2% on BrowseComp, and 71.3% on SWE-Bench Verified, demonstrating strong generalization as a state-of-the-art thinking agent model.\nBenchmark\nIntro\nK2 Thinking\nGPT-5\nClaude Sonnet 4.5 (Thinking)\nK2 0905\nDeepSeek-V3.2\nGrok-4\nReasoning Tasks\nHumanity‚Äôs Last Exam\n(Text-only)\nno tools\n23.9\n26.3\n19.8\n7.9\n19.8\n25.4\nw/ tools\n44.9\n41.7\n32.0\n21.7\n20.3\n41.0\nheavy\n51.0\n42.0\n‚Äî\n‚Äî\n‚Äî\n50.7\nAIME 2025\nno tools\n94.5\n94.6\n87.0\n51.0\n89.3\n91.7\nw/ python\n99.1\n99.6\n100.0\n75.2\n58.1\n98.8\nheavy\n100.0\n100.0\n‚Äî\n‚Äî\n‚Äî\n100.0\nHMMT 2025\nno tools\n89.4\n93.3\n74.6\n38.8\n83.6\n90.0\nw/ python\n95.1\n96.7\n88.8\n70.4\n49.5\n93.9\nheavy\n97.5\n100.0\n‚Äî\n‚Äî\n‚Äî\n96.7\nIMO-AnswerBench\nno tools\n78.6\n76.0\n65.9\n45.8\n76.0\n73.1\nGPQA-Diamond\nno tools\n84.5\n85.7\n83.4\n74.2\n79.9\n87.5\nGeneral Tasks\nMMLU-Pro\nno tools\n84.6\n87.1\n87.5\n81.9\n85.0\n‚Äî\nMMLU-Redux\nno tools\n94.4\n95.3\n95.6\n92.7\n93.7\n‚Äî\nLongform Writing\nno tools\n73.8\n71.4\n79.8\n62.8\n72.5\n‚Äî\nHealthBench\nno tools\n58.0\n67.2\n44.2\n43.8\n46.9\n‚Äî\nAgentic Search Tasks\nBrowseComp\nw/ tools\n60.2\n54.9\n24.1\n7.4\n40.1\n‚Äî\nBrowseComp-ZH\nw/ tools\n62.3\n63.0\n42.4\n22.2\n47.9\n‚Äî\nSeal-0\nw/ tools\n56.3\n51.4\n53.4\n25.2\n38.5\n‚Äî\nFinSearchComp-T3\nw/ tools\n47.4\n48.5\n44.0\n10.4\n27.0\n‚Äî\nFrames\nw/ tools\n87.0\n86.0\n85.0\n58.1\n80.2\n‚Äî\nCoding Tasks\nSWE-bench Verified\nw/ tools\n71.3\n74.9\n77.2\n69.2\n67.8\n‚Äî\nSWE-bench Multilingual\nw/ tools\n61.1\n55.3\n68.0\n55.9\n57.9\n‚Äî\nMulti-SWE-bench\nw/ tools\n41.9\n39.3\n44.3\n33.5\n30.6\n‚Äî\nSciCode\nno tools\n44.8\n42.9\n44.7\n30.7\n37.7\n‚Äî\nLiveCodeBench v6\nno tools\n83.1\n87.0\n64.0\n56.1\n74.1\n‚Äî\nOJ-Bench\n(cpp)\nno tools\n48.7\n56.2\n30.4\n25.5\n38.2\n‚Äî\nTerminal-Bench\nw/ simulated tools (JSON)\n47.1\n43.8\n51.0\n44.5\n37.7\n‚Äî"
  }
]